# ğŸ¬ LLM ê¸°ë°˜ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì‹œìŠ¤í…œ ë°œì „ ê³„íš

## ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### âœ… ì´ë¯¸ êµ¬í˜„ëœ ê°•ë ¥í•œ ê¸°ëŠ¥ë“¤
- **ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ë¶„ì„**: YOLO ê¸°ë°˜ ê°ì²´ ê°ì§€, í”„ë ˆì„ë³„ ë¶„ì„
- **Multi-modal LLM í†µí•©**: GPT, Claude, Mixtral, Ollama ì§€ì›
- **ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥**: í¬ë¡œìŠ¤ ë¹„ë””ì˜¤ ê²€ìƒ‰, ì¸íŠ¸ë¼ ë¹„ë””ì˜¤ ì¶”ì , ì‹œê°„ ê¸°ë°˜ ë¶„ì„
- **í’ë¶€í•œ ë©”íƒ€ë°ì´í„°**: ë‚ ì”¨, ì‹œê°„ëŒ€, ìƒ‰ìƒ, ì„±ë³„, ë‚˜ì´ ë“± ìƒì„¸ ë¶„ì„
- **í”„ë ˆì„ ìº¡ì…˜**: ê° í”„ë ˆì„ì— ëŒ€í•œ ì˜ë¯¸ì  ì„¤ëª… ìƒì„±

## ğŸš€ í•µì‹¬ ë°œì „ ë°©í–¥

### 1. Scene-based Video Understanding (ì¥ë©´ ê¸°ë°˜ ë¹„ë””ì˜¤ ì´í•´)

í˜„ì¬ í”„ë ˆì„ë³„ ë¶„ì„ì„ ì¥ë©´ ë‹¨ìœ„ë¡œ í™•ì¥í•˜ì—¬ ë” ì˜ë¯¸ìˆëŠ” ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë„ë¡ ê°œì„ :

```python
# ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€
class VideoScene(models.Model):
    video = models.ForeignKey(Video, on_delete=models.CASCADE)
    scene_id = models.IntegerField()
    start_timestamp = models.FloatField()
    end_timestamp = models.FloatField()
    scene_description = models.TextField()  # LLMì´ ìƒì„±í•œ ì¥ë©´ ì„¤ëª…
    scene_type = models.CharField(max_length=50)  # indoor/outdoor/street ë“±
    dominant_objects = models.JSONField()  # ì£¼ìš” ê°ì²´ë“¤
    activity_context = models.TextField()  # í™œë™ ë§¥ë½
    semantic_embedding = models.JSONField()  # ë²¡í„° ì„ë² ë”©
    weather_condition = models.CharField(max_length=20)
    time_of_day = models.CharField(max_length=20)
    lighting_condition = models.CharField(max_length=20)
    created_at = models.DateTimeField(auto_now_add=True)
```

### 2. Enhanced LLM Integration (í–¥ìƒëœ LLM í†µí•©)

```python
class VideoLLMAnalyzer:
    def __init__(self):
        self.scene_analyzer = SceneAnalyzer()
        self.semantic_search = SemanticSearchEngine()
        self.query_processor = QueryProcessor()
    
    def analyze_video_scenes(self, video_path):
        """ë¹„ë””ì˜¤ë¥¼ ì¥ë©´ë³„ë¡œ ë¶„ì„í•˜ê³  LLMìœ¼ë¡œ ì„¤ëª… ìƒì„±"""
        scenes = self.scene_analyzer.detect_scenes(video_path)
        enhanced_scenes = []
        
        for scene in scenes:
            # í”„ë ˆì„ ìƒ˜í”Œë§
            key_frames = self.extract_key_frames(scene)
            
            # LLMìœ¼ë¡œ ì¥ë©´ ì„¤ëª… ìƒì„±
            scene_description = self.generate_scene_description(key_frames)
            
            # ì˜ë¯¸ì  ì„ë² ë”© ìƒì„±
            embedding = self.create_semantic_embedding(scene_description)
            
            enhanced_scenes.append({
                'scene': scene,
                'description': scene_description,
                'embedding': embedding,
                'metadata': self.extract_scene_metadata(scene)
            })
        
        return enhanced_scenes
    
    def semantic_video_search(self, query, video_id=None):
        """ì˜ë¯¸ì  ë¹„ë””ì˜¤ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ë¶„ì„ ë° ì„ë² ë”© ìƒì„±
        query_embedding = self.create_query_embedding(query)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        if video_id:
            # íŠ¹ì • ë¹„ë””ì˜¤ ë‚´ ê²€ìƒ‰
            results = self.search_within_video(video_id, query_embedding)
        else:
            # ì „ì²´ ë¹„ë””ì˜¤ ê²€ìƒ‰
            results = self.search_across_videos(query_embedding)
        
        return results
```

### 3. Advanced Query Processing (ê³ ê¸‰ ì¿¼ë¦¬ ì²˜ë¦¬)

```python
class QueryProcessor:
    def __init__(self):
        self.llm_client = LLMClient()
    
    def parse_natural_query(self, query):
        """ìì—°ì–´ ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ì¡°ê±´ìœ¼ë¡œ ë³€í™˜"""
        prompt = f"""
        ë‹¤ìŒ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ì¡°ê±´ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        ì¿¼ë¦¬: "{query}"
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "search_type": "cross_video|intra_video|time_analysis",
            "target_video_id": null,
            "conditions": {{
                "weather": ["rain", "snow", "sunny"],
                "time_of_day": ["morning", "afternoon", "evening", "night"],
                "objects": ["person", "car", "building"],
                "colors": ["red", "blue", "green"],
                "activities": ["walking", "running", "standing"],
                "scene_context": ["indoor", "outdoor", "street"]
            }},
            "temporal_constraints": {{
                "start_time": null,
                "end_time": null,
                "duration_range": null
            }},
            "semantic_intent": "ì‚¬ìš©ìì˜ ì˜ë„ ì„¤ëª…"
        }}
        """
        
        response = self.llm_client.chat(prompt)
        return self.parse_structured_response(response)
```

## ğŸ“‹ ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš

### Phase 1: ë°ì´í„° êµ¬ì¡° í™•ì¥ (2-3ì£¼)
1. **ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€**:
   - `VideoScene` ëª¨ë¸
   - `SceneAnalysis` ëª¨ë¸  
   - `SemanticEmbedding` ëª¨ë¸

2. **ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¥**:
   - í”„ë ˆì„ë³„ ë¶„ì„ì„ ì¥ë©´ë³„ë¡œ ê·¸ë£¹í™”
   - ì˜ë¯¸ì  ì„ë² ë”© ìƒì„± ë° ì €ì¥

### Phase 2: LLM í†µí•© ê°•í™” (3-4ì£¼)
1. **ì¥ë©´ ì„¤ëª… ìƒì„±**:
   - í”„ë ˆì„ ì‹œí€€ìŠ¤ë¥¼ LLMì— ì…ë ¥í•˜ì—¬ ì¥ë©´ ì„¤ëª… ìƒì„±
   - í™œë™, ë§¥ë½, ê°ì • ë“± ê³ ì°¨ì› ì •ë³´ ì¶”ì¶œ

2. **ì¿¼ë¦¬ ì²˜ë¦¬ ê°œì„ **:
   - ìì—°ì–´ ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ì¡°ê±´ìœ¼ë¡œ ë³€í™˜
   - ì˜ë„ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ì„ íƒ

### Phase 3: ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ (4-5ì£¼)
1. **ì˜ë¯¸ì  ê²€ìƒ‰**:
   - ë²¡í„° ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸ì )

2. **ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥**:
   - ì‹œê°„ ê¸°ë°˜ í•„í„°ë§
   - ê°ì²´ ì¶”ì  ë° ì‹œí€€ìŠ¤ ë¶„ì„
   - ê°ì •/ë¶„ìœ„ê¸° ê¸°ë°˜ ê²€ìƒ‰

### Phase 4: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œì„  (2-3ì£¼)
1. **ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤**:
   - ìì—°ì–´ ê²€ìƒ‰ ì…ë ¥
   - ê²€ìƒ‰ ê²°ê³¼ ì‹œê°í™”
   - í•„í„°ë§ ì˜µì…˜

2. **ê²°ê³¼ í‘œì‹œ**:
   - ì¥ë©´ë³„ í•˜ì´ë¼ì´íŠ¸
   - íƒ€ì„ë¼ì¸ ë·°
   - ê´€ë ¨ í”„ë ˆì„ ê°¤ëŸ¬ë¦¬

## ğŸ¯ êµ¬ì²´ì ì¸ ì‚¬ìš© ì‚¬ë¡€

### ì˜ˆì‹œ 1: "ë¹„ê°€ì˜¤ëŠ” ë°¤ì— ì´¬ì˜ëœ ì˜ìƒì„ ì°¾ì•„ì¤˜"
```python
# ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼
{
    "search_type": "cross_video",
    "conditions": {
        "weather": ["rain"],
        "time_of_day": ["night"],
        "lighting_condition": ["dark"]
    },
    "semantic_intent": "ë¹„ê°€ ì˜¤ëŠ” ë°¤ì˜ ë¶„ìœ„ê¸°ë‚˜ ìƒí™©ì„ ë‹´ì€ ì˜ìƒ ê²€ìƒ‰"
}

# ê²€ìƒ‰ ê²°ê³¼
- ë¹„ë””ì˜¤ A: ë§¤ì¹­ ì ìˆ˜ 0.95 (ë¹„ ì˜¤ëŠ” ë°¤ ê±°ë¦¬)
- ë¹„ë””ì˜¤ B: ë§¤ì¹­ ì ìˆ˜ 0.87 (ì•¼ê°„ ë¹„ ì˜¤ëŠ” ì‹¤ë‚´)
- ë¹„ë””ì˜¤ C: ë§¤ì¹­ ì ìˆ˜ 0.72 (ì–´ë‘ìš´ ë°¤, ì•½ê°„ì˜ ë¹„)
```

### ì˜ˆì‹œ 2: "ì´ ì˜ìƒì—ì„œ ì£¼í™©ìƒ‰ ìƒì˜ë¥¼ ì…ì€ ë‚¨ì„±ì´ ì§€ë‚˜ê°„ ì¥ë©´ì„ ì¶”ì í•´ì¤˜"
```python
# ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼
{
    "search_type": "intra_video",
    "target_video_id": 123,
    "conditions": {
        "objects": ["person"],
        "colors": ["orange"],
        "gender": ["male"],
        "clothing": ["shirt", "top"]
    },
    "semantic_intent": "íŠ¹ì • ë¹„ë””ì˜¤ ë‚´ì—ì„œ ì£¼í™©ìƒ‰ ìƒì˜ë¥¼ ì…ì€ ë‚¨ì„±ì˜ ë“±ì¥ ì¥ë©´ë“¤ ì¶”ì "
}

# ê²€ìƒ‰ ê²°ê³¼
- ì¥ë©´ 1 (0:15-0:25): ì£¼í™©ìƒ‰ ìƒì˜ ë‚¨ì„± ë“±ì¥, ì‹ ë¢°ë„ 0.92
- ì¥ë©´ 3 (1:30-1:45): ê°™ì€ ì¸ë¬¼ ì¬ë“±ì¥, ì‹ ë¢°ë„ 0.88
- ì¥ë©´ 5 (2:10-2:20): ë°°ê²½ì—ì„œ ì§€ë‚˜ê°€ëŠ” ëª¨ìŠµ, ì‹ ë¢°ë„ 0.75
```

### ì˜ˆì‹œ 3: "ì´ ì˜ìƒì—ì„œ 3:00~5:00ë¶„ ì‚¬ì´ì— ì§€ë‚˜ê°„ ì‚¬ëŒë“¤ì˜ ì„±ë¹„ ë¶„í¬ëŠ” ì–´ë–»ê²Œ ë¼?"
```python
# ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼
{
    "search_type": "time_analysis",
    "target_video_id": 123,
    "temporal_constraints": {
        "start_time": 180,  # 3ë¶„ = 180ì´ˆ
        "end_time": 300     # 5ë¶„ = 300ì´ˆ
    },
    "analysis_type": "gender_distribution",
    "semantic_intent": "íŠ¹ì • ì‹œê°„ êµ¬ê°„ ë‚´ ì‚¬ëŒë“¤ì˜ ì„±ë³„ ë¶„í¬ ë¶„ì„"
}

# ë¶„ì„ ê²°ê³¼
{
    "time_range": "3:00-5:00",
    "total_persons": 15,
    "gender_distribution": {
        "male": 8,
        "female": 7
    },
    "gender_ratio": {
        "male_percentage": 53.3,
        "female_percentage": 46.7
    },
    "detailed_breakdown": [
        {"timestamp": "3:15", "gender": "male", "confidence": 0.89},
        {"timestamp": "3:22", "gender": "female", "confidence": 0.92},
        # ... ë” ë§ì€ ë°ì´í„°
    ]
}
```

## ğŸ”§ ê¸°ìˆ ì  êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. ì¥ë©´ ê°ì§€ ì•Œê³ ë¦¬ì¦˜
```python
class SceneDetector:
    def detect_scenes(self, video_path):
        """ë¹„ë””ì˜¤ì—ì„œ ì¥ë©´ ë³€í™” ê°ì§€"""
        cap = cv2.VideoCapture(video_path)
        scenes = []
        prev_frame = None
        scene_start = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if prev_frame is not None:
                # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì¥ë©´ ë³€í™” ê°ì§€
                hist_diff = cv2.compareHist(
                    cv2.calcHist([prev_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]),
                    cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]),
                    cv2.HISTCMP_CORREL
                )
                
                if hist_diff < 0.3:  # ì¥ë©´ ë³€í™” ì„ê³„ê°’
                    scenes.append({
                        'start': scene_start,
                        'end': cap.get(cv2.CAP_PROP_POS_MSEC) / 1000,
                        'frame_count': cap.get(cv2.CAP_PROP_POS_FRAMES) - scene_start
                    })
                    scene_start = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000
            
            prev_frame = frame
        
        cap.release()
        return scenes
```

### 2. ì˜ë¯¸ì  ì„ë² ë”© ìƒì„±
```python
class SemanticEmbedder:
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def create_scene_embedding(self, scene_description):
        """ì¥ë©´ ì„¤ëª…ì„ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embedding = self.embedding_model.encode(scene_description)
        return embedding.tolist()
    
    def create_query_embedding(self, query):
        """ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embedding = self.embedding_model.encode(query)
        return embedding.tolist()
    
    def calculate_similarity(self, embedding1, embedding2):
        """ë‘ ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return cosine_similarity([embedding1], [embedding2])[0][0]
```

### 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„
```python
class HybridSearchEngine:
    def __init__(self):
        self.semantic_searcher = SemanticSearcher()
        self.keyword_searcher = KeywordSearcher()
        self.metadata_searcher = MetadataSearcher()
    
    def search(self, query, search_type='hybrid'):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        results = []
        
        if search_type in ['semantic', 'hybrid']:
            semantic_results = self.semantic_searcher.search(query)
            results.extend(semantic_results)
        
        if search_type in ['keyword', 'hybrid']:
            keyword_results = self.keyword_searcher.search(query)
            results.extend(keyword_results)
        
        if search_type in ['metadata', 'hybrid']:
            metadata_results = self.metadata_searcher.search(query)
            results.extend(metadata_results)
        
        # ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        return self.merge_and_rank_results(results)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ

### 1. ìºì‹± ì „ëµ
- ì¥ë©´ë³„ ì„ë² ë”© ìºì‹±
- ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
- LLM ì‘ë‹µ ìºì‹±

### 2. ì¸ë±ì‹± ìµœì í™”
- ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™œìš© (Pinecone, Weaviate)
- ë©”íƒ€ë°ì´í„° ì¸ë±ì‹±
- ì‹œê°„ ê¸°ë°˜ ì¸ë±ì‹±

### 3. ë°°ì¹˜ ì²˜ë¦¬
- ë¹„ë””ì˜¤ ë¶„ì„ ë°°ì¹˜ ì²˜ë¦¬
- ì„ë² ë”© ìƒì„± ë°°ì¹˜ ì²˜ë¦¬
- ê²€ìƒ‰ ê²°ê³¼ ì‚¬ì „ ê³„ì‚°

## ğŸ¯ ì„±ê³µ ì§€í‘œ

### ì •ëŸ‰ì  ì§€í‘œ
- ê²€ìƒ‰ ì •í™•ë„: 90% ì´ìƒ
- ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„: 2ì´ˆ ì´ë‚´
- ì‚¬ìš©ì ë§Œì¡±ë„: 4.5/5.0 ì´ìƒ

### ì •ì„±ì  ì§€í‘œ
- ìì—°ì–´ ì¿¼ë¦¬ ì´í•´ë„
- ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„±
- ì‚¬ìš©ì ê²½í—˜ ê°œì„ ë„

ì´ ë°œì „ ê³„íšì„ í†µí•´ í˜„ì¬ì˜ ì‹¤ì‹œê°„ ê´€ì œ ì‹œìŠ¤í…œì„ LLM ê¸°ë°˜ì˜ ì§€ëŠ¥ì ì¸ ë¹„ë””ì˜¤ ê²€ìƒ‰ ë° ë¶„ì„ ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ê°•ë ¥í•œ ê¸°ë°˜ ìœ„ì— ì˜ë¯¸ì  ì´í•´ì™€ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ë”ìš± ì •í™•í•˜ê³  ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆê²Œ ë  ê²ƒì…ë‹ˆë‹¤.
