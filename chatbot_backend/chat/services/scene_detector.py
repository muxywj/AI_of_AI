# chat/services/scene_detector.py - ì¥ë©´ ê°ì§€ ì„œë¹„ìŠ¤
import cv2
import numpy as np
import os
import logging
from typing import List, Dict, Tuple
from django.conf import settings
from ..models import Video, VideoScene, SceneAnalysis

logger = logging.getLogger(__name__)

class SceneDetector:
    """ë¹„ë””ì˜¤ ì¥ë©´ ê°ì§€ ë° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scene_threshold = 0.3  # ì¥ë©´ ë³€í™” ì„ê³„ê°’
        self.min_scene_duration = 2.0  # ìµœì†Œ ì¥ë©´ ì§€ì† ì‹œê°„ (ì´ˆ)
        self.max_scene_duration = 60.0  # ìµœëŒ€ ì¥ë©´ ì§€ì† ì‹œê°„ (ì´ˆ)
        
    def detect_scenes(self, video_path: str) -> List[Dict]:
        """ë¹„ë””ì˜¤ì—ì„œ ì¥ë©´ ë³€í™”ë¥¼ ê°ì§€í•˜ê³  ì¥ë©´ ì •ë³´ë¥¼ ë°˜í™˜"""
        try:
            logger.info(f"ğŸ¬ ì¥ë©´ ê°ì§€ ì‹œì‘: {video_path}")
            
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise Exception("ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´: {total_frames}í”„ë ˆì„, {fps:.1f}fps, {duration:.1f}ì´ˆ")
            
            # ì¥ë©´ ë³€í™” ê°ì§€
            scene_changes = self._detect_scene_changes(cap, fps)
            
            # ì¥ë©´ ì •ë³´ ìƒì„±
            scenes = self._create_scene_info(scene_changes, fps, duration)
            
            cap.release()
            
            logger.info(f"âœ… ì¥ë©´ ê°ì§€ ì™„ë£Œ: {len(scenes)}ê°œ ì¥ë©´")
            return scenes
            
        except Exception as e:
            logger.error(f"âŒ ì¥ë©´ ê°ì§€ ì‹¤íŒ¨: {e}")
            raise
    
    def _detect_scene_changes(self, cap: cv2.VideoCapture, fps: float) -> List[int]:
        """ì¥ë©´ ë³€í™” ì§€ì ì„ ê°ì§€"""
        scene_changes = [0]  # ì²« ë²ˆì§¸ ì¥ë©´ ì‹œì‘
        prev_frame = None
        frame_count = 0
        
        # ìƒ˜í”Œë§ ê°„ê²© ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
        sample_interval = max(1, int(fps / 2))  # ì´ˆë‹¹ 2í”„ë ˆì„ ìƒ˜í”Œë§
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ìƒ˜í”Œë§
            if frame_count % sample_interval == 0:
                if prev_frame is not None:
                    # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì¥ë©´ ë³€í™” ê°ì§€
                    hist_diff = self._calculate_histogram_difference(prev_frame, frame)
                    
                    if hist_diff < self.scene_threshold:
                        scene_changes.append(frame_count)
                        logger.debug(f"ì¥ë©´ ë³€í™” ê°ì§€: í”„ë ˆì„ {frame_count} (ì°¨ì´: {hist_diff:.3f})")
                
                prev_frame = frame.copy()
            
            frame_count += 1
        
        # ë§ˆì§€ë§‰ í”„ë ˆì„ ì¶”ê°€
        scene_changes.append(frame_count - 1)
        
        return scene_changes
    
    def _calculate_histogram_difference(self, frame1: np.ndarray, frame2: np.ndarray) -> float:
        """ë‘ í”„ë ˆì„ ê°„ì˜ íˆìŠ¤í† ê·¸ë¨ ì°¨ì´ ê³„ì‚°"""
        try:
            # HSV ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
            hsv1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2HSV)
            hsv2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2HSV)
            
            # íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
            hist1 = cv2.calcHist([hsv1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            hist2 = cv2.calcHist([hsv2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            
            # íˆìŠ¤í† ê·¸ë¨ ì •ê·œí™”
            cv2.normalize(hist1, hist1)
            cv2.normalize(hist2, hist2)
            
            # ìƒê´€ê´€ê³„ ê³„ì‚°
            correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
            
            return correlation
            
        except Exception as e:
            logger.warning(f"íˆìŠ¤í† ê·¸ë¨ ì°¨ì´ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 1.0  # ì˜¤ë¥˜ ì‹œ ë³€í™” ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
    
    def _create_scene_info(self, scene_changes: List[int], fps: float, duration: float) -> List[Dict]:
        """ì¥ë©´ ë³€í™” ì§€ì ì„ ê¸°ë°˜ìœ¼ë¡œ ì¥ë©´ ì •ë³´ ìƒì„±"""
        scenes = []
        
        for i in range(len(scene_changes) - 1):
            start_frame = scene_changes[i]
            end_frame = scene_changes[i + 1]
            
            start_time = start_frame / fps
            end_time = end_frame / fps
            scene_duration = end_time - start_time
            
            # ìµœì†Œ/ìµœëŒ€ ì§€ì† ì‹œê°„ í•„í„°ë§
            if scene_duration < self.min_scene_duration:
                continue
            
            if scene_duration > self.max_scene_duration:
                # ê¸´ ì¥ë©´ì„ ì—¬ëŸ¬ ê°œë¡œ ë¶„í• 
                sub_scenes = self._split_long_scene(start_time, end_time, self.max_scene_duration)
                scenes.extend(sub_scenes)
            else:
                scenes.append({
                    'scene_id': len(scenes) + 1,
                    'start_timestamp': start_time,
                    'end_timestamp': end_time,
                    'duration': scene_duration,
                    'start_frame': start_frame,
                    'end_frame': end_frame,
                    'frame_count': end_frame - start_frame
                })
        
        return scenes
    
    def _split_long_scene(self, start_time: float, end_time: float, max_duration: float) -> List[Dict]:
        """ê¸´ ì¥ë©´ì„ ì—¬ëŸ¬ ê°œì˜ ì§§ì€ ì¥ë©´ìœ¼ë¡œ ë¶„í• """
        scenes = []
        current_start = start_time
        scene_id = 1
        
        while current_start < end_time:
            current_end = min(current_start + max_duration, end_time)
            duration = current_end - current_start
            
            if duration >= self.min_scene_duration:
                scenes.append({
                    'scene_id': scene_id,
                    'start_timestamp': current_start,
                    'end_timestamp': current_end,
                    'duration': duration,
                    'start_frame': int(current_start * 30),  # ê°€ì •: 30fps
                    'end_frame': int(current_end * 30),
                    'frame_count': int(duration * 30)
                })
                scene_id += 1
            
            current_start = current_end
        
        return scenes

class SceneAnalyzer:
    """ì¥ë©´ ë¶„ì„ í´ë˜ìŠ¤ - LLM í†µí•©"""
    
    def __init__(self):
        self.scene_detector = SceneDetector()
        
    def analyze_video_scenes(self, video_id: int) -> List[Dict]:
        """ë¹„ë””ì˜¤ì˜ ëª¨ë“  ì¥ë©´ì„ ë¶„ì„í•˜ê³  DBì— ì €ì¥"""
        try:
            logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ ì¥ë©´ ë¶„ì„ ì‹œì‘: {video_id}")
            
            # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            video = Video.objects.get(id=video_id)
            video_path = os.path.join(settings.MEDIA_ROOT, video.file_path)
            
            if not os.path.exists(video_path):
                raise Exception(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            
            # ì¥ë©´ ê°ì§€
            scenes = self.scene_detector.detect_scenes(video_path)
            
            # ê° ì¥ë©´ ë¶„ì„ ë° ì €ì¥
            analyzed_scenes = []
            for scene_data in scenes:
                analyzed_scene = self._analyze_and_save_scene(video, scene_data, video_path)
                analyzed_scenes.append(analyzed_scene)
            
            logger.info(f"âœ… ë¹„ë””ì˜¤ ì¥ë©´ ë¶„ì„ ì™„ë£Œ: {len(analyzed_scenes)}ê°œ ì¥ë©´")
            return analyzed_scenes
            
        except Exception as e:
            logger.error(f"âŒ ë¹„ë””ì˜¤ ì¥ë©´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    def _analyze_and_save_scene(self, video: Video, scene_data: Dict, video_path: str) -> Dict:
        """ê°œë³„ ì¥ë©´ì„ ë¶„ì„í•˜ê³  DBì— ì €ì¥"""
        try:
            # VideoScene ëª¨ë¸ ìƒì„±
            scene = VideoScene.objects.create(
                video=video,
                scene_id=scene_data['scene_id'],
                start_timestamp=scene_data['start_timestamp'],
                end_timestamp=scene_data['end_timestamp'],
                duration=scene_data['duration']
            )
            
            # ì¥ë©´ í”„ë ˆì„ ë¶„ì„
            scene_analysis = self._analyze_scene_frames(scene_data, video_path)
            
            # SceneAnalysis ëª¨ë¸ ìƒì„±
            analysis = SceneAnalysis.objects.create(
                scene=scene,
                detected_persons=scene_analysis.get('detected_persons', []),
                detected_objects=scene_analysis.get('detected_objects', []),
                person_count=scene_analysis.get('person_count', 0),
                object_count=scene_analysis.get('object_count', 0),
                activity_type=scene_analysis.get('activity_type', ''),
                activity_intensity=scene_analysis.get('activity_intensity', ''),
                brightness_level=scene_analysis.get('brightness_level', 0.0),
                contrast_level=scene_analysis.get('contrast_level', 0.0),
                sharpness_level=scene_analysis.get('sharpness_level', 0.0)
            )
            
            # ì¥ë©´ ì •ë³´ ì—…ë°ì´íŠ¸
            scene.scene_type = scene_analysis.get('scene_type', '')
            scene.dominant_objects = scene_analysis.get('dominant_objects', [])
            scene.dominant_colors = scene_analysis.get('dominant_colors', [])
            scene.weather_condition = scene_analysis.get('weather_condition', '')
            scene.time_of_day = scene_analysis.get('time_of_day', '')
            scene.lighting_condition = scene_analysis.get('lighting_condition', '')
            scene.quality_score = scene_analysis.get('quality_score', 0.0)
            scene.confidence_score = scene_analysis.get('confidence_score', 0.0)
            scene.save()
            
            return {
                'scene_id': scene.scene_id,
                'start_timestamp': scene.start_timestamp,
                'end_timestamp': scene.end_timestamp,
                'duration': scene.duration,
                'analysis': scene_analysis
            }
            
        except Exception as e:
            logger.error(f"âŒ ì¥ë©´ ë¶„ì„ ë° ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _analyze_scene_frames(self, scene_data: Dict, video_path: str) -> Dict:
        """ì¥ë©´ì˜ í”„ë ˆì„ë“¤ì„ ë¶„ì„í•˜ì—¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # ì¥ë©´ì˜ ì¤‘ê°„ í”„ë ˆì„ë“¤ ìƒ˜í”Œë§
            start_frame = scene_data['start_frame']
            end_frame = scene_data['end_frame']
            sample_frames = self._sample_scene_frames(cap, start_frame, end_frame, fps)
            
            cap.release()
            
            # í”„ë ˆì„ ë¶„ì„
            analysis_result = self._analyze_frame_samples(sample_frames)
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ ì¥ë©´ í”„ë ˆì„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _sample_scene_frames(self, cap: cv2.VideoCapture, start_frame: int, end_frame: int, fps: float) -> List[np.ndarray]:
        """ì¥ë©´ì—ì„œ ëŒ€í‘œ í”„ë ˆì„ë“¤ì„ ìƒ˜í”Œë§"""
        frames = []
        frame_count = end_frame - start_frame
        
        # ìƒ˜í”Œë§ ì „ëµ: ì‹œì‘, ì¤‘ê°„, ë í”„ë ˆì„
        if frame_count <= 3:
            sample_indices = list(range(frame_count))
        else:
            sample_indices = [0, frame_count // 2, frame_count - 1]
        
        for idx in sample_indices:
            frame_number = start_frame + idx
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
        
        return frames
    
    def _analyze_frame_samples(self, frames: List[np.ndarray]) -> Dict:
        """ìƒ˜í”Œ í”„ë ˆì„ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¥ë©´ ì •ë³´ ì¶”ì¶œ"""
        if not frames:
            return {}
        
        # ê¸°ë³¸ ë¶„ì„
        brightness_levels = []
        contrast_levels = []
        sharpness_levels = []
        dominant_colors = []
        detected_objects = []
        detected_persons = []
        
        for frame in frames:
            # ë°ê¸° ë¶„ì„
            brightness = np.mean(frame)
            brightness_levels.append(brightness)
            
            # ëŒ€ë¹„ ë¶„ì„
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            contrast = np.std(gray)
            contrast_levels.append(contrast)
            
            # ì„ ëª…ë„ ë¶„ì„ (ë¼í”Œë¼ì‹œì•ˆ ë³€ìˆ˜)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_levels.append(laplacian_var)
            
            # ìƒ‰ìƒ ë¶„ì„
            colors = self._extract_dominant_colors(frame)
            dominant_colors.extend(colors)
            
            # ê°ì²´ ê°ì§€ (ê°„ë‹¨í•œ ë²„ì „)
            objects = self._detect_objects_simple(frame)
            detected_objects.extend(objects)
        
        # í‰ê· ê°’ ê³„ì‚°
        avg_brightness = np.mean(brightness_levels)
        avg_contrast = np.mean(contrast_levels)
        avg_sharpness = np.mean(sharpness_levels)
        
        # ì¥ë©´ ìœ í˜• ê²°ì •
        scene_type = self._determine_scene_type(avg_brightness, avg_contrast, dominant_colors)
        
        # ì‹œê°„ëŒ€ ë° ë‚ ì”¨ ì¶”ì •
        time_of_day = self._estimate_time_of_day(avg_brightness, dominant_colors)
        weather_condition = self._estimate_weather_condition(avg_brightness, dominant_colors)
        
        return {
            'scene_type': scene_type,
            'dominant_colors': list(set(dominant_colors)),
            'detected_objects': detected_objects,
            'detected_persons': detected_persons,
            'person_count': len(detected_persons),
            'object_count': len(detected_objects),
            'brightness_level': avg_brightness,
            'contrast_level': avg_contrast,
            'sharpness_level': avg_sharpness,
            'time_of_day': time_of_day,
            'weather_condition': weather_condition,
            'lighting_condition': 'bright' if avg_brightness > 150 else 'normal' if avg_brightness > 100 else 'dark',
            'quality_score': min(1.0, (avg_brightness / 255 + avg_contrast / 100 + avg_sharpness / 1000) / 3),
            'confidence_score': 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„
        }
    
    def _extract_dominant_colors(self, frame: np.ndarray) -> List[str]:
        """í”„ë ˆì„ì—ì„œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        try:
            # HSVë¡œ ë³€í™˜
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            
            # ìƒ‰ìƒ ë²”ìœ„ ì •ì˜
            color_ranges = {
                'red': [(0, 30, 30), (10, 255, 255)],
                'orange': [(10, 30, 30), (25, 255, 255)],
                'yellow': [(25, 30, 30), (40, 255, 255)],
                'green': [(40, 30, 30), (80, 255, 255)],
                'blue': [(80, 30, 30), (130, 255, 255)],
                'purple': [(130, 30, 30), (160, 255, 255)],
                'pink': [(160, 30, 30), (180, 255, 255)]
            }
            
            dominant_colors = []
            for color_name, (lower, upper) in color_ranges.items():
                mask = cv2.inRange(hsv, np.array(lower), np.array(upper))
                ratio = np.sum(mask > 0) / (frame.shape[0] * frame.shape[1])
                if ratio > 0.05:  # 5% ì´ìƒì´ë©´ ì£¼ìš” ìƒ‰ìƒìœ¼ë¡œ ì¸ì •
                    dominant_colors.append(color_name)
            
            return dominant_colors
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _detect_objects_simple(self, frame: np.ndarray) -> List[str]:
        """ê°„ë‹¨í•œ ê°ì²´ ê°ì§€ (ì‹¤ì œë¡œëŠ” YOLO ë“±ì„ ì‚¬ìš©í•´ì•¼ í•¨)"""
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê¸°ì¡´ì˜ YOLO ëª¨ë¸ì„ í™œìš©
        return ['person']  # ê¸°ë³¸ê°’
    
    def _determine_scene_type(self, brightness: float, contrast: float, colors: List[str]) -> str:
        """ì¥ë©´ ìœ í˜• ê²°ì •"""
        if brightness < 80:
            return 'indoor_dark'
        elif brightness > 180:
            return 'outdoor_bright'
        elif 'blue' in colors and brightness > 120:
            return 'outdoor_day'
        elif 'orange' in colors or 'yellow' in colors:
            return 'outdoor_sunset'
        else:
            return 'indoor_normal'
    
    def _estimate_time_of_day(self, brightness: float, colors: List[str]) -> str:
        """ì‹œê°„ëŒ€ ì¶”ì •"""
        if brightness < 60:
            return 'night'
        elif brightness < 100:
            return 'dawn' if 'blue' in colors else 'evening'
        elif brightness > 150:
            return 'afternoon'
        else:
            return 'morning' if 'orange' in colors else 'day'
    
    def _estimate_weather_condition(self, brightness: float, colors: List[str]) -> str:
        """ë‚ ì”¨ ì¡°ê±´ ì¶”ì •"""
        if brightness < 80 and 'gray' in colors:
            return 'rain'
        elif brightness > 180 and 'white' in colors:
            return 'snow'
        elif brightness > 120 and 'blue' in colors:
            return 'sunny'
        else:
            return 'cloudy'

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
scene_analyzer = SceneAnalyzer()
