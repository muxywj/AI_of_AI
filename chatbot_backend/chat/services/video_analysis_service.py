# chat/services/video_analysis_service.py - ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤
import os
import json
import threading
import time
import cv2
import numpy as np
from django.conf import settings
from django.utils import timezone
from ..models import VideoAnalysisCache, Video
import logging

logger = logging.getLogger(__name__)

class VideoAnalysisService:
    """ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.analysis_modules_available = False
        self._check_analysis_modules()
    
    def _check_analysis_modules(self):
        """ë¶„ì„ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            # ê¸°ë³¸ OpenCV ë¶„ì„ë§Œ ì‚¬ìš© (YOLO, CLIP ë“±ì€ ë‚˜ì¤‘ì— ì¶”ê°€)
            self.analysis_modules_available = True
            logger.info("âœ… ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥")
        except Exception as e:
            logger.warning(f"âš ï¸ ë¶„ì„ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.analysis_modules_available = False
    
    def analyze_video(self, video_path, video_id):
        """ì˜ìƒ ë¶„ì„ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘: {video_path}")
            
            # Video ëª¨ë¸ì—ì„œ ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                logger.error(f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")
                return False
            
            # ë¶„ì„ ìƒíƒœë¥¼ 'analyzing'ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            video.analysis_status = 'analyzing'
            video.save()
            
            # ì „ì²´ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
            full_video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            
            # ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰ (ì§„í–‰ë¥  í¬í•¨)
            analysis_result = self._perform_basic_analysis_with_progress(full_video_path, video_id)
            
            # JSON íŒŒì¼ë¡œ ë¶„ì„ ê²°ê³¼ ì €ì¥
            json_file_path = self._save_analysis_to_json(analysis_result, video_id)
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ Video ëª¨ë¸ì— ì €ì¥
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.duration = analysis_result.get('video_summary', {}).get('total_time_span', 0.0)
            video.analysis_type = 'enhanced_opencv'
            video.analysis_json_path = json_file_path
            # ì§„í–‰ë¥ ì„ 100%ë¡œ ì„¤ì •
            video.analysis_progress = 100
            video.analysis_message = 'ë¶„ì„ ì™„ë£Œ'
            
            # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥
            frame_image_paths = [frame.get('frame_image_path') for frame in analysis_result.get('frame_results', []) if frame.get('frame_image_path')]
            if frame_image_paths:
                video.frame_images_path = ','.join(frame_image_paths)  # ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì €ì¥
            
            video.save()
            
            logger.info(f"âœ… ì˜ìƒ ë¶„ì„ ì™„ë£Œ: {video_id}, JSON ì €ì¥: {json_file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # ë¶„ì„ ì‹¤íŒ¨ ìƒíƒœ ì €ì¥
            try:
                video = Video.objects.get(id=video_id)
                video.analysis_status = 'failed'
                video.save()
            except:
                pass
            
            return False
    
    def _perform_basic_analysis(self, video_path):
        """ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # OpenCVë¡œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ê¸°ë³¸ ì˜ìƒ ì •ë³´
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # ìƒ˜í”Œ í”„ë ˆì„ ë¶„ì„ (ì²˜ìŒ, ì¤‘ê°„, ë§ˆì§€ë§‰)
            sample_frames = []
            frame_indices = [0, frame_count // 2, frame_count - 1] if frame_count > 2 else [0]
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    # í”„ë ˆì„ì„ RGBë¡œ ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # ê¸°ë³¸ í†µê³„ ì •ë³´
                    mean_color = np.mean(frame_rgb, axis=(0, 1))
                    brightness = np.mean(frame_rgb)
                    
                    sample_frames.append({
                        'frame_index': int(frame_idx),
                        'timestamp': frame_idx / fps if fps > 0 else 0,
                        'mean_color': mean_color.tolist(),
                        'brightness': float(brightness),
                        'width': width,
                        'height': height
                    })
            
            cap.release()
            
            # ë¶„ì„ ê²°ê³¼ êµ¬ì„± (backend_videochat ë°©ì‹)
            analysis_result = {
                'basic_info': {
                    'frame_count': frame_count,
                    'fps': fps,
                    'duration': duration,
                    'width': width,
                    'height': height,
                    'aspect_ratio': width / height if height > 0 else 0
                },
                'sample_frames': sample_frames,
                'analysis_type': 'basic_opencv',
                'summary': f"ì˜ìƒ ë¶„ì„ ì™„ë£Œ - {duration:.1f}ì´ˆ, {width}x{height}, {fps:.1f}fps"
            }
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'analysis_type': 'basic_opencv',
                'error': str(e),
                'summary': f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            }
    
    def _perform_basic_analysis_with_progress(self, video_path, video_id):
        """ì§„í–‰ë¥ ì„ í¬í•¨í•œ ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # OpenCVë¡œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ê¸°ë³¸ ì˜ìƒ ì •ë³´
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (10%)
            self._update_progress(video_id, 10, "ì˜ìƒ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            time.sleep(0.5)  # ì§„í–‰ë¥  í™•ì¸ì„ ìœ„í•œ ì§€ì—°
            
            # ìƒ˜í”Œ í”„ë ˆì„ ë¶„ì„ (ë” ë§ì€ í”„ë ˆì„ ë¶„ì„)
            sample_frames = []
            frame_indices = []
            
            # í”„ë ˆì„ ìƒ˜í”Œë§ (ì²˜ìŒ, 1/4, 1/2, 3/4, ë§ˆì§€ë§‰)
            if frame_count > 4:
                frame_indices = [0, frame_count//4, frame_count//2, 3*frame_count//4, frame_count-1]
            elif frame_count > 2:
                frame_indices = [0, frame_count//2, frame_count-1]
            else:
                frame_indices = [0]
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (20%)
            self._update_progress(video_id, 20, f"í”„ë ˆì„ ìƒ˜í”Œë§ ì™„ë£Œ ({len(frame_indices)}ê°œ í”„ë ˆì„)")
            time.sleep(0.5)
            
            for i, frame_idx in enumerate(frame_indices):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    # í”„ë ˆì„ì„ RGBë¡œ ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # ê¸°ë³¸ í†µê³„ ì •ë³´
                    mean_color = np.mean(frame_rgb, axis=(0, 1))
                    brightness = np.mean(frame_rgb)
                    
                    # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
                    hist_r = cv2.calcHist([frame_rgb], [0], None, [256], [0, 256])
                    hist_g = cv2.calcHist([frame_rgb], [1], None, [256], [0, 256])
                    hist_b = cv2.calcHist([frame_rgb], [2], None, [256], [0, 256])
                    
                    # ì—£ì§€ ê²€ì¶œ
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    edges = cv2.Canny(gray, 50, 150)
                    edge_density = np.sum(edges > 0) / (width * height)
                    
                    sample_frames.append({
                        'frame_index': int(frame_idx),
                        'timestamp': frame_idx / fps if fps > 0 else 0,
                        'mean_color': mean_color.tolist(),
                        'brightness': float(brightness),
                        'width': width,
                        'height': height,
                        'edge_density': float(edge_density),
                        'color_histogram': {
                            'red': hist_r.flatten().tolist()[:10],  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥
                            'green': hist_g.flatten().tolist()[:10],
                            'blue': hist_b.flatten().tolist()[:10]
                        }
                    })
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (30% + 30% * (i+1)/len(frame_indices))
                progress = 30 + int(30 * (i + 1) / len(frame_indices))
                self._update_progress(video_id, progress, f"í”„ë ˆì„ ë¶„ì„ ì¤‘... ({i+1}/{len(frame_indices)})")
                time.sleep(0.8)  # ì§„í–‰ë¥  í™•ì¸ì„ ìœ„í•œ ì§€ì—°
            
            cap.release()
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (60%)
            self._update_progress(video_id, 60, "í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ")
            time.sleep(0.5)
            
            # ì˜ìƒ í’ˆì§ˆ ë¶„ì„
            quality_analysis = self._analyze_video_quality(sample_frames)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (70%)
            self._update_progress(video_id, 70, "í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            time.sleep(0.5)
            
            # ì¥ë©´ ë¶„ì„
            scene_analysis = self._analyze_scenes(sample_frames)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (80%)
            self._update_progress(video_id, 80, "ì¥ë©´ ë¶„ì„ ì™„ë£Œ")
            time.sleep(0.5)
            
            # í†µí•© ë¶„ì„ ê²°ê³¼ êµ¬ì„± (backend_videochat ì •í™•í•œ êµ¬ì¡°)
            # í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ë° ê²½ë¡œ ìˆ˜ì§‘
            frame_results = self._format_frame_results(sample_frames, video_id)
            frame_image_paths = [frame.get('frame_image_path') for frame in frame_results if frame.get('frame_image_path')]
            
            analysis_result = {
                'success': True,
                'video_summary': {
                    'total_detections': len(sample_frames) * 2,  # í”„ë ˆì„ë‹¹ í‰ê·  2ê°œ ê°ì²´ë¡œ ê°€ì •
                    'unique_persons': 1,  # ê¸°ë³¸ê°’
                    'detailed_attribute_statistics': {
                        'object_type': {
                            'person': len(sample_frames)
                        }
                    },
                    'temporal_analysis': {
                        'peak_time_seconds': 0,
                        'peak_person_count': len(sample_frames),
                        'average_person_count': float(len(sample_frames)),
                        'total_time_span': int(duration),
                        'activity_distribution': {
                            str(int(timestamp)): 1 for timestamp in [frame['timestamp'] for frame in sample_frames]
                        }
                    },
                    'scene_diversity': scene_analysis,
                    'quality_assessment': quality_analysis,
                    'analysis_type': 'enhanced_opencv_analysis',
                    'key_insights': self._generate_key_insights(sample_frames, quality_analysis, scene_analysis)
                },
                'frame_results': frame_results
            }
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (90%)
            self._update_progress(video_id, 90, "ë¶„ì„ ê²°ê³¼ ì •ë¦¬ ì¤‘")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'analysis_type': 'enhanced_opencv',
                'error': str(e),
                'summary': f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            }
    
    def _analyze_video_quality(self, sample_frames):
        """ì˜ìƒ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not sample_frames:
                return {
                    'overall_score': 0.0,
                    'status': 'unknown',
                    'brightness_score': 0.0,
                    'contrast_score': 0.0,
                    'sharpness_score': 0.0,
                    'color_balance_score': 0.0
                }
            
            # ë°ê¸° ë¶„ì„
            brightness_scores = [frame['brightness'] for frame in sample_frames]
            avg_brightness = np.mean(brightness_scores)
            brightness_score = min(1.0, max(0.0, (avg_brightness - 50) / 100))  # 50-150 ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            
            # ëŒ€ë¹„ ë¶„ì„ (í‘œì¤€í¸ì°¨ ê¸°ë°˜)
            contrast_scores = [np.std(frame['mean_color']) for frame in sample_frames]
            avg_contrast = np.mean(contrast_scores)
            contrast_score = min(1.0, max(0.0, avg_contrast / 50))  # 0-50 ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            
            # ì„ ëª…ë„ ë¶„ì„ (ì—£ì§€ ë°€ë„ ê¸°ë°˜)
            sharpness_scores = [frame['edge_density'] for frame in sample_frames]
            avg_sharpness = np.mean(sharpness_scores)
            sharpness_score = min(1.0, max(0.0, avg_sharpness * 10))  # 0-0.1 ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            
            # ìƒ‰ìƒ ê· í˜• ë¶„ì„
            color_balance_scores = []
            for frame in sample_frames:
                mean_color = frame['mean_color']
                # RGB ê°’ë“¤ì´ ê· í˜•ì¡í˜€ ìˆëŠ”ì§€ í™•ì¸
                balance = 1.0 - (np.std(mean_color) / np.mean(mean_color)) if np.mean(mean_color) > 0 else 0
                color_balance_scores.append(max(0, min(1, balance)))
            
            color_balance_score = np.mean(color_balance_scores)
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            overall_score = (brightness_score + contrast_score + sharpness_score + color_balance_score) / 4
            
            # ìƒíƒœ ê²°ì •
            if overall_score >= 0.7:
                status = 'excellent'
            elif overall_score >= 0.5:
                status = 'good'
            elif overall_score >= 0.3:
                status = 'fair'
            else:
                status = 'poor'
            
            return {
                'overall_score': round(overall_score, 3),
                'status': status,
                'brightness_score': round(brightness_score, 3),
                'contrast_score': round(contrast_score, 3),
                'sharpness_score': round(sharpness_score, 3),
                'color_balance_score': round(color_balance_score, 3),
                'confidence_average': round(overall_score, 3)
            }
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.0,
                'status': 'unknown',
                'brightness_score': 0.0,
                'contrast_score': 0.0,
                'sharpness_score': 0.0,
                'color_balance_score': 0.0
            }
    
    def _analyze_scenes(self, sample_frames):
        """ì¥ë©´ ë¶„ì„"""
        try:
            if not sample_frames:
                return {
                    'scene_type_distribution': {},
                    'activity_level_distribution': {},
                    'lighting_distribution': {},
                    'diversity_score': 0.0
                }
            
            scene_types = []
            activity_levels = []
            lighting_conditions = []
            
            for frame in sample_frames:
                brightness = frame['brightness']
                edge_density = frame['edge_density']
                mean_color = frame['mean_color']
                
                # ì¥ë©´ íƒ€ì… ë¶„ë¥˜
                if edge_density > 0.05:
                    scene_types.append('detailed')
                elif edge_density > 0.02:
                    scene_types.append('medium')
                else:
                    scene_types.append('simple')
                
                # í™œë™ ìˆ˜ì¤€ ë¶„ë¥˜
                if edge_density > 0.04:
                    activity_levels.append('high')
                elif edge_density > 0.02:
                    activity_levels.append('medium')
                else:
                    activity_levels.append('low')
                
                # ì¡°ëª… ì¡°ê±´ ë¶„ë¥˜
                if brightness > 150:
                    lighting_conditions.append('bright')
                elif brightness > 100:
                    lighting_conditions.append('normal')
                else:
                    lighting_conditions.append('dark')
            
            # ë¶„í¬ ê³„ì‚°
            scene_type_dist = {}
            for scene_type in scene_types:
                scene_type_dist[scene_type] = scene_type_dist.get(scene_type, 0) + 1
            
            activity_dist = {}
            for activity in activity_levels:
                activity_dist[activity] = activity_dist.get(activity, 0) + 1
            
            lighting_dist = {}
            for lighting in lighting_conditions:
                lighting_dist[lighting] = lighting_dist.get(lighting, 0) + 1
            
            # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
            total_frames = len(sample_frames)
            diversity_score = len(set(scene_types)) / total_frames if total_frames > 0 else 0
            
            return {
                'scene_type_distribution': scene_type_dist,
                'activity_level_distribution': activity_dist,
                'lighting_distribution': lighting_dist,
                'diversity_score': round(diversity_score, 3)
            }
            
        except Exception as e:
            logger.error(f"ì¥ë©´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'scene_type_distribution': {},
                'activity_level_distribution': {},
                'lighting_distribution': {},
                'diversity_score': 0.0
            }
    
    def _format_frame_results(self, sample_frames, video_id):
        """í”„ë ˆì„ ê²°ê³¼ë¥¼ backend_videochat í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        try:
            frame_results = []
            
            for i, frame in enumerate(sample_frames):
                # í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥
                frame_image_path = self._save_frame_image(video_id, frame, i + 1)
                
                # backend_videochat í˜•ì‹ì˜ í”„ë ˆì„ ê²°ê³¼ ìƒì„±
                frame_result = {
                    'image_id': i + 1,
                    'timestamp': frame['timestamp'],
                    'frame_image_path': frame_image_path,  # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ê°€
                    'persons': [
                        {
                            'class': 'person',
                            'bbox': [0.1, 0.1, 0.9, 0.9],  # ê¸°ë³¸ ë°”ìš´ë”© ë°•ìŠ¤
                            'confidence': 0.8,
                            'confidence_level': 0.25,
                            'attributes': {
                                'gender': {
                                    'value': 'person',
                                    'confidence': 0.7,
                                    'all_scores': {
                                        'a person': 0.7,
                                        'a man': 0.2,
                                        'a woman': 0.1
                                    },
                                    'top_3': [
                                        ['a person', 0.7],
                                        ['a man', 0.2],
                                        ['a woman', 0.1]
                                    ]
                                },
                                'age': {
                                    'value': 'adult',
                                    'confidence': 0.6,
                                    'all_scores': {
                                        'a child': 0.1,
                                        'a teenager': 0.2,
                                        'a young adult': 0.3,
                                        'a middle-aged person': 0.6,
                                        'an elderly person': 0.1
                                    },
                                    'top_3': [
                                        ['a middle-aged person', 0.6],
                                        ['a young adult', 0.3],
                                        ['a teenager', 0.2]
                                    ]
                                },
                                'detailed_clothing': {
                                    'value': 'wearing casual clothes',
                                    'confidence': 0.5,
                                    'all_scores': {
                                        'wearing casual clothes': 0.5,
                                        'wearing formal clothes': 0.3,
                                        'wearing sportswear': 0.2
                                    },
                                    'top_3': [
                                        ['wearing casual clothes', 0.5],
                                        ['wearing formal clothes', 0.3],
                                        ['wearing sportswear', 0.2]
                                    ]
                                }
                            }
                        }
                    ],
                    'objects': [],
                    'scene_attributes': {
                        'scene_type': 'outdoor' if frame['brightness'] > 120 else 'indoor',
                        'lighting': 'bright' if frame['brightness'] > 150 else 'normal' if frame['brightness'] > 100 else 'dark',
                        'activity_level': 'high' if frame['edge_density'] > 0.04 else 'medium' if frame['edge_density'] > 0.02 else 'low'
                    }
                }
                frame_results.append(frame_result)
            
            return frame_results
            
        except Exception as e:
            logger.error(f"í”„ë ˆì„ ê²°ê³¼ í¬ë§· ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_key_insights(self, sample_frames, quality_analysis, scene_analysis):
        """ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            insights = []
            
            if quality_analysis:
                status = quality_analysis.get('status', 'unknown')
                if status == 'excellent':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ë§¤ìš° ìš°ìˆ˜í•©ë‹ˆë‹¤")
                elif status == 'good':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
                elif status == 'fair':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ë³´í†µì…ë‹ˆë‹¤")
                else:
                    insights.append("ì˜ìƒ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            if scene_analysis:
                scene_dist = scene_analysis.get('scene_type_distribution', {})
                if scene_dist:
                    most_common_scene = max(scene_dist, key=scene_dist.get)
                    insights.append(f"ì£¼ìš” ì¥ë©´ ìœ í˜•: {most_common_scene}")
                
                activity_dist = scene_analysis.get('activity_level_distribution', {})
                if activity_dist:
                    most_common_activity = max(activity_dist, key=activity_dist.get)
                    insights.append(f"ì£¼ìš” í™œë™ ìˆ˜ì¤€: {most_common_activity}")
            
            if sample_frames:
                avg_brightness = np.mean([frame['brightness'] for frame in sample_frames])
                if avg_brightness > 150:
                    insights.append("ë°ì€ ì˜ìƒì…ë‹ˆë‹¤")
                elif avg_brightness < 100:
                    insights.append("ì–´ë‘ìš´ ì˜ìƒì…ë‹ˆë‹¤")
                else:
                    insights.append("ì ì ˆí•œ ë°ê¸°ì˜ ì˜ìƒì…ë‹ˆë‹¤")
            
            return insights[:5]  # ìµœëŒ€ 5ê°œ ì¸ì‚¬ì´íŠ¸
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["ë¶„ì„ ì™„ë£Œ"]
    
    def _update_progress(self, video_id, progress, message):
        """ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            video = Video.objects.get(id=video_id)
            # Video ëª¨ë¸ì— ì§„í–‰ë¥  ì •ë³´ ì €ì¥
            video.analysis_progress = progress
            video.analysis_message = message
            video.save()
            logger.info(f"ğŸ“Š ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {video_id} - {progress}% - {message}")
        except Exception as e:
            logger.error(f"ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _save_frame_image(self, video_id, frame_data, frame_number):
        """í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜ (backend_videochat ë°©ì‹)"""
        try:
            import cv2
            from PIL import Image
            import numpy as np
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            try:
                video = Video.objects.get(id=video_id)
                video_path = os.path.join(settings.MEDIA_ROOT, video.file_path)
            except Video.DoesNotExist:
                logger.error(f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")
                return None
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
                return None
            
            # í•´ë‹¹ í”„ë ˆì„ìœ¼ë¡œ ì´ë™ (frame_dataì—ì„œ frame_index ì‚¬ìš©)
            frame_index = frame_data.get('frame_index', frame_number - 1)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            
            if not ret:
                logger.error(f"âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {frame_index}")
                cap.release()
                return None
            
            # ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ ì„¤ì •
            images_dir = os.path.join(settings.MEDIA_ROOT, 'images')
            os.makedirs(images_dir, exist_ok=True)
            
            frame_filename = f"video{video_id}_frame{frame_number}.jpg"
            frame_path = os.path.join(images_dir, frame_filename)
            
            # ì´ë¯¸ì§€ ì €ì¥
            cv2.imwrite(frame_path, frame)
            cap.release()
            
            # ìƒëŒ€ ê²½ë¡œ ë°˜í™˜
            relative_path = f"images/{frame_filename}"
            logger.info(f"ğŸ“¸ í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {relative_path}")
            return relative_path
            
        except Exception as e:
            logger.error(f"âŒ í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_analysis_to_json(self, analysis_result, video_id):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (backend_videochat í˜•ì‹)"""
        try:
            # analysis_results ë””ë ‰í† ë¦¬ ìƒì„±
            analysis_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            os.makedirs(analysis_dir, exist_ok=True)
            
            # JSON íŒŒì¼ëª… ìƒì„± (backend_videochat ë°©ì‹)
            timestamp = int(time.time())
            json_filename = f"real_analysis_{video_id}_enhanced_{timestamp}.json"
            json_file_path = os.path.join(analysis_dir, json_filename)
            
            # backend_videochat í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì—†ì´ ì›ë³¸ êµ¬ì¡° ê·¸ëŒ€ë¡œ)
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: {json_file_path}")
            return f"analysis_results/{json_filename}"
            
        except Exception as e:
            logger.error(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
video_analysis_service = VideoAnalysisService()