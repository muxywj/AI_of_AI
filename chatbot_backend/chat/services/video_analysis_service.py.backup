# chat/services/video_analysis_service.py - ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤
import os
import json
import threading
import time
import cv2
import numpy as np
from collections import Counter
from django.conf import settings
from django.utils import timezone
from ..models import VideoAnalysisCache, Video
import logging

# YOLO ê°ì²´ íƒì§€
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
    print("âœ… YOLO ë¡œë“œ ì„±ê³µ")
except ImportError:
    YOLO_AVAILABLE = False
    print("âš ï¸ YOLO ë¯¸ì„¤ì¹˜ - ê°ì²´ ê°ì§€ ê¸°ëŠ¥ ì œí•œ")

logger = logging.getLogger(__name__)

class VideoAnalysisService:
    """ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.analysis_modules_available = True  # ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
        
        # YOLO ëª¨ë¸ ì´ˆê¸°í™”
        self.yolo_model = None
        if YOLO_AVAILABLE:
            try:
                self.yolo_model = YOLO('yolov8n.pt')  # nano ëª¨ë¸ ì‚¬ìš© (ê°€ë²¼ì›€)
                logger.info("âœ… YOLO ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ YOLO ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.yolo_model = None
        
        logger.info("âœ… ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _detect_objects_with_yolo(self, frame_rgb):
        """YOLOë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ê°ì²´ íƒì§€"""
        if not self.yolo_model:
            return []
        
        try:
            # YOLO ì¶”ë¡  ì‹¤í–‰
            results = self.yolo_model(frame_rgb, verbose=False)
            
            detections = []
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ (xyxy í˜•ì‹)
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = float(box.conf[0].cpu().numpy())
                        class_id = int(box.cls[0].cpu().numpy())
                        class_name = self.yolo_model.names[class_id]
                        
                        # ì‚¬ëŒë§Œ ê°ì§€ (class_id = 0)
                        if class_name == 'person' and confidence > 0.5:
                            # ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
                            height, width = frame_rgb.shape[:2]
                            bbox = [
                                float(x1 / width),   # x1
                                float(y1 / height),  # y1
                                float(x2 / width),   # x2
                                float(y2 / height)   # y2
                            ]
                            
                            detection = {
                                'class': 'person',
                                'bbox': bbox,
                                'confidence': confidence,
                                'confidence_level': min(confidence * 1.25, 1.0),  # ì‹ ë¢°ë„ ë ˆë²¨
                                'attributes': {
                                    'gender': {
                                        'value': 'person',
                                        'confidence': confidence * 0.8,
                                        'all_scores': {
                                            'a person': confidence * 0.8,
                                            'a man': confidence * 0.1,
                                            'a woman': confidence * 0.1
                                        },
                                        'top_3': [
                                            ['a person', confidence * 0.8],
                                            ['a man', confidence * 0.1],
                                            ['a woman', confidence * 0.1]
                                        ]
                                    },
                                    'age': {
                                        'value': 'adult',
                                        'confidence': confidence * 0.6,
                                        'all_scores': {
                                            'a child': confidence * 0.1,
                                            'a teenager': confidence * 0.2,
                                            'a young adult': confidence * 0.3,
                                            'a middle-aged person': confidence * 0.6,
                                            'an elderly person': confidence * 0.1
                                        },
                                        'top_3': [
                                            ['a middle-aged person', confidence * 0.6],
                                            ['a young adult', confidence * 0.3],
                                            ['a teenager', confidence * 0.2]
                                        ]
                                    }
                                }
                            }
                            detections.append(detection)
                            
            logger.info(f"ğŸ¯ YOLO ê°ì²´ íƒì§€ ì™„ë£Œ: {len(detections)}ëª… ê°ì§€")
            return detections
            
        except Exception as e:
            logger.warning(f"âš ï¸ YOLO ê°ì²´ íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _load_frame_for_analysis(self, frame_image_path):
        """ë¶„ì„ì„ ìœ„í•´ í”„ë ˆì„ ì´ë¯¸ì§€ ë¡œë“œ"""
        try:
            if not frame_image_path:
                return None
            
            full_path = os.path.join(settings.MEDIA_ROOT, frame_image_path)
            if not os.path.exists(full_path):
                logger.warning(f"í”„ë ˆì„ ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {full_path}")
                return None
            
            # OpenCVë¡œ ì´ë¯¸ì§€ ë¡œë“œ (RGB í˜•ì‹)
            frame_bgr = cv2.imread(full_path)
            if frame_bgr is None:
                logger.warning(f"í”„ë ˆì„ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {full_path}")
                return None
            
            # BGRì„ RGBë¡œ ë³€í™˜
            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
            return frame_rgb
            
        except Exception as e:
            logger.warning(f"í”„ë ˆì„ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_dominant_color(self, image_region):
        """ì˜ì—­ì˜ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (HSV ê¸°ë°˜)"""
        try:
            # HSVë¡œ ë³€í™˜í•˜ì—¬ ìƒ‰ìƒ ë¶„ì„
            hsv = cv2.cvtColor(image_region, cv2.COLOR_BGR2HSV)
            h_mean = np.mean(hsv[:, :, 0])
            
            # ìƒ‰ìƒ ë²”ìœ„ë³„ ë¶„ë¥˜ (ë” ì„¸ë¶„í™”)
            if h_mean < 10 or h_mean > 170:
                return 'red'
            elif h_mean < 25:
                return 'orange'
            elif h_mean < 40:
                return 'yellow'
            elif h_mean < 80:
                return 'green'
            elif h_mean < 130:
                return 'blue'
            elif h_mean < 160:
                return 'purple'
            else:
                return 'pink'
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    def _analyze_frame_colors(self, frame_rgb):
        """í”„ë ˆì„ì˜ ì£¼ìš” ìƒ‰ìƒ ë¶„ì„"""
        try:
            # HSVë¡œ ë³€í™˜
            hsv = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2HSV)
            
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            dominant_colors = []
            
            # ìƒ‰ìƒë³„ ë§ˆìŠ¤í¬ ìƒì„± ë° ë¶„ì„
            color_ranges = {
                'red': [(0, 50, 50), (10, 255, 255)],  # ë¹¨ê°„ìƒ‰ ë²”ìœ„
                'orange': [(10, 50, 50), (25, 255, 255)],  # ì£¼í™©ìƒ‰ ë²”ìœ„
                'yellow': [(25, 50, 50), (40, 255, 255)],  # ë…¸ë€ìƒ‰ ë²”ìœ„
                'green': [(40, 50, 50), (80, 255, 255)],  # ì´ˆë¡ìƒ‰ ë²”ìœ„
                'blue': [(80, 50, 50), (130, 255, 255)],  # íŒŒë€ìƒ‰ ë²”ìœ„
                'purple': [(130, 50, 50), (160, 255, 255)],  # ë³´ë¼ìƒ‰ ë²”ìœ„
                'pink': [(160, 30, 30), (180, 255, 255), (0, 30, 30), (10, 255, 255)]  # ë¶„í™ìƒ‰ ë²”ìœ„ (ë” ë„“ì€ ë²”ìœ„)
            }
            
            for color_name, color_range in color_ranges.items():
                # ë¶„í™ìƒ‰ì˜ ê²½ìš° ë‘ ê°œì˜ ë²”ìœ„ ì‚¬ìš©
                if color_name == 'pink':
                    # ì²« ë²ˆì§¸ ë²”ìœ„ (160-180)
                    mask1 = cv2.inRange(hsv, np.array(color_range[0]), np.array(color_range[1]))
                    # ë‘ ë²ˆì§¸ ë²”ìœ„ (0-10, ë” ë°ì€ ë¶„í™ìƒ‰)
                    mask2 = cv2.inRange(hsv, np.array(color_range[2]), np.array(color_range[3]))
                    mask = cv2.bitwise_or(mask1, mask2)
                else:
                    mask = cv2.inRange(hsv, np.array(color_range[0]), np.array(color_range[1]))
                
                # í•´ë‹¹ ìƒ‰ìƒì˜ í”½ì…€ ë¹„ìœ¨ ê³„ì‚°
                color_ratio = np.sum(mask > 0) / (frame_rgb.shape[0] * frame_rgb.shape[1])
                
                # ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” ë§ì€ ìƒ‰ìƒ ê°ì§€ (2% ì´ìƒ)
                if color_ratio > 0.02:
                    dominant_colors.append({
                        'color': color_name,
                        'ratio': float(color_ratio),
                        'confidence': min(color_ratio * 2, 1.0)  # ë¹„ìœ¨ì— ë”°ë¥¸ ì‹ ë¢°ë„
                    })
            
            # ë¹„ìœ¨ ìˆœìœ¼ë¡œ ì •ë ¬
            dominant_colors.sort(key=lambda x: x['ratio'], reverse=True)
            
            return dominant_colors[:3]  # ìƒìœ„ 3ê°œ ìƒ‰ìƒë§Œ ë°˜í™˜
            
        except Exception as e:
            logger.warning(f"í”„ë ˆì„ ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_video(self, video_path, video_id):
        """ì˜ìƒ ë¶„ì„ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘: {video_path}")
            
            # Video ëª¨ë¸ì—ì„œ ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                logger.error(f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")
                return False
            
            # ë¶„ì„ ìƒíƒœë¥¼ 'analyzing'ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            video.analysis_status = 'analyzing'
            video.save()
            
            # ì „ì²´ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
            full_video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            
            # ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰ (ì§„í–‰ë¥  í¬í•¨)
            analysis_result = self._perform_basic_analysis_with_progress(full_video_path, video_id)
            
            # JSON íŒŒì¼ë¡œ ë¶„ì„ ê²°ê³¼ ì €ì¥
            json_file_path = self._save_analysis_to_json(analysis_result, video_id)
            
            if not json_file_path:
                raise Exception("JSON íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ Video ëª¨ë¸ì— ì €ì¥
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.duration = analysis_result.get('video_summary', {}).get('total_time_span', 0.0)
            video.analysis_type = 'enhanced_opencv'
            video.analysis_json_path = json_file_path
            # ì§„í–‰ë¥ ì„ 100%ë¡œ ì„¤ì •
            video.analysis_progress = 100
            video.analysis_message = 'ë¶„ì„ ì™„ë£Œ'
            
            # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥
            frame_image_paths = [frame.get('frame_image_path') for frame in analysis_result.get('frame_results', []) if frame.get('frame_image_path')]
            if frame_image_paths:
                video.frame_images_path = ','.join(frame_image_paths)  # ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì €ì¥
            
            # ì•ˆì „í•˜ê²Œ ì €ì¥
            try:
                video.save()
                logger.info(f"âœ… ì˜ìƒ ë¶„ì„ ì™„ë£Œ: {video_id}, JSON ì €ì¥: {json_file_path}")
                logger.info(f"âœ… Video ëª¨ë¸ ì €ì¥ ì™„ë£Œ: analysis_json_path = {video.analysis_json_path}")
            except Exception as save_error:
                logger.error(f"âŒ Video ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                raise Exception(f"Video ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {save_error}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            
            # êµ¬ì²´ì ì¸ ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
            error_type = "unknown"
            if "FileNotFoundError" in str(type(e)):
                error_type = "file_not_found"
            elif "PermissionError" in str(type(e)):
                error_type = "permission_error"
            elif "MemoryError" in str(type(e)):
                error_type = "memory_error"
            elif "TimeoutError" in str(type(e)):
                error_type = "timeout_error"
            
            # Video ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸
            try:
                video = Video.objects.get(id=video_id)
                video.analysis_status = 'failed'
                video.analysis_progress = 0
                video.analysis_message = f'ë¶„ì„ ì‹¤íŒ¨: {error_type}'
                video.save()
                logger.info(f"âœ… Video ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸: {video_id} - failed")
            except Exception as update_error:
                logger.error(f"âŒ Video ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_error}")
            
            return False
    
    def _perform_basic_analysis_with_progress(self, video_path, video_id):
        """ì§„í–‰ë¥ ì„ í¬í•¨í•œ ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ì‹œì‘
            self._update_progress(video_id, 10, "ì˜ìƒ íŒŒì¼ì„ ì—´ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # OpenCVë¡œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(video_path):
                raise Exception(f"ì˜ìƒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_path}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(video_path)
            if file_size == 0:
                raise Exception("ì˜ìƒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ê¸°ë³¸ ì˜ìƒ ì •ë³´
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            logger.info(f"ğŸ“¹ ì˜ìƒ ì •ë³´: {frame_count}í”„ë ˆì„, {fps:.2f}FPS, {duration:.2f}ì´ˆ, {width}x{height}")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            self._update_progress(video_id, 30, "í”„ë ˆì„ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # ìƒ˜í”Œ í”„ë ˆì„ ë¶„ì„ (ì²˜ìŒ, ì¤‘ê°„, ë§ˆì§€ë§‰)
            sample_frames = []
            frame_indices = [0, frame_count // 2, frame_count - 1] if frame_count > 2 else [0]
            
            for i, frame_idx in enumerate(frame_indices):
                try:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                    if not ret:
                        logger.warning(f"âš ï¸ í”„ë ˆì„ {frame_idx} ì½ê¸° ì‹¤íŒ¨")
                        continue
                    
                    # í”„ë ˆì„ì„ RGBë¡œ ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # ê¸°ë³¸ í†µê³„ ì •ë³´
                    mean_color = np.mean(frame_rgb, axis=(0, 1))
                    brightness = np.mean(frame_rgb)
                    
                    # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
                    try:
                    hist_r = cv2.calcHist([frame_rgb], [0], None, [256], [0, 256])
                    hist_g = cv2.calcHist([frame_rgb], [1], None, [256], [0, 256])
                    hist_b = cv2.calcHist([frame_rgb], [2], None, [256], [0, 256])
                    except Exception as hist_error:
                        logger.warning(f"íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ ì‹¤íŒ¨: {hist_error}")
                        hist_r = hist_g = hist_b = np.zeros(256)
                    
                    # ì—£ì§€ ê²€ì¶œ (ì•ˆì „í•˜ê²Œ)
                    try:
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    edges = cv2.Canny(gray, 50, 150)
                    edge_density = np.sum(edges > 0) / (width * height)
                    except Exception as edge_error:
                        logger.warning(f"ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {edge_error}")
                        edge_density = 0.0
                
                    # ìƒ‰ìƒ ë¶„ì„ ì¶”ê°€
                    dominant_colors = self._analyze_frame_colors(frame_rgb)
                    
                    sample_frames.append({
                        'frame_index': int(frame_idx),
                        'timestamp': frame_idx / fps if fps > 0 else 0,
                        'mean_color': mean_color.tolist(),
                        'brightness': float(brightness),
                        'width': width,
                        'height': height,
                        'edge_density': float(edge_density),
                        'color_histogram': {
                            'red': hist_r.flatten().tolist()[:10],  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥
                            'green': hist_g.flatten().tolist()[:10],
                            'blue': hist_b.flatten().tolist()[:10]
                        },
                        'dominant_colors': dominant_colors
                    })
                    
                except Exception as frame_error:
                    logger.warning(f"í”„ë ˆì„ {frame_idx} ë¶„ì„ ì‹¤íŒ¨: {frame_error}")
                    continue
            
            cap.release()
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            self._update_progress(video_id, 70, "ë¶„ì„ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # í’ˆì§ˆ ë¶„ì„
            try:
            quality_analysis = self._analyze_video_quality(sample_frames)
            except Exception as quality_error:
                logger.warning(f"í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {quality_error}")
                quality_analysis = None
            
            # ì¥ë©´ ë¶„ì„
            try:
                scene_analysis = self._analyze_scene_characteristics(sample_frames)
            except Exception as scene_error:
                logger.warning(f"ì¥ë©´ ë¶„ì„ ì‹¤íŒ¨: {scene_error}")
                scene_analysis = None
            
            # í”„ë ˆì„ ê²°ê³¼ í¬ë§·
            frame_results = self._format_frame_results(sample_frames, video_id)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            self._update_progress(video_id, 90, "ìµœì¢… ê²°ê³¼ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
            analysis_result = {
                'success': True,
                'video_summary': {
                    'total_detections': sum(len(frame.get('persons', [])) for frame in frame_results),
                    'unique_persons': len(set(person.get('id', i) for frame in frame_results for i, person in enumerate(frame.get('persons', [])))),
                    'detailed_attribute_statistics': {
                        'object_type': {
                            'person': sum(len(frame.get('persons', [])) for frame in frame_results)
                        }
                    },
                    'temporal_analysis': {
                        'peak_time_seconds': max([frame.get('timestamp', 0) for frame in frame_results]) if frame_results else 0,
                        'peak_person_count': max([len(frame.get('persons', [])) for frame in frame_results]) if frame_results else 0,
                        'average_person_count': np.mean([len(frame.get('persons', [])) for frame in frame_results]) if frame_results else 0,
                        'total_time_span': duration,
                        'activity_distribution': {str(int(frame.get('timestamp', 0))): 1 for frame in frame_results}
                    },
                    'scene_diversity': scene_analysis or {
                        'scene_type_distribution': {'medium': len(frame_results)},
                        'activity_level_distribution': {'high': len(frame_results)},
                        'lighting_distribution': {'dark': len(frame_results)},
                        'diversity_score': 0.25
                    },
                    'quality_assessment': quality_analysis or {
                        'overall_score': 0.5,
                        'status': 'fair',
                        'brightness_score': 0.5,
                        'contrast_score': 0.5,
                        'sharpness_score': 0.5,
                        'color_balance_score': 0.5,
                        'confidence_average': 0.5
                    },
                    'analysis_type': 'enhanced_opencv_analysis',
                    'key_insights': self._generate_key_insights(sample_frames, quality_analysis, scene_analysis)
                },
                'frame_results': frame_results
            }
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            self._update_progress(video_id, 100, "ë¶„ì„ ì™„ë£Œ!")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ ê¸°ë³¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise e
    
    def _format_frame_results(self, sample_frames, video_id):
        """í”„ë ˆì„ ê²°ê³¼ë¥¼ backend_videochat í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        try:
            frame_results = []
            
            for i, frame in enumerate(sample_frames):
                # í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥
                frame_image_path = self._save_frame_image(video_id, frame, i + 1)
                
                # ì‹¤ì œ YOLO ê°ì²´ íƒì§€ ìˆ˜í–‰
                frame_rgb = self._load_frame_for_analysis(frame_image_path)
                yolo_detections = self._detect_objects_with_yolo(frame_rgb) if frame_rgb is not None else []
                
                # backend_videochat í˜•ì‹ì˜ í”„ë ˆì„ ê²°ê³¼ ìƒì„±
                frame_result = {
                    'image_id': i + 1,
                    'timestamp': frame['timestamp'],
                    'frame_image_path': frame_image_path,  # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ê°€
                    'dominant_colors': frame.get('dominant_colors', []),  # ìƒ‰ìƒ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    'persons': yolo_detections,  # ì‹¤ì œ YOLO íƒì§€ ê²°ê³¼ ì‚¬ìš©
                    'objects': [],
                    'scene_attributes': {
                        'scene_type': 'outdoor' if frame['brightness'] > 120 else 'indoor',
                        'lighting': 'bright' if frame['brightness'] > 150 else 'normal' if frame['brightness'] > 100 else 'dark',
                        'activity_level': 'high' if frame['edge_density'] > 0.04 else 'medium' if frame['edge_density'] > 0.02 else 'low'
                    }
                }
                frame_results.append(frame_result)
            
            return frame_results
            
        except Exception as e:
            logger.error(f"í”„ë ˆì„ ê²°ê³¼ í¬ë§· ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_video_quality(self, sample_frames):
        """ì˜ìƒ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not sample_frames:
                return None
            
            brightness_scores = [frame['brightness'] for frame in sample_frames]
            avg_brightness = np.mean(brightness_scores)
            
            # ê°„ë‹¨í•œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            brightness_score = min(avg_brightness / 255.0, 1.0)
            contrast_score = 0.5  # ê¸°ë³¸ê°’
            sharpness_score = 0.5  # ê¸°ë³¸ê°’
            color_balance_score = 0.9  # ê¸°ë³¸ê°’
            
            overall_score = (brightness_score + contrast_score + sharpness_score + color_balance_score) / 4
            
            status = 'excellent' if overall_score > 0.8 else 'good' if overall_score > 0.6 else 'fair' if overall_score > 0.4 else 'poor'
            
            return {
                'overall_score': overall_score,
                'status': status,
                'brightness_score': brightness_score,
                'contrast_score': contrast_score,
                'sharpness_score': sharpness_score,
                'color_balance_score': color_balance_score,
                'confidence_average': overall_score
            }
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _analyze_scene_characteristics(self, sample_frames):
        """ì¥ë©´ íŠ¹ì„± ë¶„ì„"""
        try:
            if not sample_frames:
                return None
            
            scene_types = []
            lighting_levels = []
            activity_levels = []
            
            for frame in sample_frames:
                # ì¥ë©´ ìœ í˜• ê²°ì •
                scene_type = 'outdoor' if frame['brightness'] > 120 else 'indoor'
                scene_types.append(scene_type)
                
                # ì¡°ëª… ìˆ˜ì¤€ ê²°ì •
                lighting = 'bright' if frame['brightness'] > 150 else 'normal' if frame['brightness'] > 100 else 'dark'
                lighting_levels.append(lighting)
                
                # í™œë™ ìˆ˜ì¤€ ê²°ì •
                activity = 'high' if frame['edge_density'] > 0.04 else 'medium' if frame['edge_density'] > 0.02 else 'low'
                activity_levels.append(activity)
            
            return {
                'scene_type_distribution': dict(Counter(scene_types)),
                'lighting_distribution': dict(Counter(lighting_levels)),
                'activity_level_distribution': dict(Counter(activity_levels)),
                'diversity_score': len(set(scene_types)) / len(scene_types) if scene_types else 0
            }
            
        except Exception as e:
            logger.warning(f"ì¥ë©´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_key_insights(self, sample_frames, quality_analysis, scene_analysis):
        """ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            insights = []
            
            if quality_analysis:
                status = quality_analysis.get('status', 'unknown')
                if status == 'excellent':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ë§¤ìš° ìš°ìˆ˜í•©ë‹ˆë‹¤")
                elif status == 'good':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
                elif status == 'fair':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ë³´í†µì…ë‹ˆë‹¤")
                else:
                    insights.append("ì˜ìƒ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            if scene_analysis:
                scene_dist = scene_analysis.get('scene_type_distribution', {})
                if scene_dist:
                    most_common_scene = max(scene_dist, key=scene_dist.get)
                    insights.append(f"ì£¼ìš” ì¥ë©´ ìœ í˜•: {most_common_scene}")
                
                activity_dist = scene_analysis.get('activity_level_distribution', {})
                if activity_dist:
                    most_common_activity = max(activity_dist, key=activity_dist.get)
                    insights.append(f"ì£¼ìš” í™œë™ ìˆ˜ì¤€: {most_common_activity}")
            
            if sample_frames:
                avg_brightness = np.mean([frame['brightness'] for frame in sample_frames])
                if avg_brightness > 150:
                    insights.append("ë°ì€ ì˜ìƒì…ë‹ˆë‹¤")
                elif avg_brightness < 100:
                    insights.append("ì–´ë‘ìš´ ì˜ìƒì…ë‹ˆë‹¤")
                else:
                    insights.append("ì ì ˆí•œ ë°ê¸°ì˜ ì˜ìƒì…ë‹ˆë‹¤")
            
            return insights[:5]  # ìµœëŒ€ 5ê°œ ì¸ì‚¬ì´íŠ¸
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["ë¶„ì„ ì™„ë£Œ"]
    
    def _update_progress(self, video_id, progress, message):
        """ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            video = Video.objects.get(id=video_id)
            # Video ëª¨ë¸ì— ì§„í–‰ë¥  ì •ë³´ ì €ì¥
            video.analysis_progress = progress
            video.analysis_message = message
            video.save()
            logger.info(f"ğŸ“Š ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {video_id} - {progress}% - {message}")
        except Exception as e:
            logger.error(f"ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _save_frame_image(self, video_id, frame_data, frame_number):
        """í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜ (backend_videochat ë°©ì‹)"""
        try:
            import cv2
            from PIL import Image
            import numpy as np
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            try:
                video = Video.objects.get(id=video_id)
                video_path = os.path.join(settings.MEDIA_ROOT, video.file_path)
            except Video.DoesNotExist:
                logger.error(f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")
                return None
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
                return None
            
            # í•´ë‹¹ í”„ë ˆì„ìœ¼ë¡œ ì´ë™ (frame_dataì—ì„œ frame_index ì‚¬ìš©)
            frame_index = frame_data.get('frame_index', frame_number - 1)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            
            if not ret:
                logger.warning(f"âš ï¸ í”„ë ˆì„ {frame_index} ì½ê¸° ì‹¤íŒ¨")
                cap.release()
                return None
            
            # í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ ìƒì„±
            images_dir = os.path.join(settings.MEDIA_ROOT, 'images')
            os.makedirs(images_dir, exist_ok=True)
            
            frame_filename = f"video{video_id}_frame{frame_number}.jpg"
            frame_path = os.path.join(images_dir, frame_filename)
            
            # í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥
            success = cv2.imwrite(frame_path, frame)
            cap.release()
            
            if success:
            relative_path = f"images/{frame_filename}"
                logger.info(f"âœ… í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {relative_path}")
            return relative_path
            else:
                logger.error(f"âŒ í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {frame_path}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_analysis_to_json(self, analysis_result, video_id):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
            analysis_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            os.makedirs(analysis_dir, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            timestamp = int(time.time())
            filename = f"real_analysis_{video_id}_enhanced_{timestamp}.json"
            file_path = os.path.join(analysis_dir, filename)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            # ìƒëŒ€ ê²½ë¡œ ë°˜í™˜
            relative_path = f"analysis_results/{filename}"
            logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: {relative_path}")
            return relative_path
            
        except Exception as e:
            logger.error(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return None