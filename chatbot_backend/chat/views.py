from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from chat.serializers import UserSerializer
import openai
import anthropic
from groq import Groq
import ollama
import anthropic
import os
import sys
import io
import PyPDF2
from PIL import Image
import pytesseract
# import cv2  # NumPy í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¡°ê±´ë¶€ import
# import numpy as np  # NumPy í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¡°ê±´ë¶€ import
from pdf2image import convert_from_bytes
import base64
import tempfile
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile

# ì¸ì½”ë”© ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# íŒŒì¼ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def extract_text_from_pdf(file_content):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì§ì ‘ ì¶”ì¶œ + OCR ë°±ì—…)"""
    try:
        pdf_file = io.BytesIO(file_content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        
        # ë¨¼ì € ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            page_text = page.extract_text()
            text += page_text + "\n"
        
        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ OCR ì‹œë„
        if len(text.strip()) < 100:  # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ OCR ì‚¬ìš©
            print("PDF ì§ì ‘ ì¶”ì¶œ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ì—¬ OCRì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return extract_text_from_pdf_ocr(file_content)
        
        return text.strip()
    except Exception as e:
        print(f"PDF ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨, OCRì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {str(e)}")
        return extract_text_from_pdf_ocr(file_content)

def extract_text_from_pdf_ocr(file_content):
    """PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        images = convert_from_bytes(file_content, dpi=300)
        all_text = ""
        
        for i, image in enumerate(images):
            # ê°„ë‹¨í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (NumPy ì—†ì´)
            # ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            if image.mode != 'L':
                image = image.convert('L')
            
            # OCR ìˆ˜í–‰ (ì „ì²˜ë¦¬ ì—†ì´)
            page_text = pytesseract.image_to_string(image, lang='kor+eng')
            all_text += f"\n--- í˜ì´ì§€ {i+1} ---\n{page_text}\n"
        
        return all_text.strip()
    except Exception as e:
        return f"PDF OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def extract_text_from_image(file_content):
    """ì´ë¯¸ì§€ì—ì„œ OCRì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        # ì´ë¯¸ì§€ ì—´ê¸°
        image = Image.open(io.BytesIO(file_content))
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
        if image.mode != 'L':
            image = image.convert('L')  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        
        # OCR ìˆ˜í–‰ (í•œêµ­ì–´ + ì˜ì–´)
        text = pytesseract.image_to_string(image, lang='kor+eng')
        
        return text.strip()
    except Exception as e:
        return f"ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def process_uploaded_file(file):
    """ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬"""
    file_content = file.read()
    file_extension = file.name.split('.')[-1].lower()
    
    if file_extension == 'pdf':
        return extract_text_from_pdf(file_content)
    elif file_extension in ['jpg', 'jpeg', 'png', 'bmp', 'tiff']:
        # ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ìš° íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜ (Ollamaê°€ ì§ì ‘ ì½ë„ë¡)
        return f"IMAGE_FILE:{file.name}"
    else:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

def summarize_content(content, api_key=None, file_path=None):
    """ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (Ollama ì‚¬ìš©)"""
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ í™•ì¸
        if content.startswith("IMAGE_FILE:"):
            if file_path and os.path.exists(file_path):
                return analyze_image_with_ollama(file_path)
            else:
                return "ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # í…ìŠ¤íŠ¸ ë‚´ìš©ì¸ ê²½ìš°
        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (í† í° ì œí•œ ê³ ë ¤)
        if len(content) > 12000:
            content = content[:12000] + "..."
        
        # ìš”ì•½ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

ì£¼ì–´ì§„ ë‚´ìš©ì´ PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì¸ ê²½ìš°:
- OCR ì˜¤ë¥˜ë‚˜ ë¶ˆì™„ì „í•œ í…ìŠ¤íŠ¸ê°€ ìˆì„ ìˆ˜ ìˆìŒì„ ê³ ë ¤
- ê°€ëŠ¥í•œ í•œ ì›ë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ìš”ì•½
- ì¤‘ìš”í•œ ì •ë³´ëŠ” ë³´ì¡´í•˜ë˜ ê°„ê²°í•˜ê²Œ ì •ë¦¬

ìš”ì•½ ì‹œ ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œ/ëª©ì 
2. í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš”í•œ í¬ì¸íŠ¸
3. ê²°ë¡ ì´ë‚˜ ìš”ì•½ (ìˆëŠ” ê²½ìš°)

ì›ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì„ ë³´ì¡´í•˜ë©´ì„œë„ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

{content}"""
        
        # Ollama í´ë¼ì´ì–¸íŠ¸ë¡œ ìš”ì•½ ìˆ˜í–‰
        response = ollama.chat(
                   model='llama3.2:latest',  # ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ëª¨ë¸
            messages=[
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            options={
                'temperature': 0.3,
                'num_predict': 1500
            }
        )
        
        return response['message']['content']
    except Exception as e:
        print(f"Ollama ìš”ì•½ ì˜¤ë¥˜: {str(e)}")
        # Ollama ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìš”ì•½
        if len(content) > 1000:
            return f"ë¬¸ì„œ ìš”ì•½ (Ollama ì˜¤ë¥˜ë¡œ ê°„ë‹¨ ìš”ì•½): {content[:500]}..."
        return content

def analyze_image_with_ollama(image_path):
    """í•˜ì´ë¸Œë¦¬ë“œ ì´ë¯¸ì§€ ë¶„ì„ (OCR + Ollama)"""
    try:
        # 1ë‹¨ê³„: OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        try:
            from PIL import Image
            import pytesseract
            
            image = Image.open(image_path)
            ocr_text = pytesseract.image_to_string(image, lang='kor+eng')
            
            if len(ocr_text.strip()) > 10:  # í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ìˆìœ¼ë©´ OCR ê²°ê³¼ ì‚¬ìš©
                return f"ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤: {ocr_text.strip()[:200]}"
        except:
            pass
        
        # 2ë‹¨ê³„: OCR ì‹¤íŒ¨ ì‹œ Ollamaë¡œ ê°„ë‹¨í•œ ë¶„ì„ (ì˜ì–´ë¡œ ë‹µë³€)
        prompt = """IMPORTANT: Count objects very carefully. Look at the image multiple times.

Count the exact number of objects in this image. Be very precise about the count.
Then describe each object's type and main colors.

Examples:
- "1 gray and white cat, blue background"
- "2 dogs, white background" 
- "3 cars, street scene"

Answer in English very concisely. Double-check your count."""
        
        # ì„±ëŠ¥ ìµœì í™”: ë” ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš©
        try:
            # llava:7b ì‚¬ìš© (ê°€ì¥ ê°€ë²¼ìš´ ë¹„ì „ ëª¨ë¸)
            response = ollama.chat(
                model='llava:7b',
                messages=[
                    {
                        'role': 'user',
                        'content': prompt,
                        'images': [image_path]
                    }
                ],
                options={
                    'temperature': 0.1,  # ë” ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ
                    'num_predict': 300,  # í† í° ìˆ˜ ë” ì¤„ì„
                    'num_ctx': 1024  # ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ë” ì œí•œ
                }
            )
            
            # Ollama ì‘ë‹µ ë¡œê¹…
            ollama_response = response['message']['content']
            print(f"Ollama ë¶„ì„ ê²°ê³¼: {ollama_response}")
            return ollama_response
            
        except Exception as e:
            print(f"Ollama ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨, GPT APIë¡œ fallback: {str(e)}")
            # GPT APIë¡œ fallback (ë¹„ìš©ì´ ë“¤ì§€ë§Œ ì •í™•ë„ ë†’ìŒ)
            try:
                import openai
                import base64
                
                with open(image_path, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                
                response = openai.chat.completions.create(
                    model="gpt-4o-mini",  # ê°€ì¥ ì €ë ´í•œ GPT-4 ë¹„ì „ ëª¨ë¸
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": "Count the exact number of objects in this image. Then describe each object's type and main colors only. Answer in English."},
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                                }
                            ]
                        }
                    ],
                    max_tokens=100  # í† í° ìˆ˜ ì œí•œìœ¼ë¡œ ë¹„ìš© ì ˆì•½
                )
                
                gpt_response = response.choices[0].message.content
                print(f"GPT ë¶„ì„ ê²°ê³¼: {gpt_response}")
                return gpt_response
            except Exception as gpt_error:
                print(f"GPT API fallbackë„ ì‹¤íŒ¨: {str(gpt_error)}")
                return "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    except Exception as e:
        print(f"Ollama ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_optimal_response_with_ollama(ai_responses, user_question):
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± (ë¹„ìš© ì ˆì•½ + í’ˆì§ˆ í–¥ìƒ)"""
    try:
        # AI ì‘ë‹µë“¤ì„ ì •ë¦¬
        responses_text = ""
        model_names = []
        for model_name, response in ai_responses.items():
            responses_text += f"### {model_name.upper()}:\n{response}\n\n"
            model_names.append(model_name.upper())
        
        # AI ë¶„ì„ ì„¹ì…˜ ìƒì„±
        analysis_sections = ""
        for name in model_names:
            analysis_sections += f"### {name}\n- ì¥ì : [ì£¼ìš” ì¥ì ]\n- ë‹¨ì : [ì£¼ìš” ë‹¨ì ]\n- íŠ¹ì§•: [íŠ¹ë³„í•œ íŠ¹ì§•]\n"
        
        # ë¹„ìš© ì ˆì•½ì„ ìœ„í•œ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = f"""AI ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

í˜•ì‹:
## í†µí•© ë‹µë³€
[ëª¨ë“  AIì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì  ë‹µë³€]

## ê° AI ë¶„ì„
{analysis_sections}
## ë¶„ì„ ê·¼ê±°
[í†µí•© ë‹µë³€ì„ ë§Œë“  êµ¬ì²´ì  ì´ìœ ]

## ìµœì¢… ì¶”ì²œ
[ìƒí™©ë³„ AI ì„ íƒ ê°€ì´ë“œ]

ì§ˆë¬¸: {user_question}

AI ë‹µë³€ë“¤:
{responses_text}

ìœ„ ë‹µë³€ë“¤ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
        
        response = ollama.chat(
                   model='llama3.2:latest',
            messages=[
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            options={
                'temperature': 0.7,
                'num_predict': 2500
            }
        )
        
        return response['message']['content']
    except Exception as e:
        return f"Ollama ìµœì  ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_optimal_response(ai_responses, user_question, api_key=None):
    """AIë“¤ì˜ ì‘ë‹µì„ í†µí•©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± (Ollama ì‚¬ìš©)"""
    try:
        # Ollamaë¡œ ìµœì  ë‹µë³€ ìƒì„± (ë¹„ìš© ì ˆì•½)
        if not api_key:
            return generate_optimal_response_with_ollama(ai_responses, user_question)
        
        import openai
        client = openai.OpenAI(api_key=api_key)
        
        # AI ì‘ë‹µë“¤ì„ ì •ë¦¬
        responses_text = ""
        model_names = []
        for model_name, response in ai_responses.items():
            responses_text += f"### {model_name.upper()}:\n{response}\n\n"
            model_names.append(model_name.upper())
        
        # ëª¨ë¸ë³„ ë¶„ì„ ì„¹ì…˜ ë™ì  ìƒì„±
        analysis_sections = ""
        for model_name in model_names:
            analysis_sections += f"""
### {model_name}
- ì¥ì : [ì£¼ìš” ì¥ì ]
- ë‹¨ì : [ì£¼ìš” ë‹¨ì ]
- íŠ¹ì§•: [íŠ¹ë³„í•œ íŠ¹ì§•]
"""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": f"""ë‹¹ì‹ ì€ AI ì‘ë‹µ ë¶„ì„ ë° ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ AIì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì™„ì „í•˜ê³  ì •í™•í•œ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

## ğŸ¯ í†µí•© ë‹µë³€
[ê°€ì¥ ì™„ì „í•˜ê³  ì •í™•í•œ í†µí•© ë‹µë³€ - ëª¨ë“  AIì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì ì˜ ë‹µë³€]

## ğŸ“Š ê° AI ë¶„ì„
{analysis_sections}

## ğŸ” ë¶„ì„ ê·¼ê±°
[ê° AIì˜ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ì¡°í•©í•˜ì—¬ í†µí•© ë‹µë³€ì„ ë§Œë“¤ì—ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]

## ğŸ† ìµœì¢… ì¶”ì²œ
[ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë‹µë³€ê³¼ ê·¸ ì´ìœ  - ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ AIë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€ í¬í•¨]

## ğŸ’¡ ì¶”ê°€ ì¸ì‚¬ì´íŠ¸
[ì§ˆë¬¸ì— ëŒ€í•œ ë” ê¹Šì€ ì´í•´ë‚˜ ì¶”ê°€ ê³ ë ¤ì‚¬í•­]"""},
                {"role": "user", "content": f"ì§ˆë¬¸: {user_question}\n\në‹¤ìŒì€ ì—¬ëŸ¬ AIì˜ ë‹µë³€ì…ë‹ˆë‹¤:\n\n{responses_text}\nìœ„ ë‹µë³€ë“¤ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."}
            ],
            temperature=0.7,
            max_tokens=2500
        )
        
        return response.choices[0].message.content
    except Exception as e:
        return f"ìµœì í™”ëœ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

class ChatBot:
    def __init__(self, api_key, model, api_type):
        self.conversation_history = []
        self.model = model
        self.api_type = api_type
        self.api_key = api_key  # api_key ì†ì„± ì¶”ê°€
        
        # API í‚¤ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not api_key:
            raise ValueError(f"{api_type.upper()} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if api_type == 'openai':
            self.client = openai.OpenAI(api_key=api_key)
        elif api_type == 'anthropic':
            self.client = anthropic.Client(api_key=api_key)
        elif api_type == 'groq':
            self.client = Groq(api_key=api_key)
    
    def chat(self, user_input):
        try:
            # ëŒ€í™” ì‹œì‘ ì‹œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€ (íŠ¹ìˆ˜ ë¬¸ì ì œê±°)
            if not self.conversation_history:
                if self.api_type == 'anthropic':
                    system_content = "You are Claude, an AI assistant that can analyze images and respond in Korean. When you receive image analysis results from other AI systems (like Ollama), you should treat them as if you analyzed the image yourself and provide detailed, natural descriptions in Korean. Make the descriptions rich, engaging, and easy to understand while maintaining the accuracy of the original analysis."
                elif self.api_type == 'openai':
                    system_content = "You are GPT, an AI assistant that can analyze images and respond in Korean. When you receive image analysis results from other AI systems (like Ollama), you should treat them as if you analyzed the image yourself and provide detailed, natural descriptions in Korean. Make the descriptions rich, engaging, and easy to understand while maintaining the accuracy of the original analysis."
                elif self.api_type == 'groq':
                    system_content = "You are Mixtral, an AI assistant that can analyze images and respond in Korean. When you receive image analysis results from other AI systems (like Ollama), you should treat them as if you analyzed the image yourself and provide detailed, natural descriptions in Korean. Make the descriptions rich, engaging, and easy to understand while maintaining the accuracy of the original analysis."
                else:
                    system_content = "You are an AI assistant that can analyze images and respond in Korean. When you receive image analysis results from other AI systems (like Ollama), you should treat them as if you analyzed the image yourself and provide detailed, natural descriptions in Korean."
                
                self.conversation_history.append({
                    "role": "system",
                    "content": system_content
                })

                # ì‚¬ìš©ì ì…ë ¥ ì¶œë ¥ (ì¸ì½”ë”© ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                try:
                    safe_input = user_input.encode('ascii', 'ignore').decode('ascii')
                    print(f"User input: {safe_input}")
                except:
                    print("User input received")
            
            self.conversation_history.append({"role": "user", "content": user_input})
            
            # ì¸ì½”ë”© ì•ˆì „í•œ ì‘ë‹µ ë³€ìˆ˜ ì´ˆê¸°í™”
            assistant_response = ""
            
            if self.api_type == 'openai':
                # OpenAI ë°©ì‹ ì²˜ë¦¬
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.conversation_history,
                    temperature=0.7,
                    max_tokens=1024
                )
                assistant_response = response.choices[0].message.content
            
            elif self.api_type == 'anthropic':
                # Anthropic Messages API ë°©ì‹ ì²˜ë¦¬
                try:
                    client = anthropic.Client(api_key=self.api_key)
                    
                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ ìƒì„±
                    messages = []
                    for msg in self.conversation_history:
                        if msg['role'] == 'system':
                            continue  # ClaudeëŠ” system ë©”ì‹œì§€ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                        messages.append({
                            "role": msg['role'],
                            "content": msg['content']
                        })
                    
                    message = client.messages.create(
                        model="claude-3-5-haiku-20241022",
                        max_tokens=4096,
                        temperature=0.7,
                        messages=messages
                    )
                    
                    # ì‘ë‹µ ì¶”ì¶œ
                    raw_response = message.content[0].text
                    assistant_response = raw_response
                    
                    print("Claude response processed successfully")
                    
                except Exception as claude_error:
                    print(f"Claude API error: {str(claude_error)}")
                    print(f"API Key: {self.api_key[:20] if self.api_key else 'None'}...")
                    # API í‚¤ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
                    assistant_response = f"ì•ˆë…•í•˜ì„¸ìš”! '{user_input}'ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”?"


            
            elif self.api_type == 'groq':
                # Groq ë°©ì‹ ì²˜ë¦¬
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.conversation_history,
                    temperature=0.7,
                    max_tokens=1024
                )
                assistant_response = response.choices[0].message.content
            
            # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
            self.conversation_history.append({"role": "assistant", "content": assistant_response})
            return assistant_response
        except Exception as e:
            # ì¸ì½”ë”© ì•ˆì „í•œ ì˜¤ë¥˜ ì²˜ë¦¬
            try:
                error_msg = str(e)
                # íŠ¹ìˆ˜ ë¬¸ì ì œê±°
                import re
                safe_error_msg = re.sub(r'[^\x00-\x7F]+', '', error_msg)
                print(f"Error: {safe_error_msg}")
                return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {safe_error_msg}"
            except:
                print("Error occurred (encoding issue)")
                return "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ì¸ì½”ë”© ë¬¸ì œ"

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY', '')
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')



# API í‚¤ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ChatBot ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chatbots = {}
try:
    if OPENAI_API_KEY:
        chatbots['gpt'] = ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai')
except ValueError as e:
    print(f"GPT ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

try:
    if ANTHROPIC_API_KEY:
        chatbots['claude'] = ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic')
except ValueError as e:
    print(f"Claude ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

try:
    if GROQ_API_KEY:
        chatbots['mixtral'] = ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq')
        chatbots['optimal'] = ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq')
        # í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ Groq ëª¨ë¸ë“¤:
        # - llama-3.1-8b-instant (í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥)
        # - llama-3.1-70b-versatile (deprecated)
        # - mixtral-8x7b-32768 (deprecated)
        # - mixtral-8x7b-instruct (not found)
except ValueError as e:
    print(f"Groq ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

class ChatView(APIView):
    def post(self, request, bot_name):
        try:
            data = request.data
            user_message = data.get('message')
            uploaded_file = request.FILES.get('file')
            
            if not user_message and not uploaded_file:
                return Response({'error': 'No message or file provided'}, status=status.HTTP_400_BAD_REQUEST)
            
            chatbot = chatbots.get(bot_name)
            if not chatbot:
                return Response({'error': 'Invalid bot name'}, status=status.HTTP_400_BAD_REQUEST)

            # íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš° ì²˜ë¦¬
            if uploaded_file:
                try:
                    print(f"íŒŒì¼ ì—…ë¡œë“œ ê°ì§€: {uploaded_file.name}")
                    
                    # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ì‹ë³„
                    extracted_content = process_uploaded_file(uploaded_file)
                    print(f"ì²˜ë¦¬ëœ ë‚´ìš©: {extracted_content[:100]}...")
                    
                    # Ollamaë¡œ ë¶„ì„ (ì´ë¯¸ì§€ëŠ” ì§ì ‘, í…ìŠ¤íŠ¸ëŠ” ìš”ì•½)
                    print("Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë¶„ì„ ì¤‘...")
                    
                    # ì„ì‹œ íŒŒì¼ ì €ì¥
                    temp_file_path = None
                    if extracted_content.startswith("IMAGE_FILE:"):
                        # ì´ë¯¸ì§€ íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
                        import tempfile
                        import shutil
                        temp_dir = tempfile.mkdtemp()
                        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                        with open(temp_file_path, 'wb') as temp_file:
                            for chunk in uploaded_file.chunks():
                                temp_file.write(chunk)
                        print(f"ì´ë¯¸ì§€ íŒŒì¼ ì„ì‹œ ì €ì¥: {temp_file_path}")
                    
                    analyzed_content = summarize_content(extracted_content, file_path=temp_file_path)
                    
                    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì‹œì§€ ìƒì„±
                    if uploaded_file.name.lower().endswith('.pdf'):
                        final_message = f"ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{analyzed_content}"
                    else:
                        # ëª¨ë“  AIê°€ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ì¬êµ¬ì„±í•˜ì—¬ ë‹µë³€í•˜ë„ë¡ ìˆ˜ì •
                        if bot_name in ['claude', 'gpt', 'mixtral']:
                            final_message = f"""ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ Ollamaê°€ ë¶„ì„í•œ ë‚´ìš©ì…ë‹ˆë‹¤:

{analyzed_content}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë¶„ì„ ê²°ê³¼ì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ë˜, ë” í’ë¶€í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”."""
                        else:
                            final_message = f"ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ë³´ë‹ˆ {analyzed_content}ì…ë‹ˆë‹¤. ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                    print("ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    final_message = f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            else:
                final_message = user_message

            # optimal ëª¨ë¸ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if bot_name == 'optimal':
                # ë‹¤ë¥¸ AIë“¤ì˜ ì‘ë‹µì„ ìˆ˜ì§‘
                other_responses_str = data.get('other_responses', '{}')
                try:
                    import json
                    other_responses = json.loads(other_responses_str)
                except:
                    other_responses = {}
                
                if other_responses and len(other_responses) > 0:
                    # ë¹„ìš© ì ˆì•½: Ollama ì‚¬ìš©ìœ¼ë¡œ ìµœì í™”ëœ í†µí•© ë‹µë³€ ìƒì„±
                    response = generate_optimal_response(other_responses, final_message, OPENAI_API_KEY)
                else:
                    # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ ì‘ë‹µì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì‘ë‹µ
                    response = chatbot.chat(final_message)
            else:
                # ë¹„ìš© ì ˆì•½: íŒŒì¼ ë¶„ì„ ì‹œ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                if uploaded_file and 'íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•´' in final_message:
                    # ì´ë¯¸ Ollamaë¡œ ë¶„ì„ëœ ë‚´ìš©ì´ë¯€ë¡œ ê°„ë‹¨í•œ ì‘ë‹µ ìš”ì²­
                    simplified_message = f"ë‹¤ìŒ ë¶„ì„ ë‚´ìš©ì— ëŒ€í•´ ê°„ë‹¨í•œ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”:\n\n{final_message.split('ë‹¤ìŒ íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:')[1] if 'ë‹¤ìŒ íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:' in final_message else final_message}"
                    response = chatbot.chat(simplified_message)
                else:
                    response = chatbot.chat(final_message)
                
            return Response({'response': response})
        except Exception as e:
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
