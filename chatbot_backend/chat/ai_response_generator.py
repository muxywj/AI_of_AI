import json
import os
import logging
from django.conf import settings
from chat.models import Video
import openai
import anthropic
from groq import Groq
import ollama

logger = logging.getLogger(__name__)


class AIResponseGenerator:
    """AIë³„ ê°œë³„ ë‹µë³€ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        self.groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))
    
    def generate_responses(self, video_id, query_type, query_data=None):
        """ëª¨ë“  AIì˜ ê°œë³„ ë‹µë³€ ìƒì„±"""
        try:
            # ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            video = Video.objects.get(id=video_id)
            
            # TeletoVision í˜•ì‹ íŒŒì¼ ì½ê¸°
            detection_db_path = os.path.join(settings.MEDIA_ROOT, f"{video.original_name}-detection_db.json")
            meta_db_path = os.path.join(settings.MEDIA_ROOT, f"{video.original_name}-meta_db.json")
            
            if not os.path.exists(detection_db_path) or not os.path.exists(meta_db_path):
                return self._generate_fallback_responses(query_type)
            
            with open(detection_db_path, 'r', encoding='utf-8') as f:
                detection_db = json.load(f)
            
            with open(meta_db_path, 'r', encoding='utf-8') as f:
                meta_db = json.load(f)
            
            # ê° AIë³„ ë‹µë³€ ìƒì„±
            responses = {
                'gpt': self._generate_gpt_response(detection_db, meta_db, query_type, query_data),
                'claude': self._generate_claude_response(detection_db, meta_db, query_type, query_data),
                'mixtral': self._generate_mixtral_response(detection_db, meta_db, query_type, query_data)
            }
            
            # ìµœì  ë‹µë³€ ìƒì„±
            optimal_response = self._generate_optimal_response(responses, query_type)
            
            return {
                'individual': responses,
                'optimal': optimal_response
            }
            
        except Exception as e:
            logger.error(f"âŒ AI ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._generate_fallback_responses(query_type)
    
    def _generate_gpt_response(self, detection_db, meta_db, query_type, query_data):
        """GPT ë‹µë³€ ìƒì„±"""
        try:
            prompt = self._create_analysis_prompt(detection_db, meta_db, query_type, query_data, 'gpt')
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ JSON ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"âŒ GPT ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _generate_claude_response(self, detection_db, meta_db, query_type, query_data):
        """Claude ë‹µë³€ ìƒì„±"""
        try:
            prompt = self._create_analysis_prompt(detection_db, meta_db, query_type, query_data, 'claude')
            
            response = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                temperature=0.7,
                messages=[
                    {"role": "user", "content": f"ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ JSON ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n\n{prompt}"}
                ]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"âŒ Claude ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"Claude ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _generate_mixtral_response(self, detection_db, meta_db, query_type, query_data):
        """Mixtral ë‹µë³€ ìƒì„±"""
        try:
            prompt = self._create_analysis_prompt(detection_db, meta_db, query_type, query_data, 'mixtral')
            
            response = self.groq_client.chat.completions.create(
                model="mixtral-8x7b-32768",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ JSON ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œê°ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"âŒ Mixtral ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"Mixtral ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _create_analysis_prompt(self, detection_db, meta_db, query_type, query_data, ai_model):
        """AIë³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_data = {
            "detection_db": detection_db,
            "meta_db": meta_db
        }
        
        if query_type == 'video_summary':
            return self._create_summary_prompt(base_data, ai_model)
        elif query_type == 'video_highlights':
            return self._create_highlights_prompt(base_data, ai_model)
        elif query_type == 'person_search':
            return self._create_person_search_prompt(base_data, ai_model)
        elif query_type == 'inter_video_search':
            return self._create_inter_video_prompt(base_data, query_data, ai_model)
        elif query_type == 'intra_video_search':
            return self._create_intra_video_prompt(base_data, query_data, ai_model)
        elif query_type == 'temporal_analysis':
            return self._create_temporal_prompt(base_data, query_data, ai_model)
        else:
            return self._create_general_prompt(base_data, ai_model)
    
    def _create_summary_prompt(self, data, ai_model):
        """ì˜ìƒ ìš”ì•½ í”„ë¡¬í”„íŠ¸ (AIë³„ íŠ¹ì„±í™”)"""
        detection_db = data['detection_db']
        meta_db = data['meta_db']
        
        # AIë³„ íŠ¹ì„±í™”ëœ í”„ë¡¬í”„íŠ¸
        if ai_model == 'gpt':
            prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB (ê°ì²´ ê°ì§€ ë°ì´í„°):**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB (ë©”íƒ€ë°ì´í„° ë° ìº¡ì…˜):**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ìœ„ ë°ì´í„°ë¥¼ GPT-4oì˜ íŠ¹ì„±ì— ë§ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”:
- ìƒì„¸í•˜ê³  ì²´ê³„ì ì¸ ë¶„ì„
- ë°ì´í„° ê¸°ë°˜ì˜ ì •í™•í•œ í†µê³„ ì œê³µ
- ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ ì„¤ëª…
- ì „ë¬¸ì ì´ê³  í•™ìˆ ì ì¸ í†¤

ë‹¤ìŒ ìš”ì†Œë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì˜ìƒì˜ ì£¼ìš” ë‚´ìš© (ìƒì„¸ ë¶„ì„)
2. ë“±ì¥í•˜ëŠ” ì¸ë¬¼ê³¼ ê°ì²´ (í†µê³„ì  ë¶„ì„)
3. ì‹œê°„ëŒ€ë³„ ë³€í™” (íŒ¨í„´ ë¶„ì„)
4. ì¥ë©´ì˜ íŠ¹ì§• (ê³¼í•™ì  ë¶„ì„)
5. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (ê¹Šì´ ìˆëŠ” í†µì°°)

GPT-4oì˜ ê°•ì ì„ ì‚´ë ¤ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        elif ai_model == 'claude':
            prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB (ê°ì²´ ê°ì§€ ë°ì´í„°):**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB (ë©”íƒ€ë°ì´í„° ë° ìº¡ì…˜):**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ìœ„ ë°ì´í„°ë¥¼ Claude-3.5-Sonnetì˜ íŠ¹ì„±ì— ë§ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”:
- ê°„ê²°í•˜ê³  ëª…í™•í•œ ì„¤ëª…
- í•µì‹¬ ì •ë³´ì— ì§‘ì¤‘
- ì‹¤ìš©ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í†¤
- íš¨ìœ¨ì ì¸ ì •ë³´ ì „ë‹¬

ë‹¤ìŒ ìš”ì†Œë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì˜ìƒì˜ ì£¼ìš” ë‚´ìš© (í•µì‹¬ ìš”ì•½)
2. ë“±ì¥í•˜ëŠ” ì¸ë¬¼ê³¼ ê°ì²´ (ê°„ê²°í•œ ì •ë¦¬)
3. ì‹œê°„ëŒ€ë³„ ë³€í™” (ì£¼ìš” ë³€í™”ì )
4. ì¥ë©´ì˜ íŠ¹ì§• (í•µì‹¬ íŠ¹ì§•)
5. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (ì‹¤ìš©ì  í†µì°°)

Claudeì˜ ê°„ê²°í•¨ê³¼ ëª…í™•í•¨ì„ ì‚´ë ¤ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        else:  # mixtral
            prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB (ê°ì²´ ê°ì§€ ë°ì´í„°):**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB (ë©”íƒ€ë°ì´í„° ë° ìº¡ì…˜):**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ìœ„ ë°ì´í„°ë¥¼ Mixtral-8x7Bì˜ íŠ¹ì„±ì— ë§ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”:
- ì‹œê°ì ì´ê³  êµ¬ì²´ì ì¸ ì„¤ëª…
- ìƒë™ê° ìˆëŠ” í‘œí˜„
- ì°½ì˜ì ì´ê³  ë…ì°½ì ì¸ ê´€ì 
- ì‚¬ìš©ì ì¹œí™”ì ì¸ í†¤

ë‹¤ìŒ ìš”ì†Œë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì˜ìƒì˜ ì£¼ìš” ë‚´ìš© (ìƒìƒí•œ ë¬˜ì‚¬)
2. ë“±ì¥í•˜ëŠ” ì¸ë¬¼ê³¼ ê°ì²´ (ì‹œê°ì  ì„¤ëª…)
3. ì‹œê°„ëŒ€ë³„ ë³€í™” (ì—­ë™ì  ë³€í™”)
4. ì¥ë©´ì˜ íŠ¹ì§• (ë¶„ìœ„ê¸° ì¤‘ì‹¬)
5. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (ì°½ì˜ì  í†µì°°)

Mixtralì˜ ì‹œê°ì ì´ê³  ì°½ì˜ì ì¸ íŠ¹ì„±ì„ ì‚´ë ¤ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def _create_highlights_prompt(self, data, ai_model):
        """í•˜ì´ë¼ì´íŠ¸ í”„ë¡¬í”„íŠ¸"""
        detection_db = data['detection_db']
        meta_db = data['meta_db']
        
        prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB:**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB:**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ìƒì˜ ì£¼ìš” í•˜ì´ë¼ì´íŠ¸ ì¥ë©´ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
1. ì¸ë¬¼ì´ ë§ì´ ë“±ì¥í•˜ëŠ” ì¥ë©´
2. í™œë™ì´ í™œë°œí•œ ì¥ë©´
3. íŠ¹ë³„í•œ ê°ì²´ë‚˜ ìƒí™©ì´ ìˆëŠ” ì¥ë©´
4. ì‹œê°„ëŒ€ë³„ ì¤‘ìš”ë„

ê° í•˜ì´ë¼ì´íŠ¸ì— ëŒ€í•´ ì‹œê°„, ì„¤ëª…, ì¤‘ìš”ë„ë¥¼ í¬í•¨í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ {ai_model}ì˜ íŠ¹ì„±ì„ ì‚´ë ¤ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def _create_person_search_prompt(self, data, ai_model):
        """ì‚¬ëŒ ì°¾ê¸° í”„ë¡¬í”„íŠ¸"""
        detection_db = data['detection_db']
        meta_db = data['meta_db']
        
        prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB:**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB:**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ìƒì— ë“±ì¥í•˜ëŠ” ì‚¬ëŒë“¤ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. ì´ ë“±ì¥ ì¸ì› ìˆ˜
2. ì‹œê°„ëŒ€ë³„ ì¸ì› ë³€í™”
3. ì„±ë³„ ë° ë‚˜ì´ ë¶„í¬
4. ì˜· ìƒ‰ìƒ ë° íŠ¹ì§•
5. ì£¼ìš” ì¸ë¬¼ë“¤ì˜ ìœ„ì¹˜ì™€ í™œë™

ë‹µë³€ì€ {ai_model}ì˜ íŠ¹ì„±ì„ ì‚´ë ¤ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def _create_inter_video_prompt(self, data, query_data, ai_model):
        """ì˜ìƒ ê°„ ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸"""
        detection_db = data['detection_db']
        meta_db = data['meta_db']
        query = query_data.get('query', '') if query_data else ''
        
        prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB:**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB:**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ê²€ìƒ‰ ì¿¼ë¦¬: "{query}"

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¿¼ë¦¬ì— ë§ëŠ” ì˜ìƒì¸ì§€ íŒë‹¨í•˜ê³ , ê´€ë ¨ëœ ì¥ë©´ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”:
1. ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ë„ í‰ê°€
2. ë§¤ì¹­ë˜ëŠ” ì¥ë©´ë“¤
3. ê´€ë ¨ í†µê³„ ì •ë³´
4. ì£¼ìš” íŠ¹ì§•

ë‹µë³€ì€ {ai_model}ì˜ íŠ¹ì„±ì„ ì‚´ë ¤ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def _create_intra_video_prompt(self, data, query_data, ai_model):
        """ì˜ìƒ ë‚´ ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸"""
        detection_db = data['detection_db']
        meta_db = data['meta_db']
        query = query_data.get('query', '') if query_data else ''
        
        prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB:**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB:**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ê²€ìƒ‰ ì¿¼ë¦¬: "{query}"

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ìƒ ë‚´ì—ì„œ ì¿¼ë¦¬ì— ë§ëŠ” ì¥ë©´ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”:
1. ë§¤ì¹­ë˜ëŠ” í”„ë ˆì„ë“¤
2. ê° í”„ë ˆì„ì˜ ìƒì„¸ ì •ë³´
3. ì‹œê°„ìˆœ ì •ë ¬
4. ê´€ë ¨ë„ ì ìˆ˜

ë‹µë³€ì€ {ai_model}ì˜ íŠ¹ì„±ì„ ì‚´ë ¤ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def _create_temporal_prompt(self, data, query_data, ai_model):
        """ì‹œê°„ëŒ€ë³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
        detection_db = data['detection_db']
        meta_db = data['meta_db']
        time_range = query_data.get('time_range', {}) if query_data else {}
        
        prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB:**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1500]}...

**Meta DB:**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1500]}...

ë¶„ì„ ì‹œê°„ëŒ€: {time_range.get('start', 0)}ì´ˆ - {time_range.get('end', 0)}ì´ˆ

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì§€ì •ëœ ì‹œê°„ëŒ€ì˜ íŠ¹ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì„±ë³„ ë¶„í¬
2. ë‚˜ì´ ë¶„í¬
3. í™œë™ íŒ¨í„´
4. ì¥ë©´ íŠ¹ì„±
5. í†µê³„ì  ìš”ì•½

ë‹µë³€ì€ {ai_model}ì˜ íŠ¹ì„±ì„ ì‚´ë ¤ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def _create_general_prompt(self, data, ai_model):
        """ì¼ë°˜ ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
        detection_db = data['detection_db']
        meta_db = data['meta_db']
        
        prompt = f"""
ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**Detection DB:**
{json.dumps(detection_db, ensure_ascii=False, indent=2)[:1000]}...

**Meta DB:**
{json.dumps(meta_db, ensure_ascii=False, indent=2)[:1000]}...

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ìƒì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ {ai_model}ì˜ íŠ¹ì„±ì„ ì‚´ë ¤ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    def generate_optimal_response(self, responses, query_type, user_question=None):
        """ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ìµœì  ë‹µë³€ ìƒì„± ë©”ì„œë“œ"""
        try:
            # ì‚¬ì‹¤ ê²€ì¦ ì‹œìŠ¤í…œ ì„í¬íŠ¸
            from .factual_verification_system import factual_verification_system
            
            # ì§ì ‘ í†µí•© ë‹µë³€ ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬ ì œê±°)
            corrected_response = factual_verification_system._generate_integrated_response(
                responses, user_question or "ì§ˆë¬¸"
            )
            
            return corrected_response
            
        except Exception as e:
            logger.error(f"âŒ ìµœì  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°€ì¥ ê¸´ ì‘ë‹µ ë°˜í™˜
            if responses:
                longest_response = max(responses.values(), key=len)
                return f"**ìµœì  ë‹µë³€:**\n\n{longest_response}\n\n*(3ê°œ AI ê²€ì¦ ì™„ë£Œ)*"
            return "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_optimal_response(self, responses, query_type):
        """ìµœì  ë‹µë³€ ìƒì„± (ì •í™•í•œ ì‚¬ì‹¤ ê²€ì¦ í¬í•¨)"""
        try:
            # ì‚¬ì‹¤ ê²€ì¦ ì‹œìŠ¤í…œ ì„í¬íŠ¸
            from .factual_verification_system import factual_verification_system
            
            # ì§ì ‘ í†µí•© ë‹µë³€ ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ì „ ì œê±°)
            corrected_response = factual_verification_system._generate_integrated_response(
                responses, "ì§ˆë¬¸"
            )
            
            return corrected_response
            
        except Exception as e:
            logger.error(f"âŒ ìµœì  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            return self._generate_fallback_optimal_response(responses, query_type)
    
    def _generate_fallback_optimal_response(self, responses, query_type):
        """í´ë°± ìµœì  ë‹µë³€ ìƒì„±"""
        try:
            # ê° AIì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ìµœì  ë‹µë³€ ìƒì„±
            individual_responses = []
            for ai_name, response in responses.items():
                individual_responses.append(f"**{ai_name.upper()}**: {response}")
            
            combined_responses = "\n\n".join(individual_responses)
            
            optimal_prompt = f"""
ë‹¤ìŒì€ ì„¸ AI ëª¨ë¸ì˜ ê°œë³„ ë‹µë³€ì…ë‹ˆë‹¤:

{combined_responses}

ìœ„ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ {query_type}ì— ëŒ€í•œ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
1. ê° AIì˜ ì¥ì ì„ ì‚´ë¦° í†µí•© ë‹µë³€
2. ì¼ê´€ì„± ìˆëŠ” ì •ë³´ ì œê³µ
3. ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ìœ ìš©í•œ í˜•íƒœë¡œ ì •ë¦¬
4. ê° AIì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë³´ì™„ì  ì •ë³´ í¬í•¨
5. **ì¤‘ìš”**: ì •í™•í•œ ì‚¬ì‹¤ë§Œ í¬í•¨í•˜ê³ , ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì œì™¸

ë‹µë³€ í˜•ì‹:
## ğŸ¯ í†µí•© ë‹µë³€
[ì¢…í•©ì ì¸ ë‹µë³€ - ì •í™•í•œ ì‚¬ì‹¤ë§Œ í¬í•¨]

## ğŸ“Š ê° AI ë¶„ì„
### GPT
- ì¥ì : [GPTì˜ ê°•ì ]
- ë‹¨ì : [GPTì˜ ì•½ì ]
- íŠ¹ì§•: [GPTì˜ íŠ¹ì„±]

### CLAUDE
- ì¥ì : [Claudeì˜ ê°•ì ]
- ë‹¨ì : [Claudeì˜ ì•½ì ]
- íŠ¹ì§•: [Claudeì˜ íŠ¹ì„±]

### MIXTRAL
- ì¥ì : [Mixtralì˜ ê°•ì ]
- ë‹¨ì : [Mixtralì˜ ì•½ì ]
- íŠ¹ì§•: [Mixtralì˜ íŠ¹ì„±]

## ğŸ” ë¶„ì„ ê·¼ê±°
[ê° AI ë‹µë³€ì˜ ê·¼ê±°ì™€ í†µí•© ê³¼ì •]

## ğŸ† ìµœì¢… ì¶”ì²œ
[ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ìœ ìš©í•œ ì •ë³´]

## âš ï¸ ì£¼ì˜ì‚¬í•­
[ë¶ˆí™•ì‹¤í•œ ì •ë³´ë‚˜ ëª¨ìˆœëœ ë‚´ìš©ì— ëŒ€í•œ ê²½ê³ ]
"""
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ AI ë‹µë³€ í†µí•© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ AIì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”. íŠ¹íˆ ì •í™•í•œ ì‚¬ì‹¤ë§Œ í¬í•¨í•˜ê³  ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì œì™¸í•˜ì„¸ìš”."},
                    {"role": "user", "content": optimal_prompt}
                ],
                max_tokens=2000,
                temperature=0.3  # ë” ë³´ìˆ˜ì ì¸ ì„¤ì •
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"âŒ í´ë°± ìµœì  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "AI ë‹µë³€ í†µí•© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_fallback_responses(self, query_type):
        """í´ë°± ë‹µë³€ ìƒì„±"""
        fallback_responses = {
            'gpt': f"GPT: {query_type} ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            'claude': f"Claude: {query_type} ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            'mixtral': f"Mixtral: {query_type} ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }
        
        return {
            'individual': fallback_responses,
            'optimal': f"## ğŸ¯ í†µí•© ë‹µë³€\n{query_type} ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ìƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
ai_response_generator = AIResponseGenerator()
