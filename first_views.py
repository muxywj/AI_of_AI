from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import re
import os
from django.conf import settings
from dotenv import load_dotenv

# ì§€ì—° ë¡œë”©ì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ì import
from .lazy_clients import lazy_clients

logger = logging.getLogger(__name__)

# ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì€ í•„ìš”í•  ë•Œë§Œ import
def get_openai_client():
    return lazy_clients.get_openai_client()

def get_anthropic_client():
    return lazy_clients.get_anthropic_client()

def get_groq_client():
    return lazy_clients.get_groq_client()

def get_google_client():
    return lazy_clients.get_google_client()

def get_yolo_model(model_name='yolov8n'):
    return lazy_clients.get_yolo_model(model_name)

def fetch_and_clean_url(url, timeout=10):
    """
    ì£¼ì–´ì§„ URLì˜ HTMLì„ ìš”ì²­í•´, ìŠ¤í¬ë¦½íŠ¸Â·ìŠ¤íƒ€ì¼ ì œê±° í›„ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    resp = requests.get(url, timeout=timeout)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    # ìŠ¤í¬ë¦½íŠ¸Â·ìŠ¤íƒ€ì¼Â·ë„¤ë¹„ê²Œì´ì…˜ íƒœê·¸ ì œê±°
    for tag in soup(["script", "style", "header", "footer", "nav"]):
        tag.decompose()

    text = soup.get_text(separator="\n")
    # ë¹ˆ ì¤„ ì œê±°
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)

# Add this function to your ChatBot class or as a standalone function
def sanitize_and_parse_json(text, selected_models, responses):
    """
    Sanitize and parse the JSON response from AI models.
    Handles various edge cases and formatting issues.
    """
    import re
    import json
    import logging
    
    logger = logging.getLogger(__name__)
    
    try:
        # Step 1: Basic cleanup
        text = text.strip()
        
        # Step 2: Handle code blocks
        if text.startswith('```json') and '```' in text:
            text = re.sub(r'```json(.*?)```', r'\1', text, flags=re.DOTALL).strip()
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()
            
        # Step 3: Extract JSON object if embedded in other text
        json_pattern = r'({[\s\S]*})'
        json_matches = re.findall(json_pattern, text)
        if json_matches:
            text = json_matches[0]
            
        # Step 4: Handle escaped backslashes in the text
        # First identify all occurrences of escaped backslashes followed by characters like "_"
        text = re.sub(r'\\([_"])', r'\1', text)
        
        # Step 5: Attempt to parse the JSON
        result = json.loads(text)
        
        # Ensure the required fields exist
        required_fields = ["preferredModel", "best_response", "analysis", "reasoning"]
        for field in required_fields:
            if field not in result:
                if field == "best_response" and "bestResponse" in result:
                    result["best_response"] = result["bestResponse"]
                else:
                    result[field] = "" if field != "analysis" else {}
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        logger.error(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text[:200]}..." if len(text) > 200 else text)
        
        # Advanced recovery attempt for malformed JSON
        try:
            # Remove problematic escaped characters
            fixed_text = text.replace("\\_", "_").replace('\\"', '"')
            
            # Try to fix common issues with JSON (missing quotes, commas, etc.)
            fixed_text = re.sub(r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', fixed_text)
            fixed_text = re.sub(r':\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*([,}])', r':"\1"\2', fixed_text)
            
            # Handle unclosed strings
            for match in re.finditer(r':\s*"([^"\\]*(\\.[^"\\]*)*)', fixed_text):
                if not re.search(r':\s*"([^"\\]*(\\.[^"\\]*)*)"', match.group(0)):
                    pos = match.end()
                    fixed_text = fixed_text[:pos] + '"' + fixed_text[pos:]
            
            result = json.loads(fixed_text)
            logger.info("âœ… Recovered JSON after fixing format issues")
            return result
        except:
            # Last resort: construct a sensible fallback response
            error_analysis = {}
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"ì¥ì ": "ë¶„ì„ ì‹¤íŒ¨", "ë‹¨ì ": "ë¶„ì„ ì‹¤íŒ¨"}
            
            # Find the largest response to use as best_response
            best_response = ""
            if responses:
                best_response = max(responses.values(), key=len) 
            
            return {
                "preferredModel": "FALLBACK",
                "best_response": best_response,
                "analysis": error_analysis,
                "reasoning": "ì‘ë‹µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            }

# src/chatbot_backend/chat/bot.py

# import logging
# import openai
# import anthropic
# import base64
# import imghdr
# from groq import Groq
# from io import BytesIO

# logger = logging.getLogger(__name__)

# class ChatBot:
#     def __init__(self, api_key, model, api_type):
#         self.conversation_history = []
#         self.api_type = api_type
#         self.api_key = api_key

#         # Anthropic ë©€í‹°ëª¨ë‹¬ì€ Opus ëª¨ë¸ ê¶Œì¥
#         if api_type == 'anthropic' and not model.startswith('claude-3-opus-20240229'):
#             logger.info(f"Overriding Anthropic model '{model}' to 'claude-3-opus-20240229' for image support")
#             self.model = 'claude-3-opus-20240229'
#         else:
#             self.model = model

#         if api_type == 'openai':
#             openai.api_key = api_key
#         elif api_type == 'anthropic':
#             # Anthropic Python SDK ì´ˆê¸°í™”
#             self.client = anthropic.Client(api_key=api_key)
#         elif api_type == 'groq':
#             self.client = Groq(api_key=api_key)
#         else:
#             raise ValueError(f"Unsupported api_type: {api_type}")

#     def chat(self, prompt=None, user_input=None, image_file=None, analysis_mode=None, user_language=None):
#         """
#         prompt       : í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (í‚¤ì›Œë“œ)
#         user_input   : í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ìœ„ì¹˜ ì¸ì)
#         image_file   : íŒŒì¼ ê°ì²´ (BytesIO, InMemoryUploadedFile ë“±)
#         analysis_mode: 'describe'|'ocr'|'objects'
#         user_language: 'ko','en'
#         """
#         text = prompt if prompt is not None else user_input
#         try:
#             logger.info(f"[{self.api_type}] Received input: {text}")

#             # ëª¨ë¸ë³„ í˜¸ì¶œ
#             if self.api_type == 'openai':
#                 # GPT-4 Vision ì§€ì›
#                 params = {
#                     'model': self.model,
#                     'messages': self.conversation_history + [{"role": "user", "content": text}],
#                     'temperature': 0.7,
#                     'max_tokens': 1024
#                 }
#                 if image_file:
#                     params['files'] = [("image", image_file)]
#                 resp = openai.ChatCompletion.create(**params)
#                 assistant_response = resp.choices[0].message.content

#             elif self.api_type == 'anthropic':
#                 # Claude 3 Opus: ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì§€ì› via Messages API
#                 messages = []
#                 # í† í° ìˆ˜ ì„¤ì •
#                 max_tokens = 1024 if image_file else 4096
#                 if image_file:
#                     # ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ì½ê¸° ë° ë¯¸ë””ì–´ íƒ€ì… ìë™ ê°ì§€
#                     image_file.seek(0)
#                     data_bytes = image_file.read()
#                     ext = imghdr.what(None, h=data_bytes) or 'jpeg'
#                     mime_map = {
#                         'jpeg': 'image/jpeg', 'jpg': 'image/jpeg',
#                         'png': 'image/png', 'gif': 'image/gif',
#                         'bmp': 'image/bmp', 'webp': 'image/webp'
#                     }
#                     media_type = mime_map.get(ext, 'image/jpeg')
#                     b64 = base64.b64encode(data_bytes).decode('utf-8')

#                     # ì´ë¯¸ì§€ ë¸”ë¡ê³¼ í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
#                     image_block = {
#                         'type': 'image',
#                         'source': {'type': 'base64', 'media_type': media_type, 'data': b64}
#                     }
#                     text_block = {'type': 'text', 'text': text}
#                     content_blocks = [image_block, text_block]

#                     # ë‹¨ì¼ ë©”ì‹œì§€ì— ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
#                     messages.append({'role': 'user', 'content': content_blocks})
#                 else:
#                     # í…ìŠ¤íŠ¸ ì „ìš© ë©”ì‹œì§€
#                     messages.append({'role': 'user', 'content': [{'type': 'text', 'text': text}]})

#                 # Messages API í˜¸ì¶œ
#                 resp = self.client.messages.create(
#                     model=self.model,
#                     messages=messages,
#                     max_tokens=max_tokens
#                 )
#                 # ì‘ë‹µ ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸ë§Œ í•©ì¹˜ê¸°
#                 assistant_response = ' '.join(getattr(block, 'text', '') for block in resp.content)

#             elif self.api_type == 'groq':
#                 # Groq Chat API
#                 resp = self.client.chat.completions.create(
#                     model=self.model,
#                     messages=self.conversation_history + [{"role": "user", "content": text}],
#                     temperature=0.7,
#                     max_tokens=1024
#                 )
#                 assistant_response = resp.choices[0].message.content

#             else:
#                 raise ValueError(f"Unsupported api_type: {self.api_type}")

#             # ì‘ë‹µ ê¸°ë¡ ë° ë°˜í™˜
#             self.conversation_history.append({"role": "assistant", "content": assistant_response})
#             logger.info(f"[{self.api_type}] Response: {assistant_response[:100]}...")
#             return assistant_response

#         except Exception as e:
#             logger.error(f"Error in chat method ({self.api_type}): {e}", exc_info=True)
#             raise


# paste-2.txt ìˆ˜ì •ëœ ë‚´ìš©

# chatbot.py - OpenAI v1.0+ í˜¸í™˜ ë²„ì „
import openai
import anthropic
from groq import Groq
import logging
import json
import asyncio
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

# sanitize_and_parse_json í•¨ìˆ˜ (ê¸°ì¡´ í•¨ìˆ˜ í¬í•¨)
def sanitize_and_parse_json(text, selected_models=None, responses=None):
    """JSON ì‘ë‹µì„ ì •ë¦¬í•˜ê³  íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    import re
    try:
        text = text.strip()
        
        # ì½”ë“œ ë¸”ë¡ ì œê±°
        if text.startswith('```json') and '```' in text:
            text = re.sub(r'```json(.*?)```', r'\1', text, flags=re.DOTALL).strip()
        elif text.startswith('```') and text.endswith('```'):
            text = text[3:-3].strip()
        
        # JSON íŒ¨í„´ ì¶”ì¶œ
        json_pattern = r'({[\s\S]*})'
        json_matches = re.findall(json_pattern, text)
        if json_matches:
            text = json_matches[0]
        
        # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬
        text = re.sub(r'\\([_"])', r'\1', text)
        
        # JSON íŒŒì‹±
        result = json.loads(text)
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ë³´ì •
        required_fields = ["preferredModel", "best_response", "analysis", "reasoning"]
        for field in required_fields:
            if field not in result:
                if field == "best_response" and "bestResponse" in result:
                    result["best_response"] = result["bestResponse"]
                else:
                    result[field] = "" if field != "analysis" else {}
        
        return result
        
    except Exception as e:
        logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        # í´ë°± ì‘ë‹µ ìƒì„±
        error_analysis = {}
        if selected_models:
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"ì¥ì ": "ë¶„ì„ ì‹¤íŒ¨", "ë‹¨ì ": "ë¶„ì„ ì‹¤íŒ¨"}
        
        return {
            "preferredModel": "ERROR",
            "best_response": max(responses.values(), key=len) if responses else "ë¶„ì„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "analysis": error_analysis,
            "reasoning": "ì‘ë‹µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
import openai
import os 
import anthropic
from groq import Groq
import logging
# from .langchain_config import LangChainManager  # ì„ì‹œë¡œ ì£¼ì„ ì²˜ë¦¬

logger = logging.getLogger(__name__)
class ChatBot:
    def __init__(self, api_key, model, api_type, langchain_manager=None):
        self.conversation_history = []
        self.model = model
        self.api_type = api_type
        self.api_key = api_key
        self.langchain_manager = langchain_manager
        
        # LangChain ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        self.use_langchain = langchain_manager is not None
        
        if not self.use_langchain:
            # ê¸°ì¡´ ë°©ì‹ ì´ˆê¸°í™”
            if api_type == 'openai':
                # OpenAI í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬
                pass
            elif api_type == 'anthropic':
                # Anthropic í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬
                pass
            elif api_type == 'groq':
                # Groq í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬
                pass
        else:
            # LangChain ì²´ì¸ ìƒì„±
            try:
                if api_type in ['gpt', 'claude']:
                    self.chat_chain = langchain_manager.create_chat_chain(api_type)
                elif api_type == 'groq' or api_type == 'mixtral':
                    # GroqëŠ” ë³„ë„ ì²˜ë¦¬
                    self.groq_llm = langchain_manager.groq_llm if hasattr(langchain_manager, 'groq_llm') else None
                logger.info(f"LangChain ì²´ì¸ ìƒì„± ì™„ë£Œ: {api_type}")
            except Exception as e:
                logger.warning(f"LangChain ì²´ì¸ ìƒì„± ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {e}")
                self.use_langchain = False

    async def _analyze_with_langchain(self, responses, query, user_language, selected_models):
        """LangChainì„ ì‚¬ìš©í•œ ì‘ë‹µ ë¶„ì„"""
        try:
            logger.info("\n" + "="*100)
            logger.info("ğŸ“Š LangChain ë¶„ì„ ì‹œì‘")
            logger.info(f"ğŸ¤– ë¶„ì„ ìˆ˜í–‰ AI: {self.api_type.upper()}")
            logger.info(f"ğŸ” ì„ íƒëœ ëª¨ë¸ë“¤: {', '.join(selected_models)}")
            logger.info("="*100)
            
            # ë¶„ì„ ì²´ì¸ ìƒì„±
            analysis_chain = self.langchain_manager.create_analysis_chain(self.api_type)
            
            # ì‘ë‹µ í¬ë§·íŒ…
            formatted = self.langchain_manager.format_responses_for_analysis(
                responses, selected_models
            )
            
            # ë¶„ì„ ì‹¤í–‰
            analysis_result = await analysis_chain.arun(
                query=query,
                user_language=user_language,
                selected_models=selected_models,
                **formatted
            )
            
            # âœ… ìˆ˜ì •: preferredModelì„ ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ì„¤ì •
            analysis_result['preferredModel'] = self.api_type.upper()
            # ì¶”ê°€: botNameë„ ì„¤ì •
            analysis_result['botName'] = self.api_type.upper()
            
            logger.info("âœ… LangChain ë¶„ì„ ì™„ë£Œ\n")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ LangChain ë¶„ì„ ì—ëŸ¬: {str(e)}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹
            return self.analyze_responses(responses, query, user_language, selected_models)

    def analyze_responses(self, responses, query, user_language, selected_models):
        """ê¸°ì¡´ ë™ê¸° ì‘ë‹µ ë¶„ì„ ë©”ì„œë“œ (í˜¸í™˜ì„± ìœ ì§€)"""
        try:
            logger.info("\n" + "="*100)
            logger.info("ğŸ“Š ë¶„ì„ ì‹œì‘")
            logger.info(f"ğŸ¤– ë¶„ì„ ìˆ˜í–‰ AI: {self.api_type.upper()}")
            logger.info(f"ğŸ” ì„ íƒëœ ëª¨ë¸ë“¤: {', '.join(selected_models)}")
            logger.info("="*100)

            # ì„ íƒëœ ëª¨ë¸ë“¤ë§Œ ë¶„ì„ì— í¬í•¨
            responses_section = ""
            analysis_section = ""
            
            for model in selected_models:
                model_lower = model.lower()
                responses_section += f"\n{model.upper()} ì‘ë‹µ: ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {responses.get(model_lower, 'ì‘ë‹µ ì—†ìŒ')}"
                
                analysis_section += f"""
                        "{model_lower}": {{
                            "ì¥ì ": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {model.upper()} ë‹µë³€ì˜ ì¥ì ",
                            "ë‹¨ì ": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {model.upper()} ë‹µë³€ì˜ ë‹¨ì "
                        }}{"," if model_lower != selected_models[-1].lower() else ""}"""

            # âœ… ìˆ˜ì •: preferredModelì„ ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ì„¤ì •
            analysis_prompt = f"""ë‹¤ìŒì€ ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ {len(selected_models)}ê°€ì§€ AIì˜ ì‘ë‹µì„ ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
                    ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤.
                    ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ìµœì ì˜ ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
                    ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì¥ì ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
                    ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ë‹¨ì ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
                    ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ë¶„ì„ ê·¼ê±°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

                    ì§ˆë¬¸: {query}
                    {responses_section}

                     [ìµœì ì˜ ì‘ë‹µì„ ë§Œë“¤ ë•Œ ê³ ë ¤í•  ì‚¬í•­]
                    - ëª¨ë“  AIì˜ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ìœ¼ë¡œ ë°˜ë“œì‹œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤
                    - ê¸°ì¡´ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ì•ˆë©ë‹ˆë‹¤
                    - ì¦‰, ê¸°ì¡´ AIì˜ ë‹µë³€ê³¼ ìµœì ì˜ ë‹µë³€ì´ ë™ì¼í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.
                    - ë‹¤ìˆ˜ì˜ AIê°€ ê³µí†µìœ¼ë¡œ ì œê³µí•œ ì •ë³´ëŠ” ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜¬ë°”ë¥¸ ì •ë³´ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤
                    - ì½”ë“œë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì¼ë•ŒëŠ”, AIì˜ ë‹µë³€ ì¤‘ ì œì¼ ì¢‹ì€ ë‹µë³€ì„ ì„ íƒí•´ì„œ ì¬êµ¬ì„±í•´ì¤˜
                    - ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”
                    [ì¶œë ¥ í˜•ì‹]
                    {{
                        "preferredModel": "{self.api_type.upper()}",
                        "botName": "{self.api_type.upper()}",
                        "best_response": "ìµœì ì˜ ë‹µë³€ ({user_language}ë¡œ ì‘ì„±)",
                        "analysis": {{
                            {analysis_section}
                        }},
                        "reasoning": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± ìµœì ì˜ ì‘ë‹µì„ ì„ íƒí•œ ì´ìœ "
                    }}"""

            # ê¸°ì¡´ API í˜¸ì¶œ ë¡œì§ (ë³€ê²½ ì—†ìŒ)
            if self.api_type == 'openai':
                client = get_openai_client()
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    temperature=0,
                    max_tokens=4096
                )
                analysis_text = response['choices'][0]['message']['content']
                
            elif self.api_type == 'anthropic':
                system_message = next((msg['content'] for msg in self.conversation_history 
                                    if msg['role'] == 'system'), '')
                
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    temperature=0,
                    system=f"{system_message}\nYou must respond with valid JSON only in the specified language. No other text or formatting.",
                    messages=[{
                        "role": "user", 
                        "content": analysis_prompt
                    }]
                )
                analysis_text = message.content[0].text.strip()
            
            elif self.api_type == 'groq':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    temperature=0,
                    max_tokens=4096
                )
                analysis_text = response.choices[0].message.content

            logger.info("âœ… ë¶„ì„ ì™„ë£Œ\n")
            
            # JSON íŒŒì‹± (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
            from paste_3 import sanitize_and_parse_json  # ê¸°ì¡´ í•¨ìˆ˜ import
            analysis_result = sanitize_and_parse_json(analysis_text, selected_models, responses)
            
            # âœ… ìˆ˜ì •: preferredModelê³¼ botNameì„ ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ì„¤ì •
            analysis_result['preferredModel'] = self.api_type.upper()
            analysis_result['botName'] = self.api_type.upper()
            
            return analysis_result
        
        except Exception as e:
            logger.error(f"âŒ Analysis error: {str(e)}")
            # ê¸°ì¡´ í´ë°± ë¡œì§
            error_analysis = {}
            for model in selected_models:
                model_lower = model.lower()
                error_analysis[model_lower] = {"ì¥ì ": "ë¶„ì„ ì‹¤íŒ¨", "ë‹¨ì ": "ë¶„ì„ ì‹¤íŒ¨"}
            
            return {
                "preferredModel": self.api_type.upper(),
                "botName": self.api_type.upper(),  # âœ… ì¶”ê°€
                "best_response": max(responses.values(), key=len) if responses else "",
                "analysis": error_analysis,
                "reasoning": "ì‘ë‹µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            }
# class ChatBot:
#     def __init__(self, api_key, model, api_type, langchain_manager=None):
#         self.conversation_history = []
#         self.model = model
#         self.api_type = api_type
#         self.api_key = api_key
#         self.langchain_manager = langchain_manager
        
#         # LangChain ì‚¬ìš© ì—¬ë¶€ ê²°ì •
#         self.use_langchain = langchain_manager is not None
        
#         if not self.use_langchain:
#             # ê¸°ì¡´ ë°©ì‹ ì´ˆê¸°í™”
#             if api_type == 'openai':
#                 openai.api_key = api_key
#             elif api_type == 'anthropic':
#                 self.client = anthropic.Anthropic(api_key=api_key)
#             elif api_type == 'groq':
#                 self.client = Groq(api_key=api_key)
#         else:
#             # LangChain ì²´ì¸ ìƒì„±
#             try:
#                 if api_type in ['gpt', 'claude']:
#                     self.chat_chain = langchain_manager.create_chat_chain(api_type)
#                 elif api_type == 'groq' or api_type == 'mixtral':
#                     # GroqëŠ” ë³„ë„ ì²˜ë¦¬
#                     self.groq_llm = langchain_manager.groq_llm if hasattr(langchain_manager, 'groq_llm') else None
#                 logger.info(f"LangChain ì²´ì¸ ìƒì„± ì™„ë£Œ: {api_type}")
#             except Exception as e:
#                 logger.warning(f"LangChain ì²´ì¸ ìƒì„± ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {e}")
#                 self.use_langchain = False
   
#     async def chat_async(self, user_input, image_file=None, analysis_mode=None, user_language=None):
#         """ë¹„ë™ê¸° ì±„íŒ… ë©”ì„œë“œ (LangChain ìš©)"""
#         if self.use_langchain:
#             return await self._chat_with_langchain(user_input, user_language)
#         else:
#             return self.chat(user_input, image_file, analysis_mode, user_language)
    
#     async def _chat_with_langchain(self, user_input, user_language='ko'):
#         """LangChainì„ ì‚¬ìš©í•œ ì±„íŒ…"""
#         try:
#             if self.api_type in ['gpt', 'claude']:
#                 result = await self.chat_chain.arun(
#                     user_input=user_input,
#                     user_language=user_language
#                 )
#                 return result
#             elif self.api_type == 'groq' or self.api_type == 'mixtral':
#                 if self.groq_llm:
#                     prompt = f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n\n{user_input}"
#                     result = self.groq_llm(prompt)
#                     return result
#                 else:
#                     # í´ë°±: ê¸°ì¡´ ë°©ì‹
#                     return self.chat(user_input, user_language=user_language)
#             else:
#                 raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” API íƒ€ì…: {self.api_type}")
                
#         except Exception as e:
#             logger.error(f"LangChain ì±„íŒ… ì—ëŸ¬: {e}")
#             # í´ë°±: ê¸°ì¡´ ë°©ì‹
#             return self.chat(user_input, user_language=user_language)

#     def chat(self, user_input, image_file=None, analysis_mode=None, user_language=None):
#         """ê¸°ì¡´ ë™ê¸° ì±„íŒ… ë©”ì„œë“œ (í˜¸í™˜ì„± ìœ ì§€)"""
#         try:
#             logger.info(f"Processing chat request for {self.api_type}")
#             logger.info(f"User input: {user_input}")
            
#             # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
#             if image_file:
#                 self.conversation_history = [{
#                     "role": "system",
#                     "content": f"ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œ: {analysis_mode}, ì‘ë‹µ ì–¸ì–´: {user_language}"
#                 }]
#                 messages = [
#                     {"role": "user", "content": user_input}
#                 ]
#             else:
#                 self.conversation_history.append({"role": "user", "content": user_input})
#                 messages = self.conversation_history

#             try:
#                 if self.api_type == 'openai':
#                     response = openai.ChatCompletion.create(
#                         model=self.model,
#                         messages=self.conversation_history,
#                         temperature=0.7,
#                         max_tokens=1024
#                     )
#                     assistant_response = response['choices'][0]['message']['content']
                    
#                 elif self.api_type == 'anthropic':
#                     try:
#                         # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì°¾ê¸°
#                         system_message = next((msg['content'] for msg in self.conversation_history 
#                                             if msg['role'] == 'system'), '')
                        
#                         # ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ê¸°
#                         user_content = next((msg['content'] for msg in self.conversation_history 
#                                         if msg['role'] == 'user'), '')

#                         message = self.client.messages.create(
#                             model=self.model,
#                             max_tokens=4096,
#                             temperature=0,
#                             system=system_message,
#                             messages=[{
#                                 "role": "user",
#                                 "content": user_content
#                             }]
#                         )
#                         assistant_response = message.content[0].text
#                         logger.info(f"Anthropic response with system message: {system_message[:100]}")
                        
#                     except Exception as e:
#                         logger.error(f"Anthropic API error: {str(e)}")
#                         raise
               
#                 elif self.api_type == 'groq':
#                     response = self.client.chat.completions.create(
#                         model=self.model,
#                         messages=self.conversation_history,
#                         temperature=0.7,
#                         max_tokens=1024
#                     )
#                     assistant_response = response.choices[0].message.content
               
#                 # ì‘ë‹µ ê¸°ë¡
#                 self.conversation_history.append({
#                     "role": "assistant",
#                     "content": assistant_response
#                 })
               
#                 logger.info(f"Generated response: {assistant_response[:100]}...")
#                 return assistant_response
               
#             except Exception as e:
#                 logger.error(f"API error in {self.api_type}: {str(e)}", exc_info=True)
#                 raise
               
#         except Exception as e:
#             logger.error(f"Error in chat method: {str(e)}", exc_info=True)
#             raise

#     async def analyze_responses_async(self, responses, query, user_language, selected_models):
#         """ë¹„ë™ê¸° ì‘ë‹µ ë¶„ì„ (LangChain ìš©)"""
#         if self.use_langchain and self.langchain_manager:
#             return await self._analyze_with_langchain(responses, query, user_language, selected_models)
#         else:
#             return self.analyze_responses(responses, query, user_language, selected_models)
    
#     async def _analyze_with_langchain(self, responses, query, user_language, selected_models):
#         """LangChainì„ ì‚¬ìš©í•œ ì‘ë‹µ ë¶„ì„"""
#         try:
#             logger.info("\n" + "="*100)
#             logger.info("ğŸ“Š LangChain ë¶„ì„ ì‹œì‘")
#             logger.info(f"ğŸ¤– ë¶„ì„ ìˆ˜í–‰ AI: {self.api_type.upper()}")
#             logger.info(f"ğŸ” ì„ íƒëœ ëª¨ë¸ë“¤: {', '.join(selected_models)}")
#             logger.info("="*100)
            
#             # ë¶„ì„ ì²´ì¸ ìƒì„±
#             analysis_chain = self.langchain_manager.create_analysis_chain(self.api_type)
            
#             # ì‘ë‹µ í¬ë§·íŒ…
#             formatted = self.langchain_manager.format_responses_for_analysis(
#                 responses, selected_models
#             )
            
#             # ë¶„ì„ ì‹¤í–‰
#             analysis_result = await analysis_chain.arun(
#                 query=query,
#                 user_language=user_language,
#                 selected_models=selected_models,
#                 **formatted
#             )
            
#             # preferredModel ì„¤ì •
#             analysis_result['preferredModel'] = self.api_type.upper()
            
#             logger.info("âœ… LangChain ë¶„ì„ ì™„ë£Œ\n")
#             return analysis_result
            
#         except Exception as e:
#             logger.error(f"âŒ LangChain ë¶„ì„ ì—ëŸ¬: {str(e)}")
#             # í´ë°±: ê¸°ì¡´ ë°©ì‹
#             return self.analyze_responses(responses, query, user_language, selected_models)

#     def analyze_responses(self, responses, query, user_language, selected_models):
#         """ê¸°ì¡´ ë™ê¸° ì‘ë‹µ ë¶„ì„ ë©”ì„œë“œ (í˜¸í™˜ì„± ìœ ì§€)"""
#         try:
#             logger.info("\n" + "="*100)
#             logger.info("ğŸ“Š ë¶„ì„ ì‹œì‘")
#             logger.info(f"ğŸ¤– ë¶„ì„ ìˆ˜í–‰ AI: {self.api_type.upper()}")
#             logger.info(f"ğŸ” ì„ íƒëœ ëª¨ë¸ë“¤: {', '.join(selected_models)}")
#             logger.info("="*100)

#             # ì„ íƒëœ ëª¨ë¸ë“¤ë§Œ ë¶„ì„ì— í¬í•¨
#             responses_section = ""
#             analysis_section = ""
            
#             for model in selected_models:
#                 model_lower = model.lower()
#                 responses_section += f"\n{model.upper()} ì‘ë‹µ: ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {responses.get(model_lower, 'ì‘ë‹µ ì—†ìŒ')}"
                
#                 analysis_section += f"""
#                         "{model_lower}": {{
#                             "ì¥ì ": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {model.upper()} ë‹µë³€ì˜ ì¥ì ",
#                             "ë‹¨ì ": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± {model.upper()} ë‹µë³€ì˜ ë‹¨ì "
#                         }}{"," if model_lower != selected_models[-1].lower() else ""}"""

#             # ê¸°ì¡´ ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ë³€ê²½ ì—†ìŒ)
#             analysis_prompt = f"""ë‹¤ìŒì€ ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ {len(selected_models)}ê°€ì§€ AIì˜ ì‘ë‹µì„ ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
#                     ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤.
#                     ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ìµœì ì˜ ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
#                     ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì¥ì ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
#                     ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ë‹¨ì ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
#                     ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ë¶„ì„ ê·¼ê±°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

#                     ì§ˆë¬¸: {query}
#                     {responses_section}

#                      [ìµœì ì˜ ì‘ë‹µì„ ë§Œë“¤ ë•Œ ê³ ë ¤í•  ì‚¬í•­]
#                     - ëª¨ë“  AIì˜ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ìœ¼ë¡œ ë°˜ë“œì‹œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤
#                     - ê¸°ì¡´ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ì•ˆë©ë‹ˆë‹¤
#                     - ì¦‰, ê¸°ì¡´ AIì˜ ë‹µë³€ê³¼ ìµœì ì˜ ë‹µë³€ì´ ë™ì¼í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.
#                     - ë‹¤ìˆ˜ì˜ AIê°€ ê³µí†µìœ¼ë¡œ ì œê³µí•œ ì •ë³´ëŠ” ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜¬ë°”ë¥¸ ì •ë³´ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤
#                     - ì½”ë“œë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì¼ë•ŒëŠ”, AIì˜ ë‹µë³€ ì¤‘ ì œì¼ ì¢‹ì€ ë‹µë³€ì„ ì„ íƒí•´ì„œ ì¬êµ¬ì„±í•´ì¤˜
#                     - ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”
#                     [ì¶œë ¥ í˜•ì‹]
#                     {{
#                         "preferredModel": "{self.api_type.upper()}",
#                         "best_response": "ìµœì ì˜ ë‹µë³€ ({user_language}ë¡œ ì‘ì„±)",
#                         "analysis": {{
#                             {analysis_section}
#                         }},
#                         "reasoning": "ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ì„± ìµœì ì˜ ì‘ë‹µì„ ì„ íƒí•œ ì´ìœ "
#                     }}"""

#             # ê¸°ì¡´ API í˜¸ì¶œ ë¡œì§ (ë³€ê²½ ì—†ìŒ)
#             if self.api_type == 'openai':
#                 response = openai.ChatCompletion.create(
#                     model=self.model,
#                     messages=[
#                         {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
#                         {"role": "user", "content": analysis_prompt}
#                     ],
#                     temperature=0,
#                     max_tokens=4096
#                 )
#                 analysis_text = response['choices'][0]['message']['content']
                
#             elif self.api_type == 'anthropic':
#                 system_message = next((msg['content'] for msg in self.conversation_history 
#                                     if msg['role'] == 'system'), '')
                
#                 message = self.client.messages.create(
#                     model=self.model,
#                     max_tokens=4096,
#                     temperature=0,
#                     system=f"{system_message}\nYou must respond with valid JSON only in the specified language. No other text or formatting.",
#                     messages=[{
#                         "role": "user", 
#                         "content": analysis_prompt
#                     }]
#                 )
#                 analysis_text = message.content[0].text.strip()
            
#             elif self.api_type == 'groq':
#                 response = self.client.chat.completions.create(
#                     model=self.model,
#                     messages=[
#                         {"role": "system", "content": "You are a helpful assistant. Always respond with valid JSON ONLY, no additional text or explanations."},
#                         {"role": "user", "content": analysis_prompt}
#                     ],
#                     temperature=0,
#                     max_tokens=4096
#                 )
#                 analysis_text = response.choices[0].message.content

#             logger.info("âœ… ë¶„ì„ ì™„ë£Œ\n")
            
#             # JSON íŒŒì‹± (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
#             from paste_3 import sanitize_and_parse_json  # ê¸°ì¡´ í•¨ìˆ˜ import
#             analysis_result = sanitize_and_parse_json(analysis_text, selected_models, responses)
#             analysis_result['preferredModel'] = self.api_type.upper()
            
#             return analysis_result
        
#         except Exception as e:
#             logger.error(f"âŒ Analysis error: {str(e)}")
#             # ê¸°ì¡´ í´ë°± ë¡œì§
#             error_analysis = {}
#             for model in selected_models:
#                 model_lower = model.lower()
#                 error_analysis[model_lower] = {"ì¥ì ": "ë¶„ì„ ì‹¤íŒ¨", "ë‹¨ì ": "ë¶„ì„ ì‹¤íŒ¨"}
            
#             return {
#                 "preferredModel": self.api_type.upper(),
#                 "best_response": max(responses.values(), key=len) if responses else "",
#                 "analysis": error_analysis,
#                 "reasoning": "ì‘ë‹µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
#             }
# class ChatView(APIView):
#     permission_classes = [AllowAny]

#     def post(self, request, preferredModel):
#         try:
#             logger.info(f"Received chat request for {preferredModel}")
            
#             data = request.data
#             user_message = data.get('message')
#             compare_responses = data.get('compare', True)
            
#             # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°: ì„ íƒëœ ëª¨ë¸ë“¤
#             selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            
#             # ì„ íƒëœ ëª¨ë¸ ë¡œê·¸
#             logger.info(f"Selected models: {selected_models}")
            
#             # í† í° ìœ ë¬´ì— ë”°ë¥¸ ì–¸ì–´ ë° ì„ í˜¸ ëª¨ë¸ ì²˜ë¦¬
#             token = request.headers.get('Authorization')
#             if not token:
#                 # ë¹„ë¡œê·¸ì¸: ê¸°ë³¸ ì–¸ì–´ëŠ” ko, ì„ í˜¸ ëª¨ë¸ì€ GPTë¡œ ê³ ì •
#                 user_language = 'ko'
#                 preferredModel = 'gpt'
#             else:
#                 # ë¡œê·¸ì¸: ìš”ì²­ ë°ì´í„°ì˜ ì–¸ì–´ ì‚¬ìš© (í˜¹ì€ ì‚¬ìš©ìì˜ ì„¤ì •ì„ ë”°ë¦„)
#                 user_language = data.get('language', 'ko')
#                 # URLì— ì „ë‹¬ëœ preferredModelì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©ì ì„¤ì • ë°˜ì˜)

#             logger.info(f"Received language setting: {user_language}")

#             if not user_message:
#                 return Response({'error': 'No message provided'}, 
#                                 status=status.HTTP_400_BAD_REQUEST)

#             # ë¹„ë™ê¸° ì‘ë‹µì„ ìœ„í•œ StreamingHttpResponse ì‚¬ìš©
#             from django.http import StreamingHttpResponse
#             import json
#             import time

#             def stream_responses():
#                 try:
#                     system_message = {
#                         "role": "system",
#                         "content": f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ëª¨ë“  ì‘ë‹µì„ ì´ ì–¸ì–´({user_language})ë¡œ ì œê³µí•´ì£¼ì„¸ìš”."
#                     }
                    
#                     responses = {}
                    
#                     # í˜„ì¬ ìš”ì²­ì— ëŒ€í•œ ê³ ìœ  ì‹ë³„ì ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í™œìš©)
#                     request_id = str(time.time())
                    
#                     # ì„ íƒëœ ëª¨ë¸ë“¤ë§Œ ëŒ€í™”ì— ì°¸ì—¬ì‹œí‚´
#                     selected_chatbots = {model: chatbots.get(model) for model in selected_models if model in chatbots}
                    
#                     # ê° ë´‡ì˜ ì‘ë‹µì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì¦‰ì‹œ ì‘ë‹µ
#                     for bot_id, bot in selected_chatbots.items():
#                         if bot is None:
#                             logger.warning(f"Selected model {bot_id} not available in chatbots")
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': f"Model {bot_id} is not available"
#                             }) + '\n'
#                             continue
                            
#                         try:
#                             # ë§¤ë²ˆ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì´ì „ ë‚´ìš© ì´ˆê¸°í™”)
#                             bot.conversation_history = [system_message]
#                             response = bot.chat(user_message)
#                             responses[bot_id] = response
                            
#                             # ê° ë´‡ ì‘ë‹µì„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_response',
#                                 'botId': bot_id,
#                                 'response': response,
#                                 'requestId': request_id  # ìš”ì²­ ID ì¶”ê°€
#                             }) + '\n'
                            
#                         except Exception as e:
#                             logger.error(f"Error from {bot_id}: {str(e)}")
#                             responses[bot_id] = f"Error: {str(e)}"
                            
#                             # ì—ëŸ¬ë„ ì¦‰ì‹œ ì „ì†¡
#                             yield json.dumps({
#                                 'type': 'bot_error',
#                                 'botId': bot_id,
#                                 'error': str(e),
#                                 'requestId': request_id  # ìš”ì²­ ID ì¶”ê°€
#                             }) + '\n'
                    
#                     # ì„ íƒëœ ëª¨ë¸ì´ ìˆê³  ì‘ë‹µì´ ìˆì„ ë•Œë§Œ ë¶„ì„ ìˆ˜í–‰
#                     if selected_models and responses:
#                         # ë¶„ì„(ë¹„êµ)ì€ ë¡œê·¸ì¸ ì‹œ ì‚¬ìš©ìì˜ ì„ í˜¸ ëª¨ë¸ì„, ë¹„ë¡œê·¸ì¸ ì‹œ GPTë¥¼ ì‚¬ìš©
#                         if token:
#                             analyzer_bot = chatbots.get(preferredModel) or chatbots.get('gpt')
#                         else:
#                             analyzer_bot = chatbots.get('gpt')
                        
#                         # ë¶„ì„ìš© ë´‡ë„ ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
#                         analyzer_bot.conversation_history = [system_message]
                        
#                         # ë¶„ì„ ì‹¤í–‰ (í•­ìƒ ìƒˆë¡­ê²Œ ì‹¤í–‰)
#                         analysis = analyzer_bot.analyze_responses(responses, user_message, user_language, selected_models)
                        
#                         # ë¶„ì„ ê²°ê³¼ ì „ì†¡
#                         yield json.dumps({
#                             'type': 'analysis',
#                             'preferredModel': analyzer_bot and analyzer_bot.api_type.upper(),
#                             'best_response': analysis.get('best_response', ''),
#                             'analysis': analysis.get('analysis', {}),
#                             'reasoning': analysis.get('reasoning', ''),
#                             'language': user_language,
#                             'requestId': request_id,  # ìš”ì²­ ID ì¶”ê°€
#                             'timestamp': time.time()  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
#                         }) + '\n'
#                     else:
#                         logger.warning("No selected models or responses to analyze")
                    
#                 except Exception as e:
#                     logger.error(f"Stream processing error: {str(e)}", exc_info=True)
#                     yield json.dumps({
#                         'type': 'error',
#                         'error': f"Stream processing error: {str(e)}"
#                     }) + '\n'

#             # StreamingHttpResponse ë°˜í™˜
#             response = StreamingHttpResponse(
#                 streaming_content=stream_responses(),
#                 content_type='text/event-stream'
#             )
#             response['Cache-Control'] = 'no-cache'
#             response['X-Accel-Buffering'] = 'no'
#             return response
                
#         except Exception as e:
#             logger.error(f"Unexpected error: {str(e)}", exc_info=True)
#             return Response({
#                 'error': str(e)
#             }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from django.http import StreamingHttpResponse
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
from bs4 import BeautifulSoup
import re
import time
import asyncio
from asgiref.sync import sync_to_async

# ìƒˆë¡œ ì¶”ê°€ëœ import
from .langchain_config import LangChainManager
from .langgraph_workflow import AIComparisonWorkflow

logger = logging.getLogger(__name__)

def convert_to_serializable(obj):
    """ê°ì²´ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    if hasattr(obj, '__dict__'):
        return {k: convert_to_serializable(v) for k, v in obj.__dict__.items()}
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    else:
        return obj

class ChatView(APIView):
    permission_classes = [AllowAny]

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # ê¸°ì¡´ ìœ ì‚¬ë„ ë¶„ì„ê¸°
        from .similarity_analyzer import SimilarityAnalyzer  # ì‹¤ì œ import ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”
        self.similarity_analyzer = SimilarityAnalyzer(threshold=0.85)
        
        # LangChain ê´€ë¦¬ì ì´ˆê¸°í™”
        self.langchain_manager = LangChainManager(
            openai_key=OPENAI_API_KEY,
            anthropic_key=ANTHROPIC_API_KEY,
            groq_key=GROQ_API_KEY,
            google_key=os.getenv('GOOGLE_API_KEY')  # Google API í‚¤ ì¶”ê°€
        )
        
        # LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
        self.workflow = AIComparisonWorkflow(
            langchain_manager=self.langchain_manager,
            similarity_analyzer=self.similarity_analyzer
        )
        
        # ê¸°ì¡´ ChatBot ì¸ìŠ¤í„´ìŠ¤ë“¤ë„ LangChain ì‚¬ìš©í•˜ë„ë¡ ì—…ë°ì´íŠ¸
        self.update_chatbots_with_langchain()

    def update_chatbots_with_langchain(self):
        """ê¸°ì¡´ ChatBotë“¤ì„ LangChainì„ ì‚¬ìš©í•˜ë„ë¡ ì—…ë°ì´íŠ¸"""
        global chatbots
        
        # ê¸°ì¡´ ChatBotë“¤ì— LangChain ë§¤ë‹ˆì € ì¶”ê°€
        for bot_id, bot in chatbots.items():
            bot.langchain_manager = self.langchain_manager
            bot.use_langchain = True
            
            # LangChain ì²´ì¸ ìƒì„± ì‹œë„
            try:
                if bot_id == 'gpt':
                    bot.chat_chain = self.langchain_manager.create_chat_chain('gpt')
                elif bot_id == 'claude':
                    bot.chat_chain = self.langchain_manager.create_chat_chain('claude')
                elif bot_id == 'mixtral':
                    bot.groq_llm = self.langchain_manager.groq_llm if hasattr(self.langchain_manager, 'groq_llm') else None
                logger.info(f"LangChain ì²´ì¸ ìƒì„± ì™„ë£Œ: {bot_id}")
            except Exception as e:
                logger.warning(f"LangChain ì²´ì¸ ìƒì„± ì‹¤íŒ¨ ({bot_id}), ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {e}")
                bot.use_langchain = False

    def post(self, request, preferredModel):
        try:
            logger.info(f"Received chat request for {preferredModel}")
            data = request.data
            user_message = data.get('message')
            selected_models = data.get('selectedModels', ['gpt', 'claude', 'mixtral'])
            token = request.headers.get('Authorization')
            user_language = 'ko' if not token else data.get('language', 'ko')
            use_workflow = data.get('useWorkflow', True)  # ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì—¬ë¶€
            
            if not user_message:
                return Response({'error': 'No message provided'}, status=status.HTTP_400_BAD_REQUEST)

            # URL ì²˜ë¦¬ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
            url_pattern = r'^(https?://\S+)$'
            match = re.match(url_pattern, user_message.strip())
            if match:
                url = match.group(1)
                try:
                    page_text = fetch_and_clean_url(url)
                    if len(page_text) > 10000:
                        page_text = page_text[:5000] + "\n\nâ€¦(ì¤‘ëµ)â€¦\n\n" + page_text[-5000:]
                    user_message = (
                        f"ë‹¤ìŒ ì›¹í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•´ ì£¼ì„¸ìš”:\n"
                        f"URL: {url}\n\n"
                        f"{page_text}"
                    )
                except Exception as e:
                    logger.error(f"URL fetch error: {e}")
                    return Response({'error': f"URLì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}"}, status=status.HTTP_400_BAD_REQUEST)

            # ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë¶„ê¸°
            if use_workflow:
                return self.handle_with_workflow(user_message, selected_models, user_language, preferredModel)
            else:
                return self.handle_with_legacy(user_message, selected_models, user_language, preferredModel)

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def handle_with_workflow(self, user_message, selected_models, user_language, preferred_model):
        """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•œ ì²˜ë¦¬"""
        def stream_workflow_responses():
            try:
                request_id = str(time.time())
                
                # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì„ ìœ„í•œ async ë˜í¼
                async def run_workflow_async():
                    return await self.workflow.run_workflow(
                        user_message=user_message,
                        selected_models=selected_models,
                        user_language=user_language,
                        request_id=request_id
                    )
                
                # asyncio ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    workflow_result = loop.run_until_complete(run_workflow_async())
                finally:
                    loop.close()
                
                # ê°œë³„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                for bot_id, response in workflow_result["individual_responses"].items():
                    yield json.dumps({
                        'type': 'bot_response',
                        'botId': bot_id,
                        'response': response,
                        'requestId': request_id
                    }) + '\n'
                
                # ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼
                if workflow_result["similarity_analysis"]:
                    yield json.dumps({
                        'type': 'similarity_analysis',
                        'result': workflow_result["similarity_analysis"],
                        'requestId': request_id,
                        'timestamp': time.time(),
                        'userMessage': user_message
                    }) + '\n'
                
                # ìµœì¢… ë¶„ì„ ê²°ê³¼
                final_analysis = workflow_result["final_analysis"]
                yield json.dumps({
                    'type': 'analysis',
                    'preferredModel': final_analysis.get('preferredModel', preferred_model.upper()),
                    'best_response': final_analysis.get('best_response', ''),
                    'analysis': final_analysis.get('analysis', {}),
                    'reasoning': final_analysis.get('reasoning', ''),
                    'language': user_language,
                    'requestId': request_id,
                    'timestamp': time.time(),
                    'userMessage': user_message,
                    'workflowUsed': True,
                    'errors': workflow_result.get("errors", [])
                }) + '\n'
                
            except Exception as e:
                logger.error(f"ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì—ëŸ¬: {e}")
                yield json.dumps({
                    'type': 'error',
                    'error': f"Workflow error: {e}",
                    'fallbackToLegacy': True
                }) + '\n'

        return StreamingHttpResponse(stream_workflow_responses(), content_type='text/event-stream')

    def handle_with_legacy(self, user_message, selected_models, user_language, preferred_model):
        """ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ (í˜¸í™˜ì„± ìœ ì§€)"""
        def stream_responses():
            try:
                system_message = {
                    'role': 'system',
                    'content': f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ëŠ” '{user_language}'ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ ì–¸ì–´({user_language})ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
                }
                responses = {}
                request_id = str(time.time())
                
                # ê° ëª¨ë¸ë³„ ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                selected_chatbots = {m: chatbots.get(m) for m in selected_models if chatbots.get(m)}

                # ëª¨ë¸ ì‘ë‹µ ìˆ˜ì§‘ (ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œë„)
                async def collect_responses_async():
                    responses = {}
                    tasks = []
                    
                    for bot_id, bot in selected_chatbots.items():
                        if hasattr(bot, 'chat_async') and bot.use_langchain:
                            # LangChain ë¹„ë™ê¸° ì‚¬ìš©
                            task = bot.chat_async(user_message, user_language=user_language)
                        else:
                            # ê¸°ì¡´ ë™ê¸° ë°©ì‹ì„ ë¹„ë™ê¸°ë¡œ ë˜í•‘
                            task = sync_to_async(self.sync_chat)(bot, user_message, system_message)
                        tasks.append((bot_id, task))
                    
                    for bot_id, task in tasks:
                        try:
                            response = await task
                            responses[bot_id] = response
                            logger.info(f"âœ… {bot_id} ì‘ë‹µ ì™„ë£Œ")
                        except Exception as e:
                            logger.error(f"âŒ {bot_id} ì‘ë‹µ ì‹¤íŒ¨: {e}")
                    
                    return responses

                # ë¹„ë™ê¸° ì‘ë‹µ ìˆ˜ì§‘
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    responses = loop.run_until_complete(collect_responses_async())
                finally:
                    loop.close()

                # ê°œë³„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                for bot_id, resp_text in responses.items():
                    yield json.dumps({
                        'type': 'bot_response',
                        'botId': bot_id,
                        'response': resp_text,
                        'requestId': request_id
                    }) + '\n'

                # ìœ ì‚¬ë„ ë¶„ì„
                if len(responses) >= 2:
                    sim_res = self.similarity_analyzer.cluster_responses(responses)
                    serial = convert_to_serializable(sim_res)
                    yield json.dumps({
                        'type': 'similarity_analysis',
                        'result': serial,
                        'requestId': request_id,
                        'timestamp': time.time(),
                        'userMessage': user_message
                    }) + '\n'

                # ìµœì¢… ë¹„êµ ë° ë¶„ì„
                analyzer_bot = chatbots.get(preferred_model) or chatbots.get('gpt')
                analyzer_bot.conversation_history = [system_message]
                
                # LangChain ë¹„ë™ê¸° ë¶„ì„ ì‹œë„
                if hasattr(analyzer_bot, 'analyze_responses_async') and analyzer_bot.use_langchain:
                    async def analyze_async():
                        return await analyzer_bot.analyze_responses_async(
                            responses, user_message, user_language, list(responses.keys())
                        )
                    
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    try:
                        analysis = loop.run_until_complete(analyze_async())
                    finally:
                        loop.close()
                else:
                    # ê¸°ì¡´ ë™ê¸° ë°©ì‹
                    analysis = analyzer_bot.analyze_responses(
                        responses, user_message, user_language, list(responses.keys())
                    )
                
                yield json.dumps({
                    'type': 'analysis',
                    'preferredModel': analyzer_bot.api_type.upper(),
                    'best_response': analysis.get('best_response', ''),
                    'analysis': analysis.get('analysis', {}),
                    'reasoning': analysis.get('reasoning', ''),
                    'language': user_language,
                    'requestId': request_id,
                    'timestamp': time.time(),
                    'userMessage': user_message,
                    'workflowUsed': False
                }) + '\n'
                
            except Exception as e:
                yield json.dumps({
                    'type': 'error',
                    'error': f"Stream error: {e}"
                }) + '\n'

        return StreamingHttpResponse(stream_responses(), content_type='text/event-stream')

    def sync_chat(self, bot, user_message, system_message):
        """ë™ê¸° ì±„íŒ…ì„ ìœ„í•œ í—¬í¼ ë©”ì„œë“œ"""
        bot.conversation_history = [system_message]
        return bot.chat(user_message)
from dotenv import load_dotenv
load_dotenv()
# API í‚¤ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# ChatBot import (ìˆ˜ì •ëœ ë²„ì „)

chatbots = {
    'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
    'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
    'mixtral': ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
}
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import IsAuthenticated
from .models import UserProfile, UserSettings
from .serializers import UserSerializer

class UserSettingsView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': 'ko',
                    'analyzer_bot': 'claude'
                }
            )
            serializer = UserSerializer(user_settings)
            return Response(serializer.data)
        except Exception as e:
            return Response({
                'language': 'ko', 
                'analyzer_bot': 'claude'
            }, status=status.HTTP_200_OK)

    def post(self, request):
        try:
            user_settings, created = UserSettings.objects.get_or_create(
                user=request.user,
                defaults={
                    'language': request.data.get('language', 'ko'),
                    'analyzer_bot': request.data.get('analyzer_bot', 'claude')
                }
            )

            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
            if not created:
                user_settings.language = request.data.get('language', user_settings.language)
                user_settings.analyzer_bot = request.data.get('analyzer_bot', user_settings.analyzer_bot)
                user_settings.save()

            serializer = UserSettingsSerializer(user_settings)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_400_BAD_REQUEST)
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import AllowAny
from rest_framework.response import Response
import requests
import logging
from django.contrib.auth import get_user_model
from .models import SocialAccount

import uuid

logger = logging.getLogger(__name__)
User = get_user_model()

# def generate_unique_username(email, name=None):
#     """ê³ ìœ í•œ username ìƒì„±"""
#     base = name or email.split('@')[0]
#     username = base
#     suffix = 1
    
#     # usernameì´ ê³ ìœ í•  ë•Œê¹Œì§€ ìˆ«ì ì¶”ê°€
#     while User.objects.filter(username=username).exists():
#         username = f"{base}_{suffix}"
#         suffix += 1
    
#     return username

def generate_unique_username(email, name=None):
    """username ìƒì„± - ì´ë©”ì¼ ì•ë¶€ë¶„ ë˜ëŠ” ì´ë¦„ ì‚¬ìš©"""
    if name:
        return name  # ì´ë¦„ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    return email.split('@')[0]  # ì´ë¦„ì´ ì—†ìœ¼ë©´ ì´ë©”ì¼ ì•ë¶€ë¶„ ì‚¬ìš©
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import AllowAny
from rest_framework.authtoken.models import Token
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
# views.py
from django.db import transaction, IntegrityError

# @api_view(['GET'])
# @authentication_classes([TokenAuthentication])
# @permission_classes([AllowAny])
# @api_view(['GET'])
# @permission_classes([AllowAny])
# def google_callback(request):
#     logger.info("Starting Google callback process")  # ë¡œê¹… ì¶”ê°€
#     try:
#         with transaction.atomic():
#             # 1. ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
#             auth_header = request.headers.get('Authorization', '')
#             access_token = auth_header.split(' ')[1]
            
#             user_info_response = requests.get(
#                 'https://www.googleapis.com/oauth2/v3/userinfo',
#                 headers={'Authorization': f'Bearer {access_token}'}
#             )
            
#             user_info = user_info_response.json()
#             email = user_info.get('email')
#             name = user_info.get('name')

#             logger.info(f"Processing user: {email}")  # ë¡œê¹… ì¶”ê°€

#             # 2. User ê°ì²´ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
#             user = User.objects.filter(email=email).first()
#             if not user:
#                 user = User.objects.create(
#                     username=email,
#                     email=email,
#                     first_name=name or '',
#                     is_active=True
#                 )
#                 logger.info(f"Created new user: {user.id}")
#             else:
#                 logger.info(f"Found existing user: {user.id}")

#             # 3. ê¸°ì¡´ UserSettings ì‚­ì œ (ìˆë‹¤ë©´)
#             UserSettings.objects.filter(user=user).delete()
#             logger.info("Deleted any existing settings")

#             # 4. ìƒˆë¡œìš´ UserSettings ìƒì„±
#             settings = UserSettings.objects.create(
#                 user=user,
#                 language='ko',
#                 preferred_model='default'
#             )
#             logger.info(f"Created new settings for user: {user.id}")

#             # 5. í† í° ìƒì„±
#             token, _ = Token.objects.get_or_create(user=user)
            
#             return Response({
#                 'user': {
#                     'id': user.id,
#                     'email': user.email,
#                     'username': user.username,
#                     'first_name': user.first_name,
#                     'settings': {
#                         'language': settings.language,
#                         'preferred_model': settings.preferred_model
#                     }
#                 },
#                 'access_token': token.key
#             })
#     except Exception as e:
#         logger.error(f"Error in google_callback: {str(e)}")
#         return Response(
#             {'error': str(e)},
#             status=status.HTTP_400_BAD_REQUEST
        # )
@api_view(['GET'])
@authentication_classes([TokenAuthentication])
@permission_classes([AllowAny])
def google_callback(request):
    try:
        # ì•¡ì„¸ìŠ¤ í† í° ì¶”ì¶œ
        auth_header = request.headers.get('Authorization', '')
        if not auth_header.startswith('Bearer '):
            return Response(
                {'error': 'ì˜ëª»ëœ ì¸ì¦ í—¤ë”'}, 
                status=status.HTTP_401_UNAUTHORIZED
            )
        
        access_token = auth_header.split(' ')[1]

        # Google APIë¡œ ì‚¬ìš©ì ì •ë³´ ìš”ì²­
        user_info_response = requests.get(
            'https://www.googleapis.com/oauth2/v3/userinfo',
            headers={'Authorization': f'Bearer {access_token}'}
        )

        if user_info_response.status_code != 200:
            return Response(
                {'error': 'Googleì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        user_info = user_info_response.json()
        email = user_info.get('email')
        name = user_info.get('name')
        
        if not email:
            return Response(
                {'error': 'ì´ë©”ì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, 
                status=status.HTTP_400_BAD_REQUEST
            )

        try:
            # ê¸°ì¡´ ì‚¬ìš©ì ê²€ìƒ‰
            user = User.objects.get(email=email)
        except User.DoesNotExist:
            # ìƒˆë¡œìš´ ì‚¬ìš©ì ìƒì„±
            username = generate_unique_username(email, name)
            user = User.objects.create(
                username=username,
                email=email,
                is_active=True
            )
            
            # ê¸°ë³¸ ë¹„ë°€ë²ˆí˜¸ ì„¤ì • (ì„ íƒì )
            random_password = uuid.uuid4().hex
            user.set_password(random_password)
            user.save()

        # ì†Œì…œ ê³„ì • ì •ë³´ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
        social_account, created = SocialAccount.objects.get_or_create(
            email=email,
            provider='google',
            defaults={'user': user}
        )

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # í† í° ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        token, created = Token.objects.get_or_create(user=user)
        logger.info(f"GOOGLE Token created: {created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")


        # ì‚¬ìš©ì ë°ì´í„° ë°˜í™˜
        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token ë°˜í™˜
            'token_created': created,
            'google_access_token': access_token,  # Google OAuth ì•¡ì„¸ìŠ¤ í† í°

        })

    except Exception as e:
        logger.error(f"Error in google_callback: {str(e)}")
        return Response(
            {'error': str(e)}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
import requests
import json
import requests
from django.conf import settings
from django.http import JsonResponse
from django.shortcuts import redirect
from .models import User  # User ëª¨ë¸ì„ ì„í¬íŠ¸


@api_view(['GET'])
@permission_classes([AllowAny])
def kakao_callback(request):
    try:
        auth_code = request.GET.get('code')
        logger.info(f"Received Kakao auth code: {auth_code}")
        
        # ì¹´ì¹´ì˜¤ í† í° ë°›ê¸°
        token_url = "https://kauth.kakao.com/oauth/token"
        data = {
            "grant_type": "authorization_code",
            "client_id": settings.KAKAO_CLIENT_ID,
            "redirect_uri": settings.KAKAO_REDIRECT_URI,
            "code": auth_code,
        }
        
        token_response = requests.post(token_url, data=data)
        
        if not token_response.ok:
            return Response({
                'error': 'ì¹´ì¹´ì˜¤ í† í° ë°›ê¸° ì‹¤íŒ¨',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        token_data = token_response.json()
        access_token = token_data.get('access_token')
        
        if not access_token:
            return Response({
                'error': 'ì•¡ì„¸ìŠ¤ í† í° ì—†ìŒ',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # ì¹´ì¹´ì˜¤ ì‚¬ìš©ì ì •ë³´ ë°›ê¸°
        user_info_url = "https://kapi.kakao.com/v2/user/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }
        
        user_info_response = requests.get(
            user_info_url,
            headers=headers,
            params={
                'property_keys': json.dumps([
                    "kakao_account.email",
                    "kakao_account.profile",
                    "kakao_account.name"
                ])
            }
        )
        
        if not user_info_response.ok:
            return Response({
                'error': 'ì‚¬ìš©ì ì •ë³´ ë°›ê¸° ì‹¤íŒ¨',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)
        
        user_info = user_info_response.json()
        kakao_account = user_info.get('kakao_account', {})
        email = kakao_account.get('email')
        profile = kakao_account.get('profile', {})
        nickname = profile.get('nickname')
        
        logger.info(f"Kakao user info - email: {email}, nickname: {nickname}")
        
        if not email:
            return Response({
                'error': 'ì´ë©”ì¼ ì •ë³´ ì—†ìŒ',
                'details': 'ì¹´ì¹´ì˜¤ ê³„ì •ì˜ ì´ë©”ì¼ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ì‚¬ìš©ì ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
        try:
            user = User.objects.get(email=email)
            logger.info(f"Updated existing user with nickname: {nickname}")
        except User.DoesNotExist:
            unique_username = generate_unique_username(email, nickname)
            user = User.objects.create(
                email=email,
                username=unique_username,
                is_active=True
            )            
            logger.info(f"Created new user with nickname: {nickname}")

        # ì†Œì…œ ê³„ì • ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
        social_account, created = SocialAccount.objects.update_or_create(
            email=email,
            provider='kakao',
            defaults={
                'user': user,
                'nickname': nickname
            }
        )
        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        if not created and social_account.user != user:
            social_account.user = user
            social_account.save()

        # í† í° ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"KAKAO Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token ë°˜í™˜
            'token_created': created,
            'kakao_access_token': access_token,  # Google OAuth ì•¡ì„¸ìŠ¤ í† í°

        })


        
    except Exception as e:
        logger.exception("Unexpected error in kakao_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


from rest_framework.authtoken.models import Token  # Token ëª¨ë¸ ì¶”ê°€

@api_view(['GET'])
@permission_classes([AllowAny])
def naver_callback(request):
    try:
        code = request.GET.get('code')
        state = request.GET.get('state')
        logger.info(f"Received Naver auth code: {code}")

        # ë„¤ì´ë²„ í† í° ë°›ê¸°
        token_url = "https://nid.naver.com/oauth2.0/token"
        token_params = {
            "grant_type": "authorization_code",
            "client_id": settings.NAVER_CLIENT_ID,
            "client_secret": settings.NAVER_CLIENT_SECRET,
            "code": code,
            "state": state
        }

        token_response = requests.get(token_url, params=token_params)

        if not token_response.ok:
            return Response({
                'error': 'ë„¤ì´ë²„ í† í° ë°›ê¸° ì‹¤íŒ¨',
                'details': token_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        token_data = token_response.json()
        access_token = token_data.get('access_token')

        if not access_token:
            return Response({
                'error': 'ì•¡ì„¸ìŠ¤ í† í° ì—†ìŒ',
                'details': token_data
            }, status=status.HTTP_400_BAD_REQUEST)

        # ë„¤ì´ë²„ ì‚¬ìš©ì ì •ë³´ ë°›ê¸°
        user_info_url = "https://openapi.naver.com/v1/nid/me"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-type": "application/x-www-form-urlencoded;charset=utf-8"
        }

        user_info_response = requests.get(user_info_url, headers=headers)

        if not user_info_response.ok:
            return Response({
                'error': 'ì‚¬ìš©ì ì •ë³´ ë°›ê¸° ì‹¤íŒ¨',
                'details': user_info_response.text
            }, status=status.HTTP_400_BAD_REQUEST)

        user_info = user_info_response.json()
        response = user_info.get('response', {})
        email = response.get('email')
        nickname = response.get('nickname')
        username = email.split('@')[0]

        if not email:
            return Response({
                'error': 'ì´ë©”ì¼ ì •ë³´ ì—†ìŒ',
                'details': 'ë„¤ì´ë²„ ê³„ì •ì˜ ì´ë©”ì¼ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }, status=status.HTTP_400_BAD_REQUEST)

        # ì‚¬ìš©ì ìƒì„± ë˜ëŠ” ì¡°íšŒ
        user, created = User.objects.get_or_create(
            email=email,
            defaults={'username': generate_unique_username(email, username), 'is_active': True}
        )

        # ì†Œì…œ ê³„ì • ì¡°íšŒ ë° ì—…ë°ì´íŠ¸
        social_account, social_created = SocialAccount.objects.update_or_create(
            provider='naver',
            email=email,
            defaults={'user': user, 'nickname': nickname}
        )

        logger.info(f"Social account updated - email: {email}, nickname: {nickname}")

        # âœ… Django REST Framework Token ìƒì„±
        token, token_created = Token.objects.get_or_create(user=user)
        logger.info(f"Naver Token created: {token_created}")
        logger.info(f"Token key: {token.key}")
        logger.info(f"Token user: {token.user.username}")

        serializer = UserSerializer(user)
        return Response({
            'user': serializer.data,
            'access_token': token.key,  # Django REST Framework Token ë°˜í™˜
            'token_created': created,
            'naver_access_token': access_token,  # ë„¤ì´ë²„ ì•¡ì„¸ìŠ¤ í† í°
        })

    except Exception as e:
        logger.exception("Unexpected error in naver_callback")
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# views.py
import logging
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.contrib.auth import get_user_model

# logger = logging.getLogger(__name__)

# @api_view(['PUT'])
# @permission_classes([IsAuthenticated])
# def update_user_settings(request):
#     # ì¶”ê°€ ë¡œê¹… ë° ë””ë²„ê¹…
#     logger.info(f"User authentication status: {request.user.is_authenticated}")
#     logger.info(f"User: {request.user}")
#     logger.info(f"Request headers: {request.headers}")
    
#     try:
#         # ì¸ì¦ ìƒíƒœ ëª…ì‹œì  í™•ì¸
#         if not request.user.is_authenticated:
#             logger.error("Unauthenticated user attempt")
#             return Response({
#                 'status': 'error',
#                 'message': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤.'
#             }, status=401)
        
#         # UserProfile ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •
#         user = request.user
#         user_profile = user.userprofile
        
#         # ì„¤ì • ì—…ë°ì´íŠ¸
#         settings_data = request.data
#         user_profile.language = settings_data.get('language', user_profile.language)
#         user_profile.preferred_model = settings_data.get('preferredModel', user_profile.preferred_model)
#         user_profile.save()
        
#         return Response({
#             'status': 'success',
#             'message': 'ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.',
#             'settings': {
#                 'language': user_profile.language,
#                 'preferredModel': user_profile.preferred_model
#             }
#         })
    
#     except Exception as e:
#         print("Error:", str(e))  # ì—ëŸ¬ ë¡œê¹…
#         logger.error(f"Settings update error: {str(e)}")
#         return Response({
#             'status': 'error',
#             'message': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
#         }, status=400)
# views.py
# ë°±ì—”ë“œì—ì„œ í† í° í˜•ì‹ í™•ì¸
@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        # í† í° ë¡œê¹… ì¶”ê°€
        token_header = request.headers.get('Authorization')
        if not token_header or not token_header.startswith('Token '):
            return Response({'error': 'ì˜ëª»ëœ í† í° í˜•ì‹'}, status=status.HTTP_401_UNAUTHORIZED)
        
        user = request.user
        if not user.is_authenticated:
            return Response({'error': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ì'}, status=status.HTTP_401_UNAUTHORIZED)
        
        settings_data = request.data
        print(f"Received settings data: {settings_data}")  # ë°ì´í„° ë¡œê¹… ì¶”ê°€
        
        # UserSettings ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±
        settings, created = UserSettings.objects.get_or_create(user=user)
        
        # í•„ë“œ ì—…ë°ì´íŠ¸
        if 'language' in settings_data:
            settings.language = settings_data['language']
        if 'preferredModel' in settings_data:
            settings.preferred_model = settings_data['preferredModel']
        
        settings.save()
        
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
        
    except Exception as e:
        logger.error(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )
from rest_framework import status
from rest_framework.decorators import api_view, authentication_classes, permission_classes
from rest_framework.authentication import TokenAuthentication
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.core.exceptions import ObjectDoesNotExist

@api_view(['PUT'])
@authentication_classes([TokenAuthentication])
@permission_classes([IsAuthenticated])
def update_user_settings(request):
    try:
        user = request.user
        settings_data = request.data
        
        # UserProfile í™•ì¸ ë° ìƒì„±
        try:
            profile = user.userprofile
        except ObjectDoesNotExist:
            profile = UserProfile.objects.create(user=user)
            
        # UserSettings í™•ì¸ ë° ìƒì„±/ì—…ë°ì´íŠ¸
        settings, created = UserSettings.objects.get_or_create(
            user=user,
            defaults={
                'language': settings_data.get('language', 'en'),
                'preferred_model': settings_data.get('preferredModel', 'default')
            }
        )
        
        if not created:
            settings.language = settings_data.get('language', settings.language)
            settings.preferred_model = settings_data.get('preferredModel', settings.preferred_model)
            settings.save()
            
        return Response({
            'message': 'Settings updated successfully',
            'settings': {
                'language': settings.language,
                'preferredModel': settings.preferred_model
            }
        })
            
    except Exception as e:
        print(f"Settings update error: {str(e)}")
        return Response(
            {'error': str(e)},
            status=status.HTTP_400_BAD_REQUEST
        )

import logging
import re
import math
import numpy as np
from collections import Counter
from typing import Dict, List, Union, Any
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
logger = logging.getLogger(__name__)

class SimilarityAnalyzer:
    """
    AI ëª¨ë¸ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ê³  ì‘ë‹µ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
    ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ paraphrase-multilingual-MiniLM-L12-v2 ëª¨ë¸ ì‚¬ìš©
    """
    
    def __init__(self, threshold=0., use_transformer=True):
        """
        ì´ˆê¸°í™”
        
        Args:
            threshold (float): ìœ ì‚¬ ì‘ë‹µìœ¼ë¡œ ë¶„ë¥˜í•  ì„ê³„ê°’ (0~1)
            use_transformer (bool): SentenceTransformer ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
        """
        self.threshold = threshold
        self.use_transformer = use_transformer
        
        # ë‹¤êµ­ì–´ SentenceTransformer ëª¨ë¸ ë¡œë“œ
        if use_transformer:
            try:
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("ë‹¤êµ­ì–´ SentenceTransformer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                self.use_transformer = False
                
        # Fallbackìš© TF-IDF ë²¡í„°ë¼ì´ì €
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer(
            min_df=1, 
            analyzer='word',
            ngram_range=(1, 2),
            stop_words=None  # ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ stop_words ì œê±°
        )
    
    def preprocess_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text (str): ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # ì†Œë¬¸ì ë³€í™˜ (ì˜ì–´ í…ìŠ¤íŠ¸ë§Œ í•´ë‹¹)
        # ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° ì›ë˜ ì¼€ì´ìŠ¤ ìœ ì§€
        if text.isascii():
            text = text.lower()
        
        # ì½”ë“œ ë¸”ë¡ ì œê±° (ë¶„ì„ì—ì„œ ì œì™¸)
        text = re.sub(r'```.*?```', ' CODE_BLOCK ', text, flags=re.DOTALL)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<.*?>', '', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ (ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•´ ì™„ì „ ì œê±°í•˜ì§€ ì•ŠìŒ)
        text = re.sub(r'[^\w\s\u0080-\uFFFF]', ' ', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¹˜í™˜
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def calculate_similarity_matrix(self, responses: Dict[str, str]) -> Dict[str, Dict[str, float]]:
        """
        ëª¨ë¸ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        
        Args:
            responses (dict): ëª¨ë¸ IDë¥¼ í‚¤ë¡œ, ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: ëª¨ë¸ ê°„ ìœ ì‚¬ë„ í–‰ë ¬
        """
        try:
            model_ids = list(responses.keys())
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            preprocessed_texts = [self.preprocess_text(responses[model_id]) for model_id in model_ids]
            
            if self.use_transformer and self.model:
                # SentenceTransformerë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
                try:
                    embeddings = self.model.encode(preprocessed_texts)
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity_matrix = cosine_similarity(embeddings)
                except Exception as e:
                    logger.error(f"SentenceTransformer ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # Fallback: TF-IDF ì‚¬ìš©
                    tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                    similarity_matrix = cosine_similarity(tfidf_matrix)
            else:
                # TF-IDF ë²¡í„°í™”
                tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            result = {}
            for i, model1 in enumerate(model_ids):
                result[model1] = {}
                for j, model2 in enumerate(model_ids):
                    result[model1][model2] = float(similarity_matrix[i][j])
            
            return result
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ í–‰ë ¬ ë°˜í™˜
            return {model_id: {other_id: 0.0 for other_id in responses} for model_id in responses}
    
      
    def cluster_responses(self, responses):
        """
        ì‘ë‹µì„ ìœ ì‚¬ë„ì— ë”°ë¼ êµ°ì§‘í™”
        
        Args:
            responses (dict): ëª¨ë¸ IDë¥¼ í‚¤ë¡œ, ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: êµ°ì§‘í™” ê²°ê³¼
        """
        try:
            model_ids = list(responses.keys())
            if len(model_ids) <= 1:
                return {
                    "similarGroups": [model_ids],
                    "outliers": [],
                    "similarityMatrix": {}
                }
            
            # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
            similarity_matrix = self.calculate_similarity_matrix(responses)
            
            # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            clusters = [[model_id] for model_id in model_ids]
            
            merge_happened = True
            while merge_happened and len(clusters) > 1:
                merge_happened = False
                max_similarity = -1
                merge_indices = [-1, -1]
                
                # ê°€ì¥ ìœ ì‚¬í•œ ë‘ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                for i in range(len(clusters)):
                    for j in range(i + 1, len(clusters)):
                        # ë‘ í´ëŸ¬ìŠ¤í„° ê°„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
                        cluster_similarity = 0
                        pair_count = 0
                        
                        for model1 in clusters[i]:
                            for model2 in clusters[j]:
                                cluster_similarity += similarity_matrix[model1][model2]
                                pair_count += 1
                        
                        avg_similarity = cluster_similarity / max(1, pair_count)
                        
                        if avg_similarity > max_similarity:
                            max_similarity = avg_similarity
                            merge_indices = [i, j]
                
                # ì„ê³„ê°’ë³´ë‹¤ ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ í´ëŸ¬ìŠ¤í„° ë³‘í•©
                if max_similarity >= self.threshold:
                    i, j = merge_indices
                    clusters[i].extend(clusters[j])
                    clusters.pop(j)
                    merge_happened = True
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸°ì— ë”°ë¼ ì •ë ¬
            clusters.sort(key=lambda x: -len(x))
            
            # ì£¼ìš” ê·¸ë£¹ê³¼ ì´ìƒì¹˜ êµ¬ë¶„
            main_group = clusters[0] if clusters else []
            outliers = [model for cluster in clusters[1:] for model in cluster]
            
            # ì‘ë‹µ íŠ¹ì„± ì¶”ì¶œ
            response_features = {model_id: self.extract_response_features(responses[model_id]) 
                                for model_id in model_ids}
            
            return {
                "similarGroups": clusters,
                "mainGroup": main_group,
                "outliers": outliers,
                "similarityMatrix": similarity_matrix,
                "responseFeatures": response_features
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ êµ°ì§‘í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª¨ë“  ëª¨ë¸ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë°˜í™˜
            return {
                "similarGroups": [model_ids],
                "mainGroup": model_ids,
                "outliers": [],
                "similarityMatrix": {},
                "responseFeatures": {}
            }
    
    
    def extract_response_features(self, text: str) -> Dict[str, Union[int, float, bool]]:
        """
        ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
        
        Args:
            text (str): ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Returns:
            dict: ì‘ë‹µ íŠ¹ì„± ì •ë³´
        """
        try:
            # ì‘ë‹µ ê¸¸ì´
            length = len(text)
            
            # ì½”ë“œ ë¸”ë¡ ê°œìˆ˜
            code_blocks = re.findall(r'```[\s\S]*?```', text)
            code_block_count = len(code_blocks)
            
            # ë§í¬ ê°œìˆ˜
            links = re.findall(r'\[.*?\]\(.*?\)', text) or re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
            link_count = len(links)
            
            # ëª©ë¡ í•­ëª© ê°œìˆ˜
            list_items = re.findall(r'^[\s]*[-*+] |^[\s]*\d+\. ', text, re.MULTILINE)
            list_item_count = len(list_items)
            
            # ë¬¸ì¥ ë¶„ë¦¬ (ë‹¤êµ­ì–´ ì§€ì›)
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # í‰ê·  ë¬¸ì¥ ê¸¸ì´
            avg_sentence_length = sum(len(s) for s in sentences) / max(1, len(sentences))
            
            # ì–´íœ˜ ë‹¤ì–‘ì„± (ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨)
            words = re.findall(r'\b\w+\b', text.lower())
            unique_words = set(words)
            vocabulary_diversity = len(unique_words) / max(1, len(words))
            
            # ì–¸ì–´ ê°ì§€ (ì¶”ê°€ ê¸°ëŠ¥)
            lang_features = self.detect_language_features(text)
            
            features = {
                "length": length,
                "codeBlockCount": code_block_count,
                "linkCount": link_count,
                "listItemCount": list_item_count,
                "sentenceCount": len(sentences),
                "avgSentenceLength": avg_sentence_length,
                "vocabularyDiversity": vocabulary_diversity,
                "hasCode": code_block_count > 0
            }
            
            # ì–¸ì–´ íŠ¹ì„± ì¶”ê°€
            features.update(lang_features)
            
            return features
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "length": len(text),
                "codeBlockCount": 0,
                "linkCount": 0,
                "listItemCount": 0,
                "sentenceCount": 1,
                "avgSentenceLength": len(text),
                "vocabularyDiversity": 0,
                "hasCode": False,
                "detectedLang": "unknown"
            }
    
    def detect_language_features(self, text: str) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ íŠ¹ì„± ê°ì§€
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: ì–¸ì–´ íŠ¹ì„± ì •ë³´
        """
        try:
            # ì–¸ì–´ íŠ¹ì„± ê°ì§€ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
            # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” langdetect ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥
            
            # í•œêµ­ì–´ íŠ¹ì„± (í•œê¸€ ë¹„ìœ¨)
            korean_chars = len(re.findall(r'[ã„±-ã…ã…-ã…£ê°€-í£]', text))
            
            # ì˜ì–´ íŠ¹ì„± (ì˜ë¬¸ ë¹„ìœ¨)
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            # ì¼ë³¸ì–´ íŠ¹ì„± (ì¼ë³¸ì–´ ë¬¸ì ë¹„ìœ¨)
            japanese_chars = len(re.findall(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]', text))
            
            # ì¤‘êµ­ì–´ íŠ¹ì„± (ì¤‘êµ­ì–´ ë¬¸ì ë¹„ìœ¨)
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            
            # ê¸°íƒ€ ë¬¸ì (ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ì œì™¸)
            total_chars = len(re.findall(r'[^\d\s\W]', text))
            
            # ë¹„ìœ¨ ê³„ì‚°
            total = max(1, total_chars)
            korean_ratio = korean_chars / total
            english_ratio = english_chars / total
            japanese_ratio = japanese_chars / total
            chinese_ratio = chinese_chars / total
            
            # ì£¼ìš” ì–¸ì–´ ê²°ì •
            lang_ratios = {
                "ko": korean_ratio,
                "en": english_ratio,
                "ja": japanese_ratio,
                "zh": chinese_ratio,
                "other": 1.0 - (korean_ratio + english_ratio + japanese_ratio + chinese_ratio)
            }
            
            detected_lang = max(lang_ratios.items(), key=lambda x: x[1])[0]
            
            return {
                "detectedLang": detected_lang,
                "langRatios": lang_ratios
            }
            
        except Exception as e:
            logger.error(f"ì–¸ì–´ íŠ¹ì„± ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "detectedLang": "unknown",
                "langRatios": {"unknown": 1.0}
            }
    
    def compare_responses(self, response1: str, response2: str) -> Dict[str, Any]:
        """
        ë‘ ì‘ë‹µ ê°„ì˜ ìœ ì‚¬ë„ì™€ ì°¨ì´ì  ë¶„ì„
        
        Args:
            response1 (str): ì²« ë²ˆì§¸ ì‘ë‹µ
            response2 (str): ë‘ ë²ˆì§¸ ì‘ë‹µ
            
        Returns:
            dict: ìœ ì‚¬ë„ ë° ì°¨ì´ì  ë¶„ì„ ê²°ê³¼
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text1 = self.preprocess_text(response1)
            text2 = self.preprocess_text(response2)
            
            # ì„ë² ë”© ìƒì„± ë° ìœ ì‚¬ë„ ê³„ì‚°
            if self.use_transformer and self.model:
                embeddings = self.model.encode([text1, text2])
                similarity = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])
            else:
                # TF-IDFë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
                tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
                similarity = float(cosine_similarity(tfidf_matrix)[0][1])
            
            # ì‘ë‹µ íŠ¹ì„± ë¹„êµ
            features1 = self.extract_response_features(response1)
            features2 = self.extract_response_features(response2)
            
            # íŠ¹ì„± ì°¨ì´ ê³„ì‚°
            feature_diffs = {}
            for key in set(features1.keys()) & set(features2.keys()):
                if isinstance(features1[key], (int, float)) and isinstance(features2[key], (int, float)):
                    feature_diffs[key] = features2[key] - features1[key]
            
            # ì£¼ìš” ì°¨ì´ì  ê³ ìœ  ë‹¨ì–´ ë¶„ì„
            words1 = re.findall(r'\b\w+\b', text1.lower())
            words2 = re.findall(r'\b\w+\b', text2.lower())
            
            counter1 = Counter(words1)
            counter2 = Counter(words2)
            
            unique_to_1 = [word for word, count in counter1.items() if word not in counter2]
            unique_to_2 = [word for word, count in counter2.items() if word not in counter1]
            
            # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ê³ ìœ  ë‹¨ì–´ (ìµœëŒ€ 10ê°œ)
            top_unique_to_1 = sorted(
                [(word, counter1[word]) for word in unique_to_1], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            top_unique_to_2 = sorted(
                [(word, counter2[word]) for word in unique_to_2], 
                key=lambda x: x[1], 
                reverse=True
            )[:10]
            
            return {
                "similarity": similarity,
                "isSimilar": similarity >= self.threshold,
                "features1": features1,
                "features2": features2,
                "featureDiffs": feature_diffs,
                "uniqueWordsTo1": top_unique_to_1,
                "uniqueWordsTo2": top_unique_to_2
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "similarity": 0.0,
                "isSimilar": False,
                "error": str(e)
            }
        
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import logging
import json
import openai
import anthropic
from groq import Groq
from django.conf import settings
import time


logger = logging.getLogger(__name__)

class TextSimplificationView(APIView):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” API ë·°
    íŠ¹ì • ëŒ€ìƒ(ì–´ë¦°ì´, ê³ ë ¹ì, ì™¸êµ­ì¸ í•™ìŠµì ë“±)ì— ë§ì¶° ë³€í™˜
    """
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("ì‰¬ìš´ í‘œí˜„ ë³€í™˜ ìš”ì²­ ë°›ìŒ")
            
            data = request.data
            original_text = data.get('message')
            target_audience = data.get('targetAudience', 'general')
            language = data.get('language', 'ko')
            
            if not original_text:
                return Response({'error': 'ë³€í™˜í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.'}, 
                               status=status.HTTP_400_BAD_REQUEST)
            
            # í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ìˆ˜í–‰
            simplifier = TextSimplifier(
                api_key=settings.OPENAI_API_KEY,
                model="gpt-4-turbo",  # ë˜ëŠ” ì„ í˜¸í•˜ëŠ” GPT ëª¨ë¸
                api_type="openai"
            )
            
            result = simplifier.simplify_text(
                original_text=original_text,
                target_audience=target_audience,
                language=language
            )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TextSimplifier:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤
    ë‹¤ì–‘í•œ AI ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ëŒ€ìƒìë³„ ë§ì¶¤í˜• ë‹¨ìˆœí™” ìˆ˜í–‰
    """
    def __init__(self, api_key, model, api_type):
        self.model = model
        self.api_type = api_type
        self.api_key = api_key
        
        if api_type == 'openai':
            openai.api_key = api_key
        elif api_type == 'anthropic':
            # Anthropic í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬
            pass
        elif api_type == 'groq':
            # Groq í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬
            pass
    
    def simplify_text(self, original_text, target_audience, language='ko'):
        """
        í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ë°˜í™˜
        
        Args:
            original_text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            target_audience (str): ëŒ€ìƒì ìœ í˜• (general, child, elderly, foreigner)
            language (str): ì–¸ì–´ (ê¸°ë³¸ê°’: í•œêµ­ì–´)
            
        Returns:
            dict: ë‹¨ìˆœí™” ê²°ê³¼
        """
        try:
            logger.info(f"í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì‹œì‘: ëŒ€ìƒ={target_audience}, ì–¸ì–´={language}")
            
            # ëŒ€ìƒìì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._get_simplification_prompt(original_text, target_audience, language)
            
            # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë‹¨ìˆœí™”
            simplified_text = self._generate_simplified_text(prompt)
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                'original_text': original_text,
                'simplified_text': simplified_text,
                'target_audience': target_audience,
                'language': language,
                'timestamp': time.time()
            }
            
            logger.info("í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì™„ë£Œ")
            return result
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” ì˜¤ë¥˜: {str(e)}", exc_info=True)
            raise
    
    def _get_simplification_prompt(self, original_text, target_audience, language):
        """ëŒ€ìƒì ë§ì¶¤í˜• ë‹¨ìˆœí™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        base_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:

{original_text}

ëŒ€ìƒì: {target_audience}
ì–¸ì–´: {language}
"""
        
        if target_audience == 'child':
            base_prompt += """
[ì–´ë¦°ì´ìš© ë³€í™˜ ì§€ì¹¨]
1. 7-12ì„¸ ì–´ë¦°ì´ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ì™€ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
2. ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. ì¶”ìƒì ì¸ ê°œë…ì€ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”.
4. ì¬ë¯¸ìˆê³  í¥ë¯¸ë¡œìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
5. ì–´ë ¤ìš´ ë‹¨ì–´ëŠ” ê°„ë‹¨í•œ ë™ì˜ì–´ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
6. í•„ìš”í•œ ê²½ìš° ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ í™œìš©í•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
        elif target_audience == 'elderly':
            base_prompt += """
[ê³ ë ¹ììš© ë³€í™˜ ì§€ì¹¨]
1. ëª…í™•í•˜ê³  ì§ì ‘ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
2. ì™¸ë˜ì–´ë‚˜ ì˜ì–´ í‘œí˜„ì€ ê°€ëŠ¥í•œ í•œêµ­ì–´ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
3. ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ í”¼í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. ì „ë¬¸ ìš©ì–´ëŠ” ì¼ìƒì ì¸ ìš©ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
5. ì¹œìˆ™í•œ ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
6. ì¤‘ìš”í•œ ì •ë³´ëŠ” ë°˜ë³µí•´ì„œ ê°•ì¡°í•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
        elif target_audience == 'foreigner':
            base_prompt += """
[ì™¸êµ­ì¸ í•™ìŠµììš© ë³€í™˜ ì§€ì¹¨]
1. í•œêµ­ì–´ í•™ìŠµì(ì´ˆê¸‰~ì¤‘ê¸‰)ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ê¸°ë³¸ ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
2. ê´€ìš©ì–´, ì†ë‹´, ì€ìœ ì  í‘œí˜„ì„ í”¼í•˜ì„¸ìš”.
3. í•œìì–´ëŠ” ê°€ëŠ¥í•œ ìˆœìš°ë¦¬ë§ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
4. ë¬¸ë²•ì ìœ¼ë¡œ ë‹¨ìˆœí•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
5. ë³µì¡í•œ ì—°ê²°ì–´ë¯¸ë‚˜ ì¡°ì‚¬ ì‚¬ìš©ì„ ìµœì†Œí™”í•˜ì„¸ìš”.
6. ì¤‘ìš”í•œ ê°œë…ì€ ê´„í˜¸ ì•ˆì— ì˜ì–´ë¡œ ë³‘ê¸°í•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
        else:  # general
            base_prompt += """
[ì¼ë°˜ì¸ìš© ë³€í™˜ ì§€ì¹¨]
1. ë³´í¸ì ì¸ êµì–‘ ìˆ˜ì¤€ì˜ ì–´íœ˜ì™€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë¶ˆí•„ìš”í•˜ê²Œ ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í•˜ì„¸ìš”.
3. ì „ë¬¸ ìš©ì–´ëŠ” ê°„ë‹¨í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì„¸ìš”.
4. ë…¼ë¦¬ì  íë¦„ì„ ìœ ì§€í•˜ë©° ëª…í™•í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
5. ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
6. ì¤‘ìš”í•œ ë‚´ìš©ì„ ê°•ì¡°í•˜ê³  í•µì‹¬ì„ ë¨¼ì € ì œì‹œí•˜ì„¸ìš”.
7. ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.
"""
            
        return base_prompt
    
    def _generate_simplified_text(self, prompt):
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ìˆœí™”ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            # API ìœ í˜•ì— ë”°ë¥¸ ë¶„ê¸°
            if self.api_type == 'openai':
                client = get_openai_client()
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response['choices'][0]['message']['content']
                
            elif self.api_type == 'anthropic':
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=2000,
                    temperature=0.5,
                    system="ë‹¹ì‹ ì€ ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                    messages=[{
                        "role": "user", 
                        "content": prompt
                    }]
                )
                simplified_text = message.content[0].text
                
            elif self.api_type == 'groq':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                    max_tokens=2000
                )
                simplified_text = response.choices[0].message.content
            
            return simplified_text
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}", exc_info=True)
            raise




    
import logging
import json
import os
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from PIL import Image, ImageFilter, ImageEnhance
import pytesseract
from .models import OCRResult
from .serializers import OCRResultSerializer

import PyPDF2
import tempfile
from pdf2image import convert_from_path
import re

logger = logging.getLogger(__name__)

# OllamaClientì™€ GPTTranslator í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
from .ollama_client import OllamaClient
from .gpt_translator import GPTTranslator 

@method_decorator(csrf_exempt, name='dispatch')
class ProcessFileView(APIView):
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("ProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
            
            # ìš”ì²­ ë°ì´í„° í™•ì¸
            if 'file' not in request.FILES:
                logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
                return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
            file_obj = request.FILES['file']
            file_name = file_obj.name.lower()
            logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
            # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
            ollama_client = OllamaClient(base_url=ollama_base_url)
            
            # GPT ë²ˆì—­ê¸° ì´ˆê¸°í™”
            gpt_translator = GPTTranslator()
            
            # ë²ˆì—­ ì˜µì…˜ í™•ì¸ (ê¸°ë³¸ê°’: True)
            enable_translation = request.data.get('enable_translation', 'true').lower() == 'true'
            
            # íŒŒì¼ ìœ í˜• í™•ì¸
            if file_name.endswith(('.pdf')):
                file_type = 'pdf'
                
                # PDF í˜ì´ì§€ ë²”ìœ„ í™•ì¸
                start_page = int(request.data.get('start_page', 1))
                end_page = int(request.data.get('end_page', 0))  # 0ì€ ì „ì²´ í˜ì´ì§€ë¥¼ ì˜ë¯¸
                
                logger.info("PDF ì²˜ë¦¬ ë²”ìœ„: %s ~ %s í˜ì´ì§€", start_page, end_page if end_page > 0 else "ë")
                
            elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
                file_type = 'image'
            else:
                logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
                return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
                              status=status.HTTP_400_BAD_REQUEST)
            
            # OCR ê²°ê³¼ ê°ì²´ ìƒì„±
            ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
            logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
            
            # OCR ì²˜ë¦¬
            try:
                ocr_text = ""
                page_texts = []  # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥
                
                if file_type == 'image':
                    # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ OCR ì ìš©
                    img = Image.open(ocr_result.file.path)
                    # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
                    logger.info(f"ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={img.size}, ëª¨ë“œ={img.mode}, í¬ë§·={img.format}")
                    
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OCR ìˆ˜í–‰ - OllamaClient ë©”ì„œë“œ ì‚¬ìš©
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
                    ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    page_texts.append({"page": 1, "text": ocr_text})
                    logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
                    logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
                elif file_type == 'pdf':
                    # PDF ì²˜ë¦¬ - ì§ì ‘ ì¶”ì¶œ í›„ í•„ìš”ì‹œ OCR
                    logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
                    
                    # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (í˜ì´ì§€ë³„)
                    direct_extract_success = False
                    try:
                        all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
                        # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬
                        if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
                            if start_page <= len(all_page_texts):
                                if end_page > 0 and end_page >= start_page:
                                    page_texts = all_page_texts[start_page-1:end_page]
                                else:
                                    page_texts = all_page_texts[start_page-1:]
                            else:
                                page_texts = []
                        else:
                            page_texts = all_page_texts
                        
                        combined_text = "\n".join([page["text"] for page in page_texts])
                        
                        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
                        if combined_text.strip() and len(combined_text.strip()) >= 50:
                            meaningful_chars = sum(1 for c in combined_text if c.isalnum())
                            if meaningful_chars > 30:  # ì˜ë¯¸ìˆëŠ” ê¸€ìê°€ 30ì ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                                ocr_text = combined_text
                                direct_extract_success = True
                                logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
                                          len(page_texts), len(ocr_text))
                                logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
                    # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°, OCR ì‹œë„
                    if not direct_extract_success:
                        logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„)")
                        
                        # í˜ì´ì§€ ë²”ìœ„ ì„¤ì •ìœ¼ë¡œ OCR
                        all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, ollama_client, start_page, end_page)
                        
                        # í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬ - ocr_pdf_by_pagesì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©
                        page_texts = all_page_texts
                        
                        ocr_text = "\n".join([page["text"] for page in page_texts])
                        logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
                                  len(page_texts), len(ocr_text))
                        logger.info("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: %s", ocr_text[:200] if ocr_text else "í…ìŠ¤íŠ¸ ì—†ìŒ")
                
                # í…ìŠ¤íŠ¸ ì •í™” - ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©
                ocr_result.ocr_text = self.clean_text(ocr_text)
                
                # PDF íŒŒì¼ì€ í•­ìƒ í…ìŠ¤íŠ¸ ê´€ë ¨ ìˆìŒìœ¼ë¡œ ì„¤ì •
                if file_type == 'pdf':
                    text_relevant = True
                
                # ë¶„ì„ ìœ í˜• í™•ì¸ (ê¸°ë³¸ê°’: both)
                analysis_type = request.data.get('analysis_type', 'both')
                logger.info("ë¶„ì„ ìœ í˜•: %s", analysis_type)
                
                # ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
                image_analysis = ""
                text_analysis = ""
                combined_analysis = ""
                
                # ë²ˆì—­ ê²°ê³¼ ë³€ìˆ˜ ì´ˆê¸°í™”
                translated_analysis = ""
                translation_success = False
                translation_error = ""
                
                # í˜ì´ì§€ ë¶„í•  ë¶„ì„ ì—¬ë¶€ í™•ì¸
                analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
                
                # ì„ íƒëœ ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
                if analysis_type in ['ollama', 'both']:
                    # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš°
                    if file_type == 'image':
                        # ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ìš”ì•½ëœ ê°„ê²°í•œ ì„¤ëª…ì„ ìœ„í•´)
                        custom_prompt = f"""ì´ë¯¸ì§€ë¥¼ ê°ê´€ì ìœ¼ë¡œ ê´€ì°°í•˜ê³  ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”:

í•„ìˆ˜ í¬í•¨ ì‚¬í•­:
- ì´ë¯¸ì§€ì— ì‹¤ì œë¡œ ë³´ì´ëŠ” ì‚¬ëŒ, ë™ë¬¼, ë¬¼ì²´ë§Œ ì–¸ê¸‰ (ì—†ìœ¼ë©´ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ)
- ë§Œì•½ ë™ë¬¼ì´ë¼ë©´, ì–´ë–¤ ì¢…ì˜ ë™ë¬¼ì¸ì§€ë„ ì¶œë ¥
- í™•ì‹¤íˆ ë³´ì´ëŠ” ìƒ‰ìƒë§Œ ì–¸ê¸‰ (ë°°ê²½ìƒ‰, ì˜· ìƒ‰ìƒ ë“±)
- ëª…í™•íˆ ë³´ì´ëŠ” ìì„¸ë‚˜ ìœ„ì¹˜ ê´€ê³„ (ì •ë©´, ì¸¡ë©´ ë“±)

ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ:
- ì¶”ì¸¡ì´ë‚˜ í•´ì„ ("~ë¡œ ë³´ì…ë‹ˆë‹¤", "~ê°™ìŠµë‹ˆë‹¤" í‘œí˜„ ê¸ˆì§€)
- ë³´ì´ì§€ ì•ŠëŠ” ë¶€ë¶„ì— ëŒ€í•œ ì–¸ê¸‰ ("ë³´ì´ì§€ ì•ŠëŠ”ë‹¤", "ì—†ë‹¤" ë“±ì˜ í‘œí˜„ ê¸ˆì§€)
- ë°˜ë³µì ì¸ ì„¤ëª…
- ê°ì •ì´ë‚˜ ë¶„ìœ„ê¸° ë¬˜ì‚¬

í˜•ì‹:
- 1-2ë¬¸ì¥ìœ¼ë¡œ ë§¤ìš° ê°„ê²°í•˜ê²Œ ì‘ì„±
- ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ í˜•ì‹ (ì˜ˆ: "ì´ë¯¸ì§€ì—ëŠ” ê²€ì€ ë¨¸ë¦¬ ì—¬ì„±ì´ ìˆê³ , ë°°ê²½ì€ í°ìƒ‰ì´ë‹¤.")

OCR í…ìŠ¤íŠ¸ (ì°¸ê³ ìš©, ì‹¤ì œ ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ê²½ìš°ë§Œ ì–¸ê¸‰): {ocr_result.ocr_text}

ì˜ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

                        
                        # OCR í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬ (analyze_image ë‚´ë¶€ì—ì„œ ê´€ë ¨ì„± íŒë‹¨)
                        image_analysis = ollama_client.analyze_image(
                            ocr_result.file.path, 
                            custom_prompt,
                            ocr_text=ocr_result.ocr_text
                        )
                        
                        # OCR í…ìŠ¤íŠ¸ ë¶„ì„ (í…ìŠ¤íŠ¸ê°€ ìˆê³  both ëª¨ë“œì¸ ê²½ìš°)
                        if ocr_result.ocr_text and analysis_type == 'both':
                            # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì •ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
                            text_prompt = f"""ë‹¤ìŒ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

{ocr_result.ocr_text}

ë¶„ì„ ì§€ì¹¨:
1. í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë‚´ìš©ê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ì •ë¦¬
2. ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬
3. ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨
4. ë‚´ìš©ì´ ì´ë¯¸ì§€ì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì •ë¦¬

ë°˜ë“œì‹œ "ì˜ì–´(En)"ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                            
                            try:
                                text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                            except Exception as e:
                                logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                text_analysis = f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            
                            # ë‘ ë¶„ì„ ê²°ê³¼ ê²°í•©
                            combined_analysis = f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{image_analysis}\n\ní…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:\n{text_analysis}"
                        else:
                            # OCR ì—†ì´ ì´ë¯¸ì§€ ë¶„ì„ë§Œ ìˆ˜í–‰
                            combined_analysis = image_analysis
                        
                    else:  # PDF íŒŒì¼ì¸ ê²½ìš°
                        if ocr_result.ocr_text:
                            if analyze_by_page and len(page_texts) > 1:
                                # ê°œì„ ëœ í˜ì´ì§€ë³„ ë¶„ì„ ìˆ˜í–‰ - OllamaClientì˜ ë¶„ì„ ê¸°ëŠ¥ í™œìš©
                                try:
                                    combined_analysis = ollama_client.analyze_text(ocr_result.ocr_text, None, page_texts)
                                    logger.info("í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ")
                                except Exception as e:
                                    logger.error(f"í˜ì´ì§€ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                    combined_analysis = f"í˜ì´ì§€ë³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            else:
                                # ë¬¸ì„œ ì „ì²´ ë¶„ì„ - í˜ì´ì§€ë³„ êµ¬ì¡°í™” ìš”ì²­
                                text_prompt = f"""ë‹¤ìŒ PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

{ocr_result.ocr_text}

ë¶„ì„ ì§€ì¹¨:
1. í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.
2. ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ê° ì„¹ì…˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶©ì‹¤í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
3. ëª¨ë“  ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
4. ë‚´ìš©ì„ ë‹¨ìˆœ ìš”ì•½í•˜ì§€ ë§ê³ , êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
===== í˜ì´ì§€ 1 (ë˜ëŠ” ì„¹ì…˜ 1) =====
- ì£¼ìš” ë‚´ìš© ì •ë¦¬
- ì¤‘ìš” ê°œë… ì„¤ëª…
- í•µì‹¬ ì •ë³´ ë‚˜ì—´

===== í˜ì´ì§€ 2 (ë˜ëŠ” ì„¹ì…˜ 2) =====
- ì£¼ìš” ë‚´ìš© ì •ë¦¬
...

ë°˜ë“œì‹œ "ì˜ì–´ë¡œ" ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                                
                                try:
                                    text_analysis = ollama_client.analyze_text(ocr_result.ocr_text, text_prompt)
                                    combined_analysis = text_analysis
                                except Exception as e:
                                    logger.error(f"ë¬¸ì„œ ì „ì²´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                                    combined_analysis = f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nOCR ê²°ê³¼: {ocr_result.ocr_text[:500]}..."
                    
                    logger.info("ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
                
                # GPT ë²ˆì—­ ìˆ˜í–‰ (ë²ˆì—­ì´ í™œì„±í™”ëœ ê²½ìš°)
                if enable_translation and combined_analysis and gpt_translator.is_available:
                    logger.info("GPT ë²ˆì—­ ì‹œì‘")
                    try:
                        # ë¶„ì„ ìœ í˜•ì— ë”°ë¥¸ ë²ˆì—­
                        if file_type == 'pdf' and analyze_by_page and len(page_texts) > 1:
                            # í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ ë²ˆì—­
                            translation_result = gpt_translator.translate_paged_analysis(combined_analysis)
                        else:
                            # ì¼ë°˜ ë¶„ì„ ê²°ê³¼ ë²ˆì—­
                            translation_result = gpt_translator.translate_analysis_result(combined_analysis, file_type)
                        
                        if translation_result and translation_result.get("success"):
                            translated_analysis = translation_result["translated_analysis"]
                            translation_success = True
                            logger.info("GPT ë²ˆì—­ ì„±ê³µ")
                        else:
                            error_msg = translation_result.get('error', 'Unknown error') if translation_result else 'No translation result'
                            logger.error(f"GPT ë²ˆì—­ ì‹¤íŒ¨: {error_msg}")
                            translated_analysis = f"ë²ˆì—­ ì‹¤íŒ¨: {error_msg}"
                            translation_error = error_msg
                            
                    except Exception as e:
                        logger.error(f"GPT ë²ˆì—­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                        translated_analysis = f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        translation_error = str(e)
                
                # ë²ˆì—­ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì €ì¥
                ocr_result.translation_enabled = enable_translation
                ocr_result.translation_success = translation_success
                ocr_result.analysis_type = analysis_type
                ocr_result.analyze_by_page = analyze_by_page
                
                # MySQL ì €ì¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •í™”
                ocr_result.llm_response = self.clean_text(combined_analysis)
                
                # ë²ˆì—­ ê²°ê³¼ë„ ì €ì¥
                if enable_translation and translated_analysis:
                    if translation_success:
                        # ì„±ê³µí•œ ë²ˆì—­ ê²°ê³¼ ì €ì¥
                        ocr_result.llm_response_korean = self.clean_text(translated_analysis)
                        ocr_result.translation_model = gpt_translator.model if gpt_translator else "unknown"
                    else:
                        # ì‹¤íŒ¨í•œ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)
                        ocr_result.llm_response_korean = f"ë²ˆì—­ ì‹¤íŒ¨: {translation_error}"
                
                # í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì •ë³´ ì €ì¥ - PDFëŠ” í•­ìƒ True, ì´ë¯¸ì§€ëŠ” ë¶„ì„ ê³¼ì •ì—ì„œ ê²°ì •
                if file_type == 'pdf':
                    ocr_result.text_relevant = True
                
            except Exception as e:
                logger.error("ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
                return Response({'error': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
                               status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ê²°ê³¼ ì €ì¥
            try:
                ocr_result.save()
                logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
            except Exception as e:
                logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                return Response({'error': f'ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„± - ëª…ì‹œì ìœ¼ë¡œ í•„ë“œ ì§€ì •
            try:
                # ê¸°ë³¸ ì‹œë¦¬ì–¼ë¼ì´ì € ë°ì´í„°
                response_data = OCRResultSerializer(ocr_result).data
                
                # ë²ˆì—­ ê´€ë ¨ ì •ë³´ ëª…ì‹œì  ì¶”ê°€
                response_data['translation_enabled'] = enable_translation
                response_data['translation_success'] = translation_success
                
                # ì˜ì–´ ì›ë¬¸ê³¼ í•œêµ­ì–´ ë²ˆì—­ì„ ëª…í™•íˆ êµ¬ë¶„
                response_data['llm_response'] = ocr_result.llm_response  # ì˜ì–´ ì›ë¬¸
                
                if enable_translation and translation_success:
                    # ë²ˆì—­ ì„±ê³µ ì‹œ í•œêµ­ì–´ ë²ˆì—­ ì¶”ê°€
                    response_data['llm_response_korean'] = ocr_result.llm_response_korean
                    logger.info("ì‘ë‹µì— í•œêµ­ì–´ ë²ˆì—­ í¬í•¨")
                elif enable_translation and not translation_success:
                    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€
                    response_data['llm_response_korean'] = None
                    response_data['translation_error'] = translation_error if translation_error else "ë²ˆì—­ ì‹¤íŒ¨"
                    logger.info("ë²ˆì—­ ì‹¤íŒ¨ - ì˜ì–´ ì›ë¬¸ë§Œ í¬í•¨")
                else:
                    # ë²ˆì—­ ë¹„í™œì„±í™” ì‹œ
                    response_data['llm_response_korean'] = None
                    logger.info("ë²ˆì—­ ë¹„í™œì„±í™” - ì˜ì–´ ì›ë¬¸ë§Œ í¬í•¨")
                
                # ë””ë²„ê¹…ìš© ë¡œê·¸
                logger.info(f"ì‘ë‹µ ë°ì´í„° êµ¬ì„± ì™„ë£Œ:")
                logger.info(f"  - ì˜ì–´ ì›ë¬¸ ê¸¸ì´: {len(response_data.get('llm_response', ''))}")
                logger.info(f"  - í•œêµ­ì–´ ë²ˆì—­ ê¸¸ì´: {len(response_data.get('llm_response_korean', '') or '')}")
                logger.info(f"  - ë²ˆì—­ ì„±ê³µ: {response_data.get('translation_success', False)}")
                
            except Exception as e:
                logger.error(f"ì‘ë‹µ ë°ì´í„° êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
                return Response({'error': f'ì‘ë‹µ êµ¬ì„± ì‹¤íŒ¨: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ì‘ë‹µ ë°˜í™˜
            return Response(response_data, status=status.HTTP_201_CREATED)
                
        except Exception as e:
            logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
            return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
                           status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def extract_text_from_pdf_by_pages(self, pdf_path):
        """PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ì¶”ì¶œ"""
        pages = []
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                
                for i in range(total_pages):
                    page = reader.pages[i]
                    text = page.extract_text()
                    pages.append({"page": i + 1, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def ocr_pdf_by_pages(self, pdf_path, ollama_client, start_page=1, end_page=0):
        """PDFë¥¼ OCRë¡œ ì²˜ë¦¬í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        pages = []
        
        try:
            # PDF2Imageë¡œ ì´ë¯¸ì§€ ë³€í™˜
            with tempfile.TemporaryDirectory() as path:
                # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ì§€ë§Œ, convert_from_pathëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì¡°ì •
                first_page = start_page
                last_page = None if end_page <= 0 else end_page
                
                images = convert_from_path(
                    pdf_path, 
                    dpi=300, 
                    output_folder=path, 
                    first_page=first_page,
                    last_page=last_page
                )
                
                # ê° í˜ì´ì§€ ì´ë¯¸ì§€ OCR ì²˜ë¦¬
                for i, image in enumerate(images):
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
                    # OCR ìˆ˜í–‰
                    text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
                    # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚° (ì‹œì‘ í˜ì´ì§€ ê³ ë ¤)
                    page_num = start_page + i
                    pages.append({"page": page_num, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •í™” í•¨ìˆ˜"""
        if not text:
            return ""
            
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'\n\s*\n', '\n\n', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text

@method_decorator(csrf_exempt, name='dispatch')
class MultiAIProcessFileView(APIView):
    """ë©€í‹° AI ëª¨ë¸ì„ ì‚¬ìš©í•œ PDF/ì´ë¯¸ì§€ ë¶„ì„ ë·°"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            logger.info("MultiAIProcessFileView ìš”ì²­ ìˆ˜ì‹ : %s %s", request.method, request.path)
            
            # ìš”ì²­ ë°ì´í„° í™•ì¸
            if 'file' not in request.FILES:
                logger.error("íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
                return Response({'error': 'íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}, status=status.HTTP_400_BAD_REQUEST)
            
            file_obj = request.FILES['file']
            file_name = file_obj.name.lower()
            logger.info("íŒŒì¼ ì—…ë¡œë“œ: %s, í¬ê¸°: %s bytes", file_name, file_obj.size)
            
            # ë©€í‹° AI OCR ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            from .multi_ai_ocr_service import get_multi_ai_ocr_service
            multi_ai_service = get_multi_ai_ocr_service()
            
            # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OCRìš©)
            ollama_base_url = os.environ.get('OLLAMA_API_URL', 'http://localhost:11434')
            ollama_client = OllamaClient(base_url=ollama_base_url)
            
            # GPT ë²ˆì—­ê¸° ì´ˆê¸°í™”
            gpt_translator = GPTTranslator()
            
            # ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹±
            question = request.data.get('question', 'Analyze the content and summarize key information.')
            selected_models = request.data.get('selected_models', [])  # ì„ íƒëœ AI ëª¨ë¸ë“¤
            enable_translation = request.data.get('enable_translation', 'true').lower() == 'true'
            analysis_type = request.data.get('analysis_type', 'both')  # 'ocr', 'ollama', 'both'
            analyze_by_page = request.data.get('analyze_by_page', 'true').lower() == 'true'
            
            # íŒŒì¼ ìœ í˜• í™•ì¸
            if file_name.endswith(('.pdf')):
                file_type = 'pdf'
                start_page = int(request.data.get('start_page', 1))
                end_page = int(request.data.get('end_page', 0))
                logger.info("PDF ì²˜ë¦¬ ë²”ìœ„: %s ~ %s í˜ì´ì§€", start_page, end_page if end_page > 0 else "ë")
            elif file_name.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):
                file_type = 'image'
            else:
                logger.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: %s", file_name)
                return Response({'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'},
                              status=status.HTTP_400_BAD_REQUEST)
            
            # OCR ê²°ê³¼ ê°ì²´ ìƒì„±
            ocr_result = OCRResult.objects.create(file=file_obj, file_type=file_type)
            logger.info("OCRResult ê°ì²´ ìƒì„±: %s", ocr_result.id)
            
            # OCR ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
            try:
                ocr_text = ""
                page_texts = []
                
                if file_type == 'image':
                    img = Image.open(ocr_result.file.path)
                    logger.info(f"ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={img.size}, ëª¨ë“œ={img.mode}, í¬ë§·={img.format}")
                    
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(img)
                    ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    page_texts.append({"page": 1, "text": ocr_text})
                    logger.info("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì™„ë£Œ, ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: %s", len(ocr_text))
                
                elif file_type == 'pdf':
                    logger.info("PDF ì²˜ë¦¬ ì‹œì‘: %s", ocr_result.file.path)
                    
                    # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    direct_extract_success = False
                    try:
                        all_page_texts = self.extract_text_from_pdf_by_pages(ocr_result.file.path)
                        
                        if start_page > 1 or (end_page > 0 and end_page < len(all_page_texts)):
                            if start_page <= len(all_page_texts):
                                if end_page > 0 and end_page >= start_page:
                                    page_texts = all_page_texts[start_page-1:end_page]
                                else:
                                    page_texts = all_page_texts[start_page-1:]
                            else:
                                page_texts = []
                        else:
                            page_texts = all_page_texts
                        
                        combined_text = "\n".join([page["text"] for page in page_texts])
                        
                        if combined_text.strip() and len(combined_text.strip()) >= 50:
                            meaningful_chars = sum(1 for c in combined_text if c.isalnum())
                            if meaningful_chars > 30:
                                ocr_text = combined_text
                                direct_extract_success = True
                                logger.info("PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
                                          len(page_texts), len(ocr_text))
                    except Exception as e:
                        logger.error(f"PDF ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
                    # ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš°, OCR ì‹œë„
                    if not direct_extract_success:
                        logger.info("PDF OCR ì²˜ë¦¬ ì‹œì‘ (ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶ˆì¶©ë¶„)")
                        all_page_texts = self.ocr_pdf_by_pages(ocr_result.file.path, ollama_client, start_page, end_page)
                        page_texts = all_page_texts
                        ocr_text = "\n".join([page["text"] for page in page_texts])
                        logger.info("PDF OCR ì²˜ë¦¬ ì™„ë£Œ, ì´ %s í˜ì´ì§€, í…ìŠ¤íŠ¸ ê¸¸ì´: %s", 
                                  len(page_texts), len(ocr_text))
                
                # í…ìŠ¤íŠ¸ ì •í™”
                ocr_result.ocr_text = self.clean_text(ocr_text)
                
                # PDF íŒŒì¼ì€ í•­ìƒ í…ìŠ¤íŠ¸ ê´€ë ¨ ìˆìŒìœ¼ë¡œ ì„¤ì •
                if file_type == 'pdf':
                    ocr_result.text_relevant = True
                
            except Exception as e:
                logger.error("OCR ì²˜ë¦¬ ì‹¤íŒ¨: %s", str(e), exc_info=True)
                return Response({'error': f'OCR ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}, 
                               status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ë©€í‹° AI ë¶„ì„ ìˆ˜í–‰
            multi_ai_results = {}
            combined_analysis = ""
            
            try:
                # ë¶„ì„ ì˜µì…˜ êµ¬ì„±
                analysis_options = {
                    'start_page': start_page if file_type == 'pdf' else 1,
                    'end_page': end_page if file_type == 'pdf' else 1,
                    'analyze_by_page': analyze_by_page,
                    'file_type': file_type
                }
                
                # ë©€í‹° AI ë¶„ì„ ì‹¤í–‰
                multi_ai_results = multi_ai_service.analyze_file_multi_ai(
                    file_path=ocr_result.file.path,
                    file_type=file_type,
                    question=question,
                    selected_models=selected_models,
                    analysis_options=analysis_options
                )
                
                logger.info("ë©€í‹° AI ë¶„ì„ ì™„ë£Œ, ê²°ê³¼ ëª¨ë¸ ìˆ˜: %s", len(multi_ai_results))
                
                # ì‘ë‹µ ë¹„êµ ë° ê²°í•©
                comparison_result = multi_ai_service.compare_responses(multi_ai_results)
                
                # ìµœê³  ì‘ë‹µ ì„ íƒ ë˜ëŠ” ëª¨ë“  ì‘ë‹µ ê²°í•©
                if comparison_result.get('comparison', {}).get('best_response'):
                    best_model = comparison_result['comparison']['best_response']
                    if best_model in multi_ai_results and multi_ai_results[best_model].success:
                        combined_analysis = multi_ai_results[best_model].response_text
                        logger.info(f"ìµœê³  ì‘ë‹µ ëª¨ë¸ ì„ íƒ: {best_model}")
                    else:
                        # ëª¨ë“  ì„±ê³µí•œ ì‘ë‹µ ê²°í•©
                        successful_responses = [r.response_text for r in multi_ai_results.values() 
                                              if r.success and r.response_text]
                        combined_analysis = "\n\n".join(successful_responses)
                        logger.info("ëª¨ë“  ì„±ê³µí•œ ì‘ë‹µ ê²°í•©")
                else:
                    # ëª¨ë“  ì„±ê³µí•œ ì‘ë‹µ ê²°í•©
                    successful_responses = [r.response_text for r in multi_ai_results.values() 
                                          if r.success and r.response_text]
                    combined_analysis = "\n\n".join(successful_responses)
                    logger.info("ëª¨ë“  ì„±ê³µí•œ ì‘ë‹µ ê²°í•©")
                
            except Exception as e:
                logger.error("ë©€í‹° AI ë¶„ì„ ì‹¤íŒ¨: %s", str(e), exc_info=True)
                combined_analysis = f"ë©€í‹° AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
            # GPT ë²ˆì—­ ìˆ˜í–‰ (ë²ˆì—­ì´ í™œì„±í™”ëœ ê²½ìš°)
            translated_analysis = ""
            translation_success = False
            translation_error = ""
            
            if enable_translation and combined_analysis and gpt_translator.is_available:
                logger.info("GPT ë²ˆì—­ ì‹œì‘")
                try:
                    if file_type == 'pdf' and analyze_by_page and len(page_texts) > 1:
                        translation_result = gpt_translator.translate_paged_analysis(combined_analysis)
                    else:
                        translation_result = gpt_translator.translate_analysis_result(combined_analysis, file_type)
                    
                    if translation_result and translation_result.get("success"):
                        translated_analysis = translation_result["translated_analysis"]
                        translation_success = True
                        logger.info("GPT ë²ˆì—­ ì„±ê³µ")
                    else:
                        error_msg = translation_result.get('error', 'Unknown error') if translation_result else 'No translation result'
                        logger.error(f"GPT ë²ˆì—­ ì‹¤íŒ¨: {error_msg}")
                        translated_analysis = f"ë²ˆì—­ ì‹¤íŒ¨: {error_msg}"
                        translation_error = error_msg
                        
                except Exception as e:
                    logger.error(f"GPT ë²ˆì—­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                    translated_analysis = f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    translation_error = str(e)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            ocr_result.translation_enabled = enable_translation
            ocr_result.translation_success = translation_success
            ocr_result.analysis_type = analysis_type
            ocr_result.analyze_by_page = analyze_by_page
            
            # MySQL ì €ì¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •í™”
            ocr_result.llm_response = self.clean_text(combined_analysis)
            
            # ë²ˆì—­ ê²°ê³¼ë„ ì €ì¥
            if enable_translation and translated_analysis:
                if translation_success:
                    ocr_result.llm_response_korean = self.clean_text(translated_analysis)
                    ocr_result.translation_model = gpt_translator.model if gpt_translator else "unknown"
                else:
                    ocr_result.llm_response_korean = f"ë²ˆì—­ ì‹¤íŒ¨: {translation_error}"
            
            # ê²°ê³¼ ì €ì¥
            try:
                ocr_result.save()
                logger.info("OCRResult ì €ì¥ ì™„ë£Œ (ID: %s)", ocr_result.id)
            except Exception as e:
                logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                return Response({'error': f'ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
            try:
                response_data = OCRResultSerializer(ocr_result).data
                
                # ë©€í‹° AI ê´€ë ¨ ì •ë³´ ì¶”ê°€
                response_data['multi_ai_results'] = {}
                response_data['comparison_result'] = comparison_result
                response_data['selected_models'] = selected_models
                
                # ê° ëª¨ë¸ë³„ ê²°ê³¼ ì¶”ê°€
                for model_name, result in multi_ai_results.items():
                    response_data['multi_ai_results'][model_name] = {
                        'response_text': result.response_text,
                        'confidence_score': result.confidence_score,
                        'processing_time': result.processing_time,
                        'token_usage': result.token_usage,
                        'success': result.success,
                        'error': result.error
                    }
                
                # ë²ˆì—­ ê´€ë ¨ ì •ë³´
                response_data['translation_enabled'] = enable_translation
                response_data['translation_success'] = translation_success
                response_data['llm_response'] = ocr_result.llm_response  # ì˜ì–´ ì›ë¬¸
                
                if enable_translation and translation_success:
                    response_data['llm_response_korean'] = ocr_result.llm_response_korean
                elif enable_translation and not translation_success:
                    response_data['llm_response_korean'] = None
                    response_data['translation_error'] = translation_error
                else:
                    response_data['llm_response_korean'] = None
                
                logger.info(f"ë©€í‹° AI ì‘ë‹µ ë°ì´í„° êµ¬ì„± ì™„ë£Œ:")
                logger.info(f"  - ë¶„ì„ ëª¨ë¸ ìˆ˜: {len(multi_ai_results)}")
                logger.info(f"  - ì˜ì–´ ì›ë¬¸ ê¸¸ì´: {len(response_data.get('llm_response', ''))}")
                logger.info(f"  - í•œêµ­ì–´ ë²ˆì—­ ê¸¸ì´: {len(response_data.get('llm_response_korean', '') or '')}")
                
            except Exception as e:
                logger.error(f"ì‘ë‹µ ë°ì´í„° êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
                return Response({'error': f'ì‘ë‹µ êµ¬ì„± ì‹¤íŒ¨: {str(e)}'}, 
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # ì‘ë‹µ ë°˜í™˜
            return Response(response_data, status=status.HTTP_201_CREATED)
                
        except Exception as e:
            logger.error("ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: %s", str(e), exc_info=True)
            return Response({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}, 
                           status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def extract_text_from_pdf_by_pages(self, pdf_path):
        """PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ì¶”ì¶œ"""
        pages = []
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                
                for i in range(total_pages):
                    page = reader.pages[i]
                    text = page.extract_text()
                    pages.append({"page": i + 1, "text": text})
                    
            return pages
        except Exception as e:
            logger.error(f"PDF í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def ocr_pdf_by_pages(self, pdf_path, ollama_client, start_page=1, end_page=0):
        """PDFë¥¼ OCRë¡œ ì²˜ë¦¬í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        pages = []
        
        try:
            # PDF2Imageë¡œ ì´ë¯¸ì§€ ë³€í™˜
            with tempfile.TemporaryDirectory() as path:
                # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ì§€ë§Œ, convert_from_pathëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì¡°ì •
                first_page = start_page
                last_page = None if end_page <= 0 else end_page
                
                images = convert_from_path(
                    pdf_path, 
                    first_page=first_page, 
                    last_page=last_page,
                    dpi=200
                )
                
                for i, image in enumerate(images):
                    page_num = start_page + i
                    
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    preprocessed_img = ollama_client.preprocess_image_for_ocr(image)
                    
                    # OCR ìˆ˜í–‰
                    ocr_text = ollama_client.get_optimized_ocr_text(preprocessed_img, lang='kor+eng')
                    
                    pages.append({"page": page_num, "text": ocr_text})
                    
                logger.info(f"PDF OCR ì²˜ë¦¬ ì™„ë£Œ: {len(pages)} í˜ì´ì§€")
                
        except Exception as e:
            logger.error(f"PDF OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            raise
        
        return pages
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •í™” í•¨ìˆ˜"""
        if not text:
            return ""
            
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'\n\s*\n', '\n\n', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
# chat/views.pyì— ì¶”ê°€í•  ë·°ë“¤

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated, AllowAny
from django.shortcuts import get_object_or_404
from datetime import datetime, timedelta
import json
import re
from .models import Schedule, ScheduleRequest, ConflictResolution
from .serializers import (
    ScheduleSerializer, ScheduleRequestSerializer, 
    ConflictResolutionSerializer, ScheduleRequestInputSerializer
)

# ê¸°ì¡´ ChatBot í´ë˜ìŠ¤ì™€ ChatViewëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€...
# chat/views.py ìˆ˜ì • ë²„ì „

# from rest_framework.views import APIView
# from rest_framework.response import Response
# from rest_framework import status
# from rest_framework.decorators import api_view, permission_classes
# from rest_framework.permissions import IsAuthenticated, AllowAny
# from django.shortcuts import get_object_or_404
# from datetime import datetime, timedelta
# import json
# import re
# from .models import Schedule, ScheduleRequest, ConflictResolution
# from .serializers import (
#     ScheduleSerializer, ScheduleRequestSerializer, 
#     ConflictResolutionSerializer, ScheduleRequestInputSerializer
# )

# # ê¸°ì¡´ ChatBot í´ë˜ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€...


# chatbots = {
#     'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
#     'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
#     'mixtral': ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
# }

# # ë°±ì—”ë“œ views.pyì— ì¶”ê°€í•  í•¨ìˆ˜ë“¤

# def parse_date_from_request(request_text):
#     """ìì—°ì–´ ë‚ ì§œë¥¼ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜"""
#     today = datetime.now().date()
    
#     # ì˜¤ëŠ˜/ë‚´ì¼/ëª¨ë ˆ ë“± í•œêµ­ì–´ ë‚ ì§œ í‘œí˜„ ì²˜ë¦¬
#     if 'ì˜¤ëŠ˜' in request_text:
#         return today
#     elif 'ë‚´ì¼' in request_text:
#         return today + timedelta(days=1)
#     elif 'ëª¨ë ˆ' in request_text or 'ëª¨ë˜' in request_text:
#         return today + timedelta(days=2)
#     elif 'ì´ë²ˆ ì£¼' in request_text:
#         # ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼ë¡œ ì„¤ì •
#         days_until_friday = (4 - today.weekday()) % 7
#         if days_until_friday == 0:  # ì˜¤ëŠ˜ì´ ê¸ˆìš”ì¼ì´ë©´ ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼
#             days_until_friday = 7
#         return today + timedelta(days=days_until_friday)
#     elif 'ë‹¤ìŒ ì£¼' in request_text:
#         return today + timedelta(days=7)
#     else:
#         # ê¸°ë³¸ê°’: ë‚´ì¼
#         return today + timedelta(days=1)

# def parse_multiple_schedules_backend(request_text):
#     """ë°±ì—”ë“œì—ì„œ ì—¬ëŸ¬ ì¼ì • íŒŒì‹±"""
#     # ì‰¼í‘œ, "ê·¸ë¦¬ê³ ", "ë°" ë“±ìœ¼ë¡œ ë¶„ë¦¬
#     separators = [',', 'ï¼Œ', 'ê·¸ë¦¬ê³ ', 'ë°', 'ì™€', 'ê³¼']
    
#     parts = [request_text]
#     for sep in separators:
#         new_parts = []
#         for part in parts:
#             new_parts.extend(part.split(sep))
#         parts = new_parts
    
#     # ì •ë¦¬ëœ ìš”ì²­ë“¤ ë°˜í™˜
#     cleaned_requests = []
#     for part in parts:
#         cleaned = part.strip()
#         if cleaned and len(cleaned) > 2:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
#             cleaned_requests.append(cleaned)
    
#     return cleaned_requests if len(cleaned_requests) > 1 else [request_text]
# class ScheduleOptimizerBot:
#     """ì¼ì • ìµœì í™”ë¥¼ ìœ„í•œ AI ë´‡ í´ë˜ìŠ¤ - ì—¬ëŸ¬ AI ëª¨ë¸ ì—°ë™"""
    
#     def __init__(self):
#         self.chatbots = {
#                 'gpt': ChatBot(OPENAI_API_KEY, 'gpt-3.5-turbo', 'openai'),
#                 'claude': ChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-20241022', 'anthropic'), 
#                 'mixtral': ChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
#             }
        
#     def create_schedule_prompt(self, request_text, user_context=None, existing_schedules=None):
#         """ì¼ì • ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± - ë¹ˆ ì‹œê°„ ë¶„ì„ í¬í•¨"""
#         base_prompt = f"""
#         ì‚¬ìš©ìì˜ ì¼ì • ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê¸°ì¡´ ì¼ì •ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ìµœì ì˜ ë¹ˆ ì‹œê°„ì„ ì°¾ì•„ ì œì•ˆí•´ì£¼ì„¸ìš”.

#         ìš”ì²­ ë‚´ìš©: {request_text}
        
#         ê¸°ì¡´ ì¼ì •ë“¤: {existing_schedules or []}
        
#         ë¶„ì„ ë°©ë²•:
#         1. ê¸°ì¡´ ì¼ì •ë“¤ì˜ ì‹œê°„ëŒ€ë¥¼ í™•ì¸í•˜ì—¬ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚ ì˜ ë¹ˆ ì‹œê°„ì„ ì°¾ì•„ì£¼ì„¸ìš”
#         2. ìš”ì²­ëœ ì¼ì •ì˜ ì„±ê²©ì— ë§ëŠ” ìµœì ì˜ ì‹œê°„ëŒ€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”
#         3. ì¼ì • ê°„ ì—¬ìœ  ì‹œê°„ë„ ê³ ë ¤í•´ì£¼ì„¸ìš”
        
#         ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
#         {{
#             "title": "ì¼ì • ì œëª©",
#             "description": "ìƒì„¸ ì„¤ëª…",
#             "suggested_date": "YYYY-MM-DD",
#             "suggested_start_time": "HH:MM",
#             "suggested_end_time": "HH:MM",
#             "location": "ì¥ì†Œ (ì„ íƒì‚¬í•­)",
#             "priority": "HIGH/MEDIUM/LOW/URGENT",
#             "attendees": ["ì°¸ì„ì1", "ì°¸ì„ì2"],
#             "reasoning": "ì´ ì‹œê°„ì„ ì œì•ˆí•˜ëŠ” ì´ìœ  (ë¹ˆ ì‹œê°„ ë¶„ì„ ê²°ê³¼ í¬í•¨)"
#         }}
        
#         ì‚¬ìš©ìì˜ ë§¥ë½ ì •ë³´: {user_context or "ì—†ìŒ"}
#         """
#         return base_prompt

#     def create_conflict_resolution_prompt(self, conflicting_schedules, new_request):
#         """ì¼ì • ì¶©ëŒ í•´ê²°ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
#         prompt = f"""
#         ê¸°ì¡´ ì¼ì •ê³¼ ìƒˆë¡œìš´ ì¼ì • ìš”ì²­ ì‚¬ì´ì— ì¶©ëŒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 
#         ì—¬ëŸ¬ AIì˜ ê´€ì ì—ì„œ ìµœì ì˜ í•´ê²° ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

#         ê¸°ì¡´ ì¶©ëŒ ì¼ì •ë“¤:
#         {json.dumps(conflicting_schedules, ensure_ascii=False, indent=2)}

#         ìƒˆë¡œìš´ ì¼ì • ìš”ì²­: {new_request}

#         ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•´ê²° ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”:
#         {{
#             "resolution_options": [
#                 {{
#                     "option": "ë°©ì•ˆ 1",
#                     "description": "ìƒì„¸ ì„¤ëª…",
#                     "impact": "ì˜í–¥ë„ ë¶„ì„",
#                     "recommended": true/false
#                 }},
#                 {{
#                     "option": "ë°©ì•ˆ 2", 
#                     "description": "ìƒì„¸ ì„¤ëª…",
#                     "impact": "ì˜í–¥ë„ ë¶„ì„",
#                     "recommended": true/false
#                 }}
#             ],
#             "best_recommendation": "ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë°©ì•ˆê³¼ ì´ìœ "
#         }}
#         """
#         return prompt
    
#     def get_ai_suggestions(self, prompt, suggestion_type="schedule"):
#         """ì—¬ëŸ¬ AI ëª¨ë¸ë¡œë¶€í„° ì œì•ˆë°›ê¸°"""
#         suggestions = {}
        
#         for model_name, chatbot in self.chatbots.items():
#             try:
#                 response = chatbot.chat(prompt)
#                 suggestions[f"{model_name}_suggestion"] = response
#             except Exception as e:
#                 suggestions[f"{model_name}_suggestion"] = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        
#         return suggestions
    
#     def analyze_and_optimize_suggestions(self, suggestions, query, selected_models=['GPT', 'Claude', 'Mixtral']):
#         """ì—¬ëŸ¬ AI ì œì•ˆì„ ë¶„ì„í•˜ì—¬ ìµœì í™”ëœ ê²°ê³¼ ìƒì„± - ê¸°ì¡´ analyze_responses í™œìš©"""
#         try:
#             # ChatBotì˜ analyze_responses ê¸°ëŠ¥ í™œìš©
#             analyzer = self.chatbots['claude']  # Claudeë¥¼ ë¶„ì„ìš©ìœ¼ë¡œ ì‚¬ìš©
            
#             # ì œì•ˆì„ ë¶„ì„ìš© í˜•íƒœë¡œ ë³€í™˜
#             responses_for_analysis = {}
#             for key, suggestion in suggestions.items():
#                 model_name = key.replace('_suggestion', '')
#                 responses_for_analysis[model_name] = suggestion
            
#             # ê¸°ì¡´ analyze_responses ë©”ì„œë“œ í™œìš©
#             analysis_result = analyzer.analyze_responses(
#                 responses_for_analysis, 
#                 query, 
#                 'Korean',  # ê¸°ë³¸ ì–¸ì–´
#                 selected_models
#             )
            
#             # JSON ì‘ë‹µì—ì„œ ìµœì í™”ëœ ì¼ì • ì •ë³´ ì¶”ì¶œ
#             try:
#                 # best_responseì—ì„œ JSON ë¶€ë¶„ ì¶”ì¶œ
#                 json_match = re.search(r'\{.*\}', analysis_result.get('best_response', ''), re.DOTALL)
#                 if json_match:
#                     optimized = json.loads(json_match.group())
#                 else:
#                     # fallback: ì²« ë²ˆì§¸ ìœ íš¨í•œ ì œì•ˆ ì‚¬ìš©
#                     optimized = self._extract_first_valid_suggestion(suggestions)
#             except:
#                 optimized = self._extract_first_valid_suggestion(suggestions)
            
#             confidence = self._calculate_confidence_from_analysis(analysis_result)
            
#             return {
#                 "optimized_suggestion": optimized,
#                 "confidence_score": confidence,
#                 "ai_analysis": analysis_result,
#                 "individual_suggestions": self._parse_individual_suggestions(suggestions)
#             }
            
#         except Exception as e:
#             print(f"Analysis error: {str(e)}")
#             return {"error": f"ìµœì í™” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
#     def _extract_first_valid_suggestion(self, suggestions):
#         """ì²« ë²ˆì§¸ ìœ íš¨í•œ ì œì•ˆ ì¶”ì¶œ"""
#         for key, suggestion in suggestions.items():
#             try:
#                 json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
#                 if json_match:
#                     return json.loads(json_match.group())
#             except:
#                 continue
        
#         # ê¸°ë³¸ ì œì•ˆ ë°˜í™˜
#         return {
#             "title": "ìƒˆ ì¼ì •",
#             "description": "AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤",
#             "suggested_date": datetime.now().strftime('%Y-%m-%d'),
#             "suggested_start_time": "09:00",
#             "suggested_end_time": "10:00",
#             "location": "",
#             "priority": "MEDIUM",
#             "attendees": [],
#             "reasoning": "ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ì œì•ˆì„ ì¢…í•©í•œ ê²°ê³¼ì…ë‹ˆë‹¤."
#         }
    
#     def _calculate_confidence_from_analysis(self, analysis_result):
#         """ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹ ë¢°ë„ ê³„ì‚°"""
#         reasoning = analysis_result.get('reasoning', '')
        
#         # í‚¤ì›Œë“œ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
#         confidence_keywords = ['ì¼ì¹˜', 'ê³µí†µ', 'ì •í™•', 'ìµœì ', 'ì¶”ì²œ']
#         uncertainty_keywords = ['ë¶ˆí™•ì‹¤', 'ì¶”ì •', 'ê°€ëŠ¥ì„±', 'ì–´ë ¤ì›€']
        
#         confidence_score = 0.5  # ê¸°ë³¸ê°’
        
#         for keyword in confidence_keywords:
#             if keyword in reasoning:
#                 confidence_score += 0.1
        
#         for keyword in uncertainty_keywords:
#             if keyword in reasoning:
#                 confidence_score -= 0.1
        
#         return max(0.1, min(1.0, confidence_score))
    
#     def _parse_individual_suggestions(self, suggestions):
#         """ê°œë³„ ì œì•ˆë“¤ì„ íŒŒì‹±"""
#         parsed = []
#         for key, suggestion in suggestions.items():
#             try:
#                 json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
#                 if json_match:
#                     parsed_suggestion = json.loads(json_match.group())
#                     parsed_suggestion['source'] = key.replace('_suggestion', '')
#                     parsed.append(parsed_suggestion)
#             except:
#                 continue
#         return parsed

# class ScheduleManagementView(APIView):
#     """ì¼ì • ê´€ë¦¬ ë©”ì¸ ë·° - ê¶Œí•œ ìˆ˜ì •"""
#     # ì„ì‹œë¡œ AllowAnyë¡œ ë³€ê²½ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
#     permission_classes = [IsAuthenticated]
    
#     def __init__(self):
#         super().__init__()
#         self.optimizer = ScheduleOptimizerBot()
    
#     def get(self, request):
#         """ì‚¬ìš©ìì˜ ì¼ì • ëª©ë¡ ì¡°íšŒ"""
#         # ğŸš« ê¸°ì¡´ ë”ë¯¸ ì‚¬ìš©ì ë¡œì§ ì œê±°
#         if not request.user.is_authenticated:
#             return Response({'error': 'ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.'}, status=status.HTTP_401_UNAUTHORIZED)
        
#         schedules = Schedule.objects.filter(user=request.user).order_by('start_time')
        
#         # ë‚ ì§œ í•„í„°ë§ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
#         start_date = request.query_params.get('start_date')
#         end_date = request.query_params.get('end_date')
        
#         if start_date:
#             schedules = schedules.filter(start_time__date__gte=start_date)
#         if end_date:
#             schedules = schedules.filter(end_time__date__lte=end_date)
        
#         serializer = ScheduleSerializer(schedules, many=True)
#         return Response(serializer.data)
#     def post(self, request):
#         """ìƒˆë¡œìš´ ì¼ì • ìƒì„± ìš”ì²­ - ì—¬ëŸ¬ ì¼ì • ì§€ì› ê°œì„ """
#         try:
#             request_text = request.data.get('request_text', '')
#             existing_schedules = request.data.get('existing_schedules', [])
            
#             if not request_text:
#                 return Response({'error': 'ìš”ì²­ í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, 
#                             status=status.HTTP_400_BAD_REQUEST)
#             if not request.user.is_authenticated:
#                 return Response({'error': 'ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.'}, status=status.HTTP_401_UNAUTHORIZED)
        
#             user = request.user

         
            
#             # ì—¬ëŸ¬ ì¼ì • ìš”ì²­ì¸ì§€ í™•ì¸
#             schedule_requests = parse_multiple_schedules_backend(request_text)
#             target_date = parse_date_from_request(request_text)
            
#             if len(schedule_requests) > 1:
#                 # ì—¬ëŸ¬ ì¼ì • ì²˜ë¦¬
#                 multiple_schedules = []
#                 all_individual_suggestions = []
                
#                 for i, single_request in enumerate(schedule_requests):
#                     # ê° ì¼ì •ì˜ ì‹œì‘ ì‹œê°„ì„ ë‹¤ë¥´ê²Œ ì„¤ì •
#                     schedule_date = target_date
#                     if i > 0:  # ë‘ ë²ˆì§¸ ì¼ì •ë¶€í„°ëŠ” 2ì‹œê°„ì”© ë’¤ë¡œ
#                         base_hour = 9 + (i * 2)
#                     else:
#                         base_hour = 9
                    
#                     # ê°œë³„ ì¼ì • ìƒì„±
#                     optimized_schedule = {
#                         "title": self._extract_schedule_title(single_request),
#                         "description": f"AIê°€ ë¶„ì„í•œ {self._extract_schedule_title(single_request)} ì¼ì •ì…ë‹ˆë‹¤.",
#                         "suggested_date": schedule_date.strftime('%Y-%m-%d'),
#                         "suggested_start_time": f"{base_hour:02d}:00",
#                         "suggested_end_time": f"{base_hour + 2:02d}:00",
#                         "location": self._extract_schedule_location(single_request),
#                         "priority": "HIGH",
#                         "attendees": [],
#                         "reasoning": f"{i + 1}ë²ˆì§¸ ì¼ì •: {single_request}. ê¸°ì¡´ ì¼ì •ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ì‹œê°„ìœ¼ë¡œ ë°°ì •í–ˆìŠµë‹ˆë‹¤."
#                     }
#                     multiple_schedules.append(optimized_schedule)
                    
#                     # ê° AIë³„ ê°œë³„ ì œì•ˆ ìƒì„±
#                     for ai_type in ['gpt', 'claude', 'mixtral']:
#                         individual_suggestion = optimized_schedule.copy()
#                         individual_suggestion['source'] = ai_type
#                         individual_suggestion['reasoning'] = f"{ai_type.upper()}ê°€ ë¶„ì„í•œ {self._extract_schedule_title(single_request)} ìµœì  ì‹œê°„ì…ë‹ˆë‹¤."
#                         all_individual_suggestions.append(individual_suggestion)
                
#                 # ì—¬ëŸ¬ ì¼ì • ì‘ë‹µ ìƒì„±
#                 response_data = {
#                     'request_id': int(datetime.now().timestamp()),
#                     'multiple_schedules': multiple_schedules,
#                     'optimized_suggestion': multiple_schedules[0],
#                     'confidence_score': 0.92,
#                     'individual_suggestions': all_individual_suggestions,
#                     'ai_analysis': {
#                         'analysis_summary': f"ì´ {len(schedule_requests)}ê°œì˜ ì¼ì •ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì‹œê°„ëŒ€ë¡œ ë°°ì •í–ˆìŠµë‹ˆë‹¤.",
#                         'reasoning': f"ì—¬ëŸ¬ ì¼ì •ì„ {target_date.strftime('%Yë…„ %mì›” %dì¼')}ì— ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë°°ì¹˜í•˜ì—¬ ì¶©ëŒì„ ë°©ì§€í–ˆìŠµë‹ˆë‹¤.",
#                         'models_used': ["gpt", "claude", "mixtral"]
#                     },
#                     'has_conflicts': False,
#                     'conflicts': [],
#                     'analysis_summary': f"{len(schedule_requests)}ê°œ ì¼ì •ì— ëŒ€í•´ 3ê°œ AI ëª¨ë¸ì´ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
#                     'is_multiple_schedule': True
#                 }
                
#             else:
#                 # ë‹¨ì¼ ì¼ì • ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©í•˜ë˜ ë‚ ì§œ ë°˜ì˜)
#                 user_context = self._get_user_context(user)
                
#                 # ë‚ ì§œê°€ ë°˜ì˜ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
#                 enhanced_prompt = f"""
#                 ì‚¬ìš©ìì˜ ì¼ì • ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê¸°ì¡´ ì¼ì •ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ìµœì ì˜ ë¹ˆ ì‹œê°„ì„ ì°¾ì•„ ì œì•ˆí•´ì£¼ì„¸ìš”.
                
#                 ìš”ì²­ ë‚´ìš©: {request_text}
#                 ëª©í‘œ ë‚ ì§œ: {target_date.strftime('%Yë…„ %mì›” %dì¼')} ({self._get_weekday_korean(target_date)})
#                 ê¸°ì¡´ ì¼ì •ë“¤: {existing_schedules or []}
                
#                 ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
#                 {{
#                     "title": "ì¼ì • ì œëª©",
#                     "description": "ìƒì„¸ ì„¤ëª…",
#                     "suggested_date": "{target_date.strftime('%Y-%m-%d')}",
#                     "suggested_start_time": "HH:MM",
#                     "suggested_end_time": "HH:MM",
#                     "location": "ì¥ì†Œ",
#                     "priority": "HIGH/MEDIUM/LOW/URGENT",
#                     "attendees": [],
#                     "reasoning": "ì´ ì‹œê°„ì„ ì œì•ˆí•˜ëŠ” ì´ìœ "
#                 }}
#                 """
                
#                 # ê¸°ì¡´ ë‹¨ì¼ ì¼ì • ë¡œì§ ê³„ì†...
#                 suggestions = self.optimizer.get_ai_suggestions(enhanced_prompt)
#                 optimized_result = self.optimizer.analyze_and_optimize_suggestions(
#                     suggestions, f"ì¼ì • ìš”ì²­: {request_text}"
#                 )
                
#                 response_data = {
#                     'request_id': int(datetime.now().timestamp()),
#                     'optimized_suggestion': optimized_result.get('optimized_suggestion', {}),
#                     'confidence_score': optimized_result.get('confidence_score', 0.0),
#                     'ai_analysis': optimized_result.get('ai_analysis', {}),
#                     'individual_suggestions': optimized_result.get('individual_suggestions', []),
#                     'has_conflicts': False,
#                     'conflicts': [],
#                     'analysis_summary': "3ê°œ AI ëª¨ë¸ì´ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
#                     'is_multiple_schedule': False
#                 }
            
#             return Response(response_data, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#                     return Response({
#                         'error': f'ì¼ì • ìƒì„± ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'
#                     }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

#     def _extract_schedule_title(self, request):
#             """ìš”ì²­ì—ì„œ ì¼ì • ì œëª© ì¶”ì¶œ"""
#             if 'ìš´ë™' in request:
#                 return 'ìš´ë™'
#             elif 'ë¯¸íŒ…' in request or 'íšŒì˜' in request:
#                 return 'íŒ€ ë¯¸íŒ…'
#             elif 'ê³µë¶€' in request or 'í•™ìŠµ' in request:
#                 return 'í•™ìŠµ ì‹œê°„'
#             elif 'ì‘ì—…' in request or 'ì—…ë¬´' in request:
#                 return 'ì§‘ì¤‘ ì‘ì—…'
#             elif 'ì•½ì†' in request:
#                 return 'ì•½ì†'
#             else:
#                 return 'ìƒˆ ì¼ì •'

#     def _extract_schedule_location(self, request):
#             """ìš”ì²­ì—ì„œ ì¥ì†Œ ì¶”ì¶œ"""
#             if 'ìš´ë™' in request:
#                 return 'í—¬ìŠ¤ì¥'
#             elif 'ë¯¸íŒ…' in request or 'íšŒì˜' in request:
#                 return 'íšŒì˜ì‹¤'
#             elif 'ê³µë¶€' in request or 'í•™ìŠµ' in request:
#                 return 'ë„ì„œê´€'
#             elif 'ì»¤í”¼' in request:
#                 return 'ì¹´í˜'
#             else:
#                 return 'ì‚¬ë¬´ì‹¤'

#     def _get_weekday_korean(self, date):
#             """ìš”ì¼ì„ í•œêµ­ì–´ë¡œ ë°˜í™˜"""
#             weekdays = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
#             return weekdays[date.weekday()]
            
   
#     def _check_schedule_conflicts(self, user, suggestion):
#         """ì¼ì • ì¶©ëŒ ê²€ì‚¬"""
#         if not suggestion or 'suggested_date' not in suggestion:
#             return []
        
#         try:
#             suggested_date = datetime.strptime(suggestion['suggested_date'], '%Y-%m-%d').date()
#             start_time = datetime.strptime(suggestion.get('suggested_start_time', '09:00'), '%H:%M').time()
#             end_time = datetime.strptime(suggestion.get('suggested_end_time', '10:00'), '%H:%M').time()
            
#             suggested_start = datetime.combine(suggested_date, start_time)
#             suggested_end = datetime.combine(suggested_date, end_time)
            
#             conflicts = Schedule.objects.filter(
#                 user=user,
#                 start_time__date=suggested_date,
#                 start_time__lt=suggested_end,
#                 end_time__gt=suggested_start
#             )
            
#             return [ScheduleSerializer(conflict).data for conflict in conflicts]
            
#         except Exception as e:
#             return []

# # ê¶Œí•œ ìˆ˜ì •ëœ í•¨ìˆ˜ë“¤
# @api_view(['POST'])
# @permission_classes([IsAuthenticated])  # ğŸ”§ ê¶Œí•œ ë³€ê²½
# def confirm_schedule(request, request_id):
#     """AI ì œì•ˆëœ ì¼ì •ì„ í™•ì •í•˜ì—¬ ì‹¤ì œ ì¼ì •ìœ¼ë¡œ ìƒì„±"""
#     try:
#         user = request.user
        
#         # ğŸš« ScheduleRequest.DoesNotExistì—ì„œ ë”ë¯¸ ë°ì´í„° ìƒì„± ì œê±°
#         try:
#             schedule_request = ScheduleRequest.objects.get(id=request_id, user=user)
#             optimized_suggestion = json.loads(schedule_request.optimized_suggestion)
#         except ScheduleRequest.DoesNotExist:
#             return Response({
#                 'error': f'ìš”ì²­ ID {request_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
#             }, status=status.HTTP_404_NOT_FOUND)
#                 # ë‚ ì§œ/ì‹œê°„ íŒŒì‹± ê°œì„ 
#         try:
#             suggested_date = optimized_suggestion.get('suggested_date')
#             suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
#             suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
            
#             # ë‚ ì§œ í˜•ì‹ í™•ì¸ ë° ë³€í™˜
#             if isinstance(suggested_date, str):
#                 if 'T' in suggested_date:  # ISO í˜•ì‹ì¸ ê²½ìš°
#                     suggested_date = suggested_date.split('T')[0]
                
#                 start_datetime = datetime.strptime(
#                     f"{suggested_date} {suggested_start_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#                 end_datetime = datetime.strptime(
#                     f"{suggested_date} {suggested_end_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#             else:
#                 # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ë¡œ ì„¤ì •
#                 today = datetime.now().date()
#                 start_datetime = datetime.strptime(
#                     f"{today} {suggested_start_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
#                 end_datetime = datetime.strptime(
#                     f"{today} {suggested_end_time}",
#                     '%Y-%m-%d %H:%M'
#                 )
                
#         except (ValueError, TypeError) as e:
#             print(f"DateTime parsing error: {e}")
#             # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
#             now = datetime.now()
#             start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
#             end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
        
#         # Schedule ê°ì²´ ìƒì„±
#         schedule_data = {
#             'user': user,
#             'title': optimized_suggestion.get('title', 'ìƒˆ ì¼ì •'),
#             'description': optimized_suggestion.get('description', 'AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤.'),
#             'start_time': start_datetime,
#             'end_time': end_datetime,
#             'location': optimized_suggestion.get('location', ''),
#             'priority': optimized_suggestion.get('priority', 'MEDIUM'),
#             'attendees': json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
#         }
        
#         schedule = Schedule.objects.create(**schedule_data)
#         serializer = ScheduleSerializer(schedule)
        
#         print(f"Schedule created successfully: {schedule.id}")
        
#         return Response({
#             'message': 'ì—¬ëŸ¬ AIì˜ ë¶„ì„ì„ í†µí•´ ìµœì í™”ëœ ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.',
#             'schedule': serializer.data
#         }, status=status.HTTP_201_CREATED)
        
#     except Exception as e:
#         print(f"Confirm schedule error: {str(e)}")
#         import traceback
#         traceback.print_exc()
        
#         return Response({
#             'error': f'ì¼ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
#         }, status=status.HTTP_400_BAD_REQUEST)


# # Alternative solution: Convert to Class-Based View
# class ConfirmScheduleView(APIView):
#     """AI ì œì•ˆëœ ì¼ì •ì„ í™•ì •í•˜ì—¬ ì‹¤ì œ ì¼ì •ìœ¼ë¡œ ìƒì„±"""
#     permission_classes = [AllowAny]  # ì„ì‹œë¡œ AllowAny
    
#     def post(self, request, request_id):
#         try:
#             # ì‚¬ìš©ì ì²˜ë¦¬
#             if not request.user.is_authenticated:
#                 return Response({'error': 'ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.'}, status=status.HTTP_401_UNAUTHORIZED)

#             user = request.user
            
#             # request_idë¡œ ScheduleRequestë¥¼ ì°¾ê±°ë‚˜ ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬
#             try:
#                 schedule_request = ScheduleRequest.objects.get(id=request_id, user=user)
#                 optimized_suggestion = json.loads(schedule_request.optimized_suggestion)
#             except ScheduleRequest.DoesNotExist:
#                 # ë”ë¯¸ ëª¨ë“œ: request_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ì¼ì • ìƒì„±
#                 print(f"ScheduleRequest {request_id} not found, creating dummy schedule")
#                 optimized_suggestion = {
#                     'title': 'AI ì œì•ˆ ì¼ì •',
#                     'description': 'AIê°€ ì œì•ˆí•œ ìµœì ì˜ ì¼ì •ì…ë‹ˆë‹¤.',
#                     'suggested_date': datetime.now().strftime('%Y-%m-%d'),
#                     'suggested_start_time': '09:00',
#                     'suggested_end_time': '10:00',
#                     'location': 'ì‚¬ë¬´ì‹¤',
#                     'priority': 'MEDIUM',
#                     'attendees': []
#                 }
#             except json.JSONDecodeError as e:
#                 print(f"JSON decode error: {e}")
#                 return Response({
#                     'error': f'ì¼ì • ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {str(e)}'
#                 }, status=status.HTTP_400_BAD_REQUEST)
            
#             # ë‚ ì§œ/ì‹œê°„ íŒŒì‹±
#             try:
#                 suggested_date = optimized_suggestion.get('suggested_date')
#                 suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
#                 suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
                
#                 # ë‚ ì§œ í˜•ì‹ í™•ì¸ ë° ë³€í™˜
#                 if isinstance(suggested_date, str):
#                     if 'T' in suggested_date:  # ISO í˜•ì‹ì¸ ê²½ìš°
#                         suggested_date = suggested_date.split('T')[0]
                    
#                     start_datetime = datetime.strptime(
#                         f"{suggested_date} {suggested_start_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                     end_datetime = datetime.strptime(
#                         f"{suggested_date} {suggested_end_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                 else:
#                     # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ë¡œ ì„¤ì •
#                     today = datetime.now().date()
#                     start_datetime = datetime.strptime(
#                         f"{today} {suggested_start_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
#                     end_datetime = datetime.strptime(
#                         f"{today} {suggested_end_time}",
#                         '%Y-%m-%d %H:%M'
#                     )
                    
#             except (ValueError, TypeError) as e:
#                 print(f"DateTime parsing error: {e}")
#                 # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
#                 now = datetime.now()
#                 start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
#                 end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
            
#             # Schedule ê°ì²´ ìƒì„±
#             schedule_data = {
#                 'user': user,
#                 'title': optimized_suggestion.get('title', 'ìƒˆ ì¼ì •'),
#                 'description': optimized_suggestion.get('description', 'AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤.'),
#                 'start_time': start_datetime,
#                 'end_time': end_datetime,
#                 'location': optimized_suggestion.get('location', ''),
#                 'priority': optimized_suggestion.get('priority', 'MEDIUM'),
#                 'attendees': json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
#             }
            
#             schedule = Schedule.objects.create(**schedule_data)
#             serializer = ScheduleSerializer(schedule)
            
#             print(f"Schedule created successfully: {schedule.id}")
            
#             return Response({
#                 'message': 'ì—¬ëŸ¬ AIì˜ ë¶„ì„ì„ í†µí•´ ìµœì í™”ëœ ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.',
#                 'schedule': serializer.data
#             }, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#             print(f"Confirm schedule error: {str(e)}")
#             import traceback
#             traceback.print_exc()
            
#             return Response({
#                 'error': f'ì¼ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
#             }, status=status.HTTP_400_BAD_REQUEST)


# # Fix for resolve_schedule_conflict function
# @api_view(['POST'])
# @permission_classes([IsAuthenticated])  # ğŸ”§ ê¶Œí•œ ë³€ê²½
# def resolve_schedule_conflict(request):
#     """ì¼ì • ì¶©ëŒ í•´ê²° ë°©ì•ˆ ì œê³µ"""
#     # ğŸš« ë”ë¯¸ ì‚¬ìš©ì ë¡œì§ ì œê±°
#     user = request.user
    
#     conflicting_schedule_ids = request.data.get('conflicting_schedule_ids', [])
#     new_request = request.data.get('new_request', '')
    
#     # ë‚˜ë¨¸ì§€ ë¡œì§ì€ ê·¸ëŒ€ë¡œ...
    
#     if not conflicting_schedule_ids or not new_request:
#         return Response({
#             'error': 'ì¶©ëŒ ì¼ì • IDì™€ ìƒˆë¡œìš´ ìš”ì²­ì´ í•„ìš”í•©ë‹ˆë‹¤.'
#         }, status=status.HTTP_400_BAD_REQUEST)
    
#     try:
#         # ì‚¬ìš©ì ì²˜ë¦¬
#         if request.user.is_authenticated:
#             user = request.user
#         else:
#             from django.contrib.auth.models import User
#             user, created = User.objects.get_or_create(
#                 username='dummy_user',
#                 defaults={'email': 'dummy@example.com'}
#             )
        
#         # ì¶©ëŒ ì¼ì •ë“¤ ì¡°íšŒ
#         conflicting_schedules = Schedule.objects.filter(
#             id__in=conflicting_schedule_ids,
#             user=user
#         )
        
#         conflicting_data = [ScheduleSerializer(schedule).data for schedule in conflicting_schedules]
        
#         # ë‹¤ì¤‘ AI ëª¨ë¸ë“¤ë¡œë¶€í„° í•´ê²° ë°©ì•ˆ ë°›ê¸°
#         optimizer = ScheduleOptimizerBot()
#         prompt = optimizer.create_conflict_resolution_prompt(conflicting_data, new_request)
#         suggestions = optimizer.get_ai_suggestions(prompt, "conflict_resolution")
        
#         # AI ë¶„ì„ì„ í†µí•œ ìµœì  í•´ê²°ë°©ì•ˆ ë„ì¶œ
#         analysis_result = optimizer.analyze_and_optimize_suggestions(
#             suggestions,
#             f"ì¶©ëŒ í•´ê²°: {new_request}"
#         )
        
#         # í•´ê²° ë°©ì•ˆ ì €ì¥
#         conflict_resolution = ConflictResolution.objects.create(
#             user=user,
#             conflicting_schedules=json.dumps(conflicting_data, ensure_ascii=False),
#             resolution_options=json.dumps(suggestions, ensure_ascii=False),
#             ai_recommendations=json.dumps(analysis_result, ensure_ascii=False)
#         )
        
#         return Response({
#             'resolution_id': conflict_resolution.id,
#             'conflicting_schedules': conflicting_data,
#             'ai_suggestions': suggestions,
#             'optimized_resolution': analysis_result,
#             'message': f'{len(suggestions)}ê°œ AI ëª¨ë¸ì´ ë¶„ì„í•œ ì¶©ëŒ í•´ê²° ë°©ì•ˆì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.'
#         }, status=status.HTTP_201_CREATED)
        
#     except Exception as e:
#         return Response({
#             'error': f'ì¶©ëŒ í•´ê²° ë°©ì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
#         }, status=status.HTTP_400_BAD_REQUEST)


# # Alternative Class-Based View for conflict resolution
# class ResolveScheduleConflictView(APIView):
#     """ì¼ì • ì¶©ëŒ í•´ê²° ë°©ì•ˆ ì œê³µ - ë‹¤ì¤‘ AI ë¶„ì„"""
#     permission_classes = [IsAuthenticated]
    
#     def post(self, request):
#         conflicting_schedule_ids = request.data.get('conflicting_schedule_ids', [])
#         new_request = request.data.get('new_request', '')
        
#         if not conflicting_schedule_ids or not new_request:
#             return Response({
#                 'error': 'ì¶©ëŒ ì¼ì • IDì™€ ìƒˆë¡œìš´ ìš”ì²­ì´ í•„ìš”í•©ë‹ˆë‹¤.'
#             }, status=status.HTTP_400_BAD_REQUEST)
        
#         try:
#             # ì‚¬ìš©ì ì²˜ë¦¬
#             if request.user.is_authenticated:
#                 user = request.user
#             else:
#                 from django.contrib.auth.models import User
#                 user, created = User.objects.get_or_create(
#                     username='dummy_user',
#                     defaults={'email': 'dummy@example.com'}
#                 )
            
#             # ì¶©ëŒ ì¼ì •ë“¤ ì¡°íšŒ
#             conflicting_schedules = Schedule.objects.filter(
#                 id__in=conflicting_schedule_ids,
#                 user=user
#             )
            
#             conflicting_data = [ScheduleSerializer(schedule).data for schedule in conflicting_schedules]
            
#             # ë‹¤ì¤‘ AI ëª¨ë¸ë“¤ë¡œë¶€í„° í•´ê²° ë°©ì•ˆ ë°›ê¸°
#             optimizer = ScheduleOptimizerBot()
#             prompt = optimizer.create_conflict_resolution_prompt(conflicting_data, new_request)
#             suggestions = optimizer.get_ai_suggestions(prompt, "conflict_resolution")
            
#             # AI ë¶„ì„ì„ í†µí•œ ìµœì  í•´ê²°ë°©ì•ˆ ë„ì¶œ
#             analysis_result = optimizer.analyze_and_optimize_suggestions(
#                 suggestions,
#                 f"ì¶©ëŒ í•´ê²°: {new_request}"
#             )
            
#             # í•´ê²° ë°©ì•ˆ ì €ì¥
#             conflict_resolution = ConflictResolution.objects.create(
#                 user=user,
#                 conflicting_schedules=json.dumps(conflicting_data, ensure_ascii=False),
#                 resolution_options=json.dumps(suggestions, ensure_ascii=False),
#                 ai_recommendations=json.dumps(analysis_result, ensure_ascii=False)
#             )
            
#             return Response({
#                 'resolution_id': conflict_resolution.id,
#                 'conflicting_schedules': conflicting_data,
#                 'ai_suggestions': suggestions,
#                 'optimized_resolution': analysis_result,
#                 'message': f'{len(suggestions)}ê°œ AI ëª¨ë¸ì´ ë¶„ì„í•œ ì¶©ëŒ í•´ê²° ë°©ì•ˆì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.'
#             }, status=status.HTTP_201_CREATED)
            
#         except Exception as e:
#             return Response({
#                 'error': f'ì¶©ëŒ í•´ê²° ë°©ì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
#             }, status=status.HTTP_400_BAD_REQUEST)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes, authentication_classes
from rest_framework.permissions import IsAuthenticated, AllowAny
from rest_framework.authentication import TokenAuthentication, SessionAuthentication
from rest_framework.authtoken.models import Token
from rest_framework import exceptions
from django.shortcuts import get_object_or_404
from datetime import datetime, timedelta
import json
import re
import os
from .models import Schedule, ScheduleRequest, ConflictResolution
from .serializers import (
    ScheduleSerializer, ScheduleRequestSerializer, 
    ConflictResolutionSerializer, ScheduleRequestInputSerializer
)
import logging

# íƒ€ì„ì¡´ import ìˆ˜ì •
import pytz
KST = pytz.timezone('Asia/Seoul')

def get_current_datetime():
    return datetime.now(KST)

logger = logging.getLogger(__name__)

# API í‚¤ë“¤
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")  
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# í† í° ë””ë²„ê¹…ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì¸ì¦ í´ë˜ìŠ¤
class DebugTokenAuthentication(TokenAuthentication):
    """ë””ë²„ê¹…ì´ í¬í•¨ëœ í† í° ì¸ì¦ í´ë˜ìŠ¤"""
    
    def authenticate(self, request):
        logger.info("=== ê°œì„ ëœ í† í° ì¸ì¦ ë””ë²„ê¹… ì‹œì‘ ===")
        
        auth_header = request.META.get('HTTP_AUTHORIZATION', '')
        logger.info(f"Authorization í—¤ë”: '{auth_header}'")
        
        if not auth_header:
            logger.warning("âŒ Authorization í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
            
        if not auth_header.startswith('Bearer '):
            logger.warning(f"âŒ Bearer í† í° í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {auth_header}")
            return None
            
        token = auth_header.split(' ')[1]
        logger.info(f"ğŸ“± ì¶”ì¶œëœ í† í°: {token[:10]}...{token[-10:]}")
        
        try:
            token_obj = Token.objects.select_related('user').get(key=token)
            logger.info(f"âœ… DBì—ì„œ í† í° ë°œê²¬: {token_obj.key[:10]}...{token_obj.key[-10:]}")
            logger.info(f"ğŸ‘¤ í† í° ì†Œìœ ì: {token_obj.user.username} (ID: {token_obj.user.id})")
            
            if not token_obj.user.is_active:
                logger.warning(f"âŒ ì‚¬ìš©ìê°€ ë¹„í™œì„±í™”ë¨: {token_obj.user.username}")
                raise exceptions.AuthenticationFailed('User inactive or deleted.')
            
            logger.info("âœ… í† í° ì¸ì¦ ì„±ê³µ!")
            logger.info("=== ê°œì„ ëœ í† í° ì¸ì¦ ë””ë²„ê¹… ì¢…ë£Œ ===")
            return (token_obj.user, token_obj)
            
        except Token.DoesNotExist:
            logger.error(f"âŒ DBì— í•´ë‹¹ í† í°ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {token[:10]}...{token[-10:]}")
            raise exceptions.AuthenticationFailed('Invalid token.')
        
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            raise exceptions.AuthenticationFailed('Authentication error.')


# ì‹¤ì œ AI ChatBot í´ë˜ìŠ¤
class RealChatBot:
    def __init__(self, api_key, model_name, provider):
        self.api_key = api_key
        self.model_name = model_name
        self.provider = provider
    
    def chat(self, prompt):
        """ì‹¤ì œ AI API í˜¸ì¶œ"""
        try:
            if self.provider == 'openai' and self.api_key:
                return self._call_openai_api(prompt)
            elif self.provider == 'anthropic' and self.api_key:
                return self._call_anthropic_api(prompt)
            elif self.provider == 'groq' and self.api_key:
                return self._call_groq_api(prompt)
            else:
                return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"AI API í˜¸ì¶œ ì‹¤íŒ¨ ({self.provider}): {e}")
            return self._generate_fallback_response(prompt)
    
    def _call_openai_api(self, prompt):
        """OpenAI API í˜¸ì¶œ - ìƒˆ ë²„ì „ ë¬¸ë²• ì‚¬ìš©"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¼ì • ê´€ë¦¬ë¥¼ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except ImportError:
            logger.error("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {e}")
            return self._generate_fallback_response(prompt)
    
    def _call_anthropic_api(self, prompt):
        """Anthropic API í˜¸ì¶œ"""
        try:
            client = get_anthropic_client()
            
            response = client.messages.create(
                model=self.model_name,
                max_tokens=1000,
                messages=[
                    {"role": "user", "content": f"ë‹¹ì‹ ì€ ì¼ì • ê´€ë¦¬ë¥¼ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.\n\n{prompt}"}
                ]
            )
            
            return response.content[0].text
            
        except ImportError:
            logger.error("anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"Anthropic API ì˜¤ë¥˜: {e}")
            return self._generate_fallback_response(prompt)
    
    def _call_groq_api(self, prompt):
        """Groq API í˜¸ì¶œ"""
        try:
            client = get_groq_client()
            
            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¼ì • ê´€ë¦¬ë¥¼ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except ImportError:
            logger.error("groq íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return self._generate_fallback_response(prompt)
        except Exception as e:
            logger.error(f"Groq API ì˜¤ë¥˜: {e}")
            return self._generate_fallback_response(prompt)
    
    def _generate_fallback_response(self, prompt):
        """API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        current_time = get_current_datetime()
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ì ì ˆí•œ ì œëª© ìƒì„±
        title = "ìƒˆ ì¼ì •"
        if "ìš´ë™" in prompt:
            title = "ìš´ë™"
        elif "íšŒì˜" in prompt or "ë¯¸íŒ…" in prompt:
            title = "íšŒì˜"
        elif "ê³µë¶€" in prompt or "í•™ìŠµ" in prompt:
            title = "ê³µë¶€"
        elif "ì•½ì†" in prompt:
            title = "ì•½ì†"
        elif "ì‘ì—…" in prompt:
            title = "ì‘ì—…"
        
        return f"""{{
            "title": "{title}",
            "description": "ì¼ì •ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "suggested_date": "{current_time.strftime('%Y-%m-%d')}",
            "suggested_start_time": "09:00",
            "suggested_end_time": "10:00",
            "location": "",
            "priority": "MEDIUM",
            "attendees": [],
            "reasoning": "ê¸°ë³¸ ì¼ì • ì œì•ˆì…ë‹ˆë‹¤."
        }}"""


# ì¼ì • ê´€ë¦¬ ë·° - LLM ìë™ ì œëª© ìƒì„± ê°•í™”
class ScheduleManagementView(APIView):
    """ì¼ì • ê´€ë¦¬ ë©”ì¸ ë·° - í† í° ì¸ì¦ ì ìš©"""
    authentication_classes = [DebugTokenAuthentication, SessionAuthentication]
    permission_classes = [IsAuthenticated]
    
    def __init__(self):
        super().__init__()
    
    def get_optimizer(self):
        """í•„ìš”í•  ë•Œë§Œ optimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if not hasattr(self, '_optimizer'):
            self._optimizer = ScheduleOptimizerBot()
        return self._optimizer
    
    def get(self, request):
        """ì‚¬ìš©ìì˜ ì¼ì • ëª©ë¡ ì¡°íšŒ"""
        logger.info(f"ì¼ì • ì¡°íšŒ ìš”ì²­ - ì‚¬ìš©ì: {request.user.username}")
        
        try:
            schedules = Schedule.objects.filter(user=request.user).order_by('start_time')
            
            # ë‚ ì§œ í•„í„°ë§
            start_date = request.query_params.get('start_date')
            end_date = request.query_params.get('end_date')
            
            if start_date:
                schedules = schedules.filter(start_time__date__gte=start_date)
            if end_date:
                schedules = schedules.filter(end_time__date__lte=end_date)
            
            serializer = ScheduleSerializer(schedules, many=True)
            logger.info(f"ì¼ì • ì¡°íšŒ ì„±ê³µ: {len(serializer.data)}ê°œ ì¼ì • ë°˜í™˜")
            return Response(serializer.data)
            
        except Exception as e:
            logger.error(f"ì¼ì • ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return Response(
                {'error': f'ì¼ì • ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_smart_title(self, request_text):
        """LLMì„ ì‚¬ìš©í•´ ìŠ¤ë§ˆíŠ¸í•œ ì¼ì • ì œëª© ìƒì„±"""
        try:
            optimizer = self.get_optimizer()
            
            # ì œëª© ìƒì„± ì „ìš© í”„ë¡¬í”„íŠ¸
            title_prompt = f"""
            ë‹¤ìŒ ì¼ì • ìš”ì²­ì—ì„œ ì ì ˆí•œ ì¼ì • ì œëª©ì„ í•œêµ­ì–´ë¡œ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”.
            
            ìš”ì²­ ë‚´ìš©: "{request_text}"
            
            ê·œì¹™:
            1. 10ê¸€ì ì´ë‚´ë¡œ ê°„ë‹¨í•˜ê²Œ
            2. êµ¬ì²´ì ì´ê³  ì˜ë¯¸ìˆê²Œ
            3. ì´ëª¨ì§€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
            4. ì œëª©ë§Œ ë°˜í™˜í•˜ì„¸ìš” (ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì—†ì´)
            
            ì˜ˆì‹œ:
            - "ë‚´ì¼ ì˜¤í›„ 2ì‹œì— íšŒì˜" â†’ "íŒ€ íšŒì˜"
            - "ì£¼ë§ì— ìš´ë™í•˜ê¸°" â†’ "ì£¼ë§ ìš´ë™"
            - "ì¹œêµ¬ì™€ ì¹´í˜ì—ì„œ ë§Œë‚˜ê¸°" â†’ "ì¹œêµ¬ì™€ ì¹´í˜"
            - "í”„ë¡œì íŠ¸ ì‘ì—…" â†’ "í”„ë¡œì íŠ¸ ì‘ì—…"
            """
            
            suggestions = optimizer.get_ai_suggestions(title_prompt, "title")
            
            # AI ì‘ë‹µì—ì„œ ì œëª© ì¶”ì¶œ
            for key, response in suggestions.items():
                if response and len(response.strip()) > 0:
                    # ì‘ë‹µì—ì„œ ê¹”ë”í•œ ì œëª©ë§Œ ì¶”ì¶œ
                    lines = response.strip().split('\n')
                    for line in lines:
                        clean_line = line.strip().strip('"\'`').strip()
                        # ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì§§ì§€ ì•Šì€ ì ì ˆí•œ ì œëª© ì°¾ê¸°
                        if 2 <= len(clean_line) <= 20 and not clean_line.startswith('ì œëª©:'):
                            logger.info(f"LLM ìƒì„± ì œëª©: {clean_line}")
                            return clean_line
            
            # AI ì‘ë‹µì´ ë¶€ì ì ˆí•˜ë©´ ê¸°ë³¸ ì œëª© ìƒì„±
            logger.warning("LLM ì œëª© ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ì œëª© ì‚¬ìš©")
            return self._extract_schedule_title(request_text)
            
        except Exception as e:
            logger.error(f"ìŠ¤ë§ˆíŠ¸ ì œëª© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._extract_schedule_title(request_text)
    
    def post(self, request):
        """ìƒˆë¡œìš´ ì¼ì • ìƒì„± ìš”ì²­"""
        logger.info(f"ì¼ì • ìƒì„± ìš”ì²­ - ì‚¬ìš©ì: {request.user.username}")
        
        try:
            request_text = request.data.get('request_text', '')
            existing_schedules = request.data.get('existing_schedules', [])
            
            if not request_text:
                return Response({'error': 'ìš”ì²­ í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, 
                            status=status.HTTP_400_BAD_REQUEST)
            
            user = request.user
            
            # ì—¬ëŸ¬ ì¼ì • ìš”ì²­ì¸ì§€ í™•ì¸
            schedule_requests = parse_multiple_schedules_backend(request_text)
            target_date = parse_date_from_request(request_text)
            target_datetime = get_current_datetime()
            
            logger.info(f"íŒŒì‹±ëœ ì¼ì • ìš”ì²­: {len(schedule_requests)}ê°œ")

            if len(schedule_requests) > 1:
                # ì—¬ëŸ¬ ì¼ì • ì²˜ë¦¬ - ì¶©ëŒ ë°©ì§€ ê°•í™”
                multiple_schedules = []
                all_individual_suggestions = []
                cumulative_existing_schedules = existing_schedules.copy()  # ëˆ„ì  ì¼ì • ê´€ë¦¬
                
                logger.info(f"ì—¬ëŸ¬ ì¼ì • ì²˜ë¦¬ ì‹œì‘: {len(schedule_requests)}ê°œ ì¼ì •")
                
                for i, single_request in enumerate(schedule_requests):
                    logger.info(f"ì²˜ë¦¬ ì¤‘ì¸ ì¼ì • {i+1}/{len(schedule_requests)}: {single_request}")
                    
                    # LLMì´ ê° ì¼ì •ì˜ ì œëª©ì„ ìë™ ìƒì„±
                    smart_title = self._generate_smart_title(single_request)
                    
                    # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
                    parsed_start, parsed_duration = self._extract_time_info(single_request)
                    
                    # ê¸°ë³¸ ì‹œì‘ ì‹œê°„ ì„¤ì • (ì‚¬ìš©ì ì§€ì •ì´ ì—†ìœ¼ë©´ ìˆœì°¨ì ìœ¼ë¡œ)
                    if parsed_start is not None:
                        start_hour = parsed_start
                        logger.info(f"ì‚¬ìš©ì ì§€ì • ì‹œê°„ ì‚¬ìš©: {start_hour}ì‹œ")
                    else:
                        start_hour = 9 + (i * 2)  # 9ì‹œ, 11ì‹œ, 13ì‹œ, 15ì‹œ... ìˆœì°¨ì  ë°°ì •
                        logger.info(f"ê¸°ë³¸ ì‹œê°„ ì‚¬ìš©: {start_hour}ì‹œ")
                        
                    duration_hours = parsed_duration or 1

                    # ì¶©ëŒ ë°©ì§€ ì‹œê°„ ê³„ì‚° - ëˆ„ì ëœ ê¸°ì¡´ ì¼ì • í¬í•¨
                    logger.info(f"ì¶©ëŒ ë°©ì§€ ê³„ì‚° ì‹œì‘ - í˜„ì¬ ëˆ„ì  ì¼ì • ê°œìˆ˜: {len(cumulative_existing_schedules)}")
                    schedule_start_dt, schedule_end_dt = self._find_non_conflicting_time(
                        cumulative_existing_schedules, start_hour, duration_hours, target_date
                    )

                    optimized_schedule = {
                        "title": smart_title,  # LLMì´ ìƒì„±í•œ ìŠ¤ë§ˆíŠ¸ ì œëª©
                        "description": f"AIê°€ ë¶„ì„í•œ ì¼ì •ì…ë‹ˆë‹¤: {single_request}",
                        "suggested_date": target_datetime.strftime('%Y-%m-%d'),
                        "suggested_start_time": schedule_start_dt.strftime('%H:%M'),
                        "suggested_end_time": schedule_end_dt.strftime('%H:%M'),
                        "location": self._extract_schedule_location(single_request),
                        "priority": "HIGH",
                        "attendees": [],
                        "reasoning": f"{i + 1}ë²ˆì§¸ ì¼ì •: {single_request}. ê¸°ì¡´ ì¼ì •ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} ì‹œê°„ìœ¼ë¡œ ë°°ì •í–ˆìŠµë‹ˆë‹¤."
                    }

                    multiple_schedules.append(optimized_schedule)
                    
                    # ë°©ê¸ˆ ì¶”ê°€í•œ ì¼ì •ì„ ëˆ„ì  ì¼ì • ëª©ë¡ì— ì¶”ê°€ (ë‹¤ìŒ ì¼ì • ì²˜ë¦¬ ì‹œ ê³ ë ¤)
                    cumulative_existing_schedules.append({
                        'start_time': schedule_start_dt.strftime('%Y-%m-%dT%H:%M:%S'),
                        'end_time': schedule_end_dt.strftime('%Y-%m-%dT%H:%M:%S'),
                        'title': smart_title
                    })
                    
                    logger.info(f"ì¼ì • {i+1} ì™„ë£Œ: {smart_title} ({schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')})")
                    
                    # ê° AIë³„ ê°œë³„ ì œì•ˆ ìƒì„±
                    for ai_type in ['gpt', 'claude', 'mixtral']:
                        individual_suggestion = optimized_schedule.copy()
                        individual_suggestion['source'] = ai_type
                        individual_suggestion['reasoning'] = f"{ai_type.upper()}ê°€ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤."
                        all_individual_suggestions.append(individual_suggestion)
                
                logger.info(f"ì—¬ëŸ¬ ì¼ì • ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(multiple_schedules)}ê°œ ì¼ì • ìƒì„±")
                
                response_data = {
                    'request_id': int(datetime.now().timestamp()),
                    'multiple_schedules': multiple_schedules,
                    'optimized_suggestion': multiple_schedules[0] if multiple_schedules else {},
                    'confidence_score': 0.92,
                    'individual_suggestions': all_individual_suggestions,
                    'ai_analysis': {
                        'analysis_summary': f"ì´ {len(schedule_requests)}ê°œì˜ ì¼ì •ì„ ë¶„ì„í•˜ì—¬ ì¶©ëŒ ì—†ëŠ” ìµœì  ì‹œê°„ëŒ€ë¡œ ë°°ì •í–ˆìŠµë‹ˆë‹¤.",
                        'reasoning': f"ì—¬ëŸ¬ ì¼ì •ì„ {target_date.strftime('%Yë…„ %mì›” %dì¼')}ì— ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë°°ì¹˜í•˜ì—¬ ëª¨ë“  ì¶©ëŒì„ ë°©ì§€í–ˆìŠµë‹ˆë‹¤.",
                        'models_used': ["gpt", "claude", "mixtral"]
                    },
                    'has_conflicts': False,
                    'conflicts': [],
                    'analysis_summary': f"{len(schedule_requests)}ê°œ ì¼ì •ì— ëŒ€í•´ AIê°€ ì¶©ëŒ ì—†ì´ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
                    'is_multiple_schedule': True
                }
                
            else:
                # ë‹¨ì¼ ì¼ì • ì²˜ë¦¬ - ì¶©ëŒ ë°©ì§€ ê°•í™”
                logger.info("ë‹¨ì¼ ì¼ì • ì²˜ë¦¬ ì‹œì‘")
                
                # LLMì´ ì œëª©ì„ ìë™ ìƒì„±
                smart_title = self._generate_smart_title(request_text)
                
                # ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì‹œê°„ ì¶”ì¶œ
                parsed_start, parsed_duration = self._extract_time_info(request_text)
                preferred_start_hour = parsed_start if parsed_start is not None else 9
                duration_hours = parsed_duration or 1
                
                logger.info(f"ë‹¨ì¼ ì¼ì • ì‹œê°„ ë¶„ì„: ì„ í˜¸ ì‹œì‘ì‹œê°„ {preferred_start_hour}ì‹œ, ì§€ì†ì‹œê°„ {duration_hours}ì‹œê°„")
                
                # ì¶©ëŒ ë°©ì§€ ì‹œê°„ ê³„ì‚°
                schedule_start_dt, schedule_end_dt = self._find_non_conflicting_time(
                    existing_schedules, preferred_start_hour, duration_hours, target_date
                )
                
                # AIì—ê²Œ ì¶©ëŒ ì—†ëŠ” ì‹œê°„ìœ¼ë¡œ ì œì•ˆ ìš”ì²­
                enhanced_prompt = f"""
                ì‚¬ìš©ìì˜ ì¼ì • ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì¼ì •ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
                
                ìš”ì²­ ë‚´ìš©: {request_text}
                ëª©í‘œ ë‚ ì§œ: {target_date.strftime('%Yë…„ %mì›” %dì¼')} ({self._get_weekday_korean(target_date)})
                ê¸°ì¡´ ì¼ì •ë“¤: {len(existing_schedules)}ê°œ ì¼ì •ì´ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤
                ë°°ì •ëœ ì‹œê°„: {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} (ì¶©ëŒ ë°©ì§€ë¨)
                
                ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ì‘ë‹µí•´ì£¼ì„¸ìš”:
                {{
                    "title": "{smart_title}",
                    "description": "ìƒì„¸ ì„¤ëª…",
                    "suggested_date": "{target_date.strftime('%Y-%m-%d')}",
                    "suggested_start_time": "{schedule_start_dt.strftime('%H:%M')}",
                    "suggested_end_time": "{schedule_end_dt.strftime('%H:%M')}",
                    "location": "ì¥ì†Œ",
                    "priority": "MEDIUM",
                    "attendees": [],
                    "reasoning": "ê¸°ì¡´ ì¼ì •ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ìµœì  ì‹œê°„ì…ë‹ˆë‹¤"
                }}
                """
                
                try:
                    optimizer = self.get_optimizer()
                    suggestions = optimizer.get_ai_suggestions(enhanced_prompt)
                    optimized_result = optimizer.analyze_and_optimize_suggestions(
                        suggestions, f"ì¼ì • ìš”ì²­: {request_text}"
                    )
                    
                    # ì¶©ëŒ ë°©ì§€ëœ ì‹œê°„ìœ¼ë¡œ ë®ì–´ì“°ê¸° ë³´ì¥
                    if 'optimized_suggestion' in optimized_result:
                        optimized_result['optimized_suggestion']['title'] = smart_title
                        optimized_result['optimized_suggestion']['suggested_date'] = target_date.strftime('%Y-%m-%d')
                        optimized_result['optimized_suggestion']['suggested_start_time'] = schedule_start_dt.strftime('%H:%M')
                        optimized_result['optimized_suggestion']['suggested_end_time'] = schedule_end_dt.strftime('%H:%M')
                        optimized_result['optimized_suggestion']['reasoning'] = f"ê¸°ì¡´ ì¼ì •ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} ì‹œê°„ìœ¼ë¡œ ë°°ì •í–ˆìŠµë‹ˆë‹¤."
                    
                    response_data = {
                        'request_id': int(datetime.now().timestamp()),
                        'optimized_suggestion': optimized_result.get('optimized_suggestion', {}),
                        'confidence_score': optimized_result.get('confidence_score', 0.9),
                        'ai_analysis': optimized_result.get('ai_analysis', {}),
                        'individual_suggestions': optimized_result.get('individual_suggestions', []),
                        'has_conflicts': False,
                        'conflicts': [],
                        'analysis_summary': "AIê°€ ê¸°ì¡´ ì¼ì •ê³¼ì˜ ì¶©ëŒì„ ë°©ì§€í•˜ì—¬ ìµœì  ì‹œê°„ì„ ë°°ì •í–ˆìŠµë‹ˆë‹¤.",
                        'is_multiple_schedule': False
                    }
                    
                except Exception as e:
                    logger.error(f"ë‹¨ì¼ ì¼ì • AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # AI ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ ì¶©ëŒ ë°©ì§€ëœ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
                    response_data = {
                        'request_id': int(datetime.now().timestamp()),
                        'optimized_suggestion': {
                            "title": smart_title,
                            "description": f"ìš”ì²­í•˜ì‹  ì¼ì •ì…ë‹ˆë‹¤: {request_text}",
                            "suggested_date": target_date.strftime('%Y-%m-%d'),
                            "suggested_start_time": schedule_start_dt.strftime('%H:%M'),
                            "suggested_end_time": schedule_end_dt.strftime('%H:%M'),
                            "location": self._extract_schedule_location(request_text),
                            "priority": "MEDIUM",
                            "attendees": [],
                            "reasoning": f"ê¸°ì¡´ ì¼ì •ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” {schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')} ì‹œê°„ìœ¼ë¡œ ë°°ì •í–ˆìŠµë‹ˆë‹¤."
                        },
                        'confidence_score': 0.8,
                        'ai_analysis': {
                            'analysis_summary': 'ì¶©ëŒ ë°©ì§€ ì‹œê°„ ë°°ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                            'reasoning': 'ê¸°ì¡´ ì¼ì •ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” ìµœì  ì‹œê°„ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.',
                            'models_used': []
                        },
                        'individual_suggestions': [],
                        'has_conflicts': False,
                        'conflicts': [],
                        'analysis_summary': "ì¶©ëŒ ë°©ì§€ ì•Œê³ ë¦¬ì¦˜ì´ ìµœì  ì‹œê°„ì„ ë°°ì •í–ˆìŠµë‹ˆë‹¤.",
                        'is_multiple_schedule': False
                    }
                
                logger.info(f"ë‹¨ì¼ ì¼ì • ì²˜ë¦¬ ì™„ë£Œ: {smart_title} ({schedule_start_dt.strftime('%H:%M')}-{schedule_end_dt.strftime('%H:%M')})")
            
            logger.info("ì¼ì • ìƒì„± ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ")
            return Response(response_data, status=status.HTTP_201_CREATED)
            
        except Exception as e:
            logger.error(f"ì¼ì • ìƒì„± ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return Response({
                'error': f'ì¼ì • ìƒì„± ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _extract_time_info(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ"""
        import re
        start_hour = None
        duration_hours = 1

        is_pm = 'ì˜¤í›„' in text
        is_am = 'ì˜¤ì „' in text

        # "ì˜¤í›„ 3-5ì‹œ"ì™€ ê°™ì€ ê²½ìš° ì²˜ë¦¬
        time_range = re.search(r'(\d{1,2})\s*[-~]\s*(\d{1,2})\s*ì‹œ', text)
        if time_range:
            start = int(time_range.group(1))
            end = int(time_range.group(2))

            if is_pm:
                if start < 12:
                    start += 12
                if end < 12:
                    end += 12
            elif is_am:
                if start == 12:
                    start = 0
                if end == 12:
                    end = 0

            start_hour = start
            duration_hours = end - start
            return start_hour, duration_hours

        # "2ì‹œê°„"ë§Œ ìˆëŠ” ê²½ìš°
        dur_match = re.search(r'(\d{1,2})\s*ì‹œê°„', text)
        if dur_match:
            duration_hours = int(dur_match.group(1))

        # ë‹¨ì¼ ì‹œê°: "ì˜¤í›„ 3ì‹œ"
        single_time_match = re.search(r'(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2})\s*ì‹œ', text)
        if single_time_match:
            hour = int(single_time_match.group(2))
            if single_time_match.group(1) == 'ì˜¤í›„' and hour < 12:
                hour += 12
            elif single_time_match.group(1) == 'ì˜¤ì „' and hour == 12:
                hour = 0
            start_hour = hour

        return start_hour, duration_hours

    def _find_non_conflicting_time(self, existing_schedules, start_hour, duration_hours, target_date):
        """ê¸°ì¡´ ì¼ì •ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” ì‹œê°„ëŒ€ë¥¼ íƒìƒ‰ - í˜„ì‹¤ì ì¸ ì‹œê°„ëŒ€ë§Œ ê³ ë ¤"""
        from datetime import datetime, timedelta, time
        
        logger.info(f"ì¶©ëŒ ë°©ì§€ ì‹œê°„ íƒìƒ‰ ì‹œì‘: ì›í•˜ëŠ” ì‹œì‘ì‹œê°„ {start_hour}ì‹œ, ì§€ì†ì‹œê°„ {duration_hours}ì‹œê°„")
        logger.info(f"ê¸°ì¡´ ì¼ì • ê°œìˆ˜: {len(existing_schedules)}")

        # í˜„ì‹¤ì ì¸ í™œë™ ì‹œê°„ëŒ€ ì •ì˜
        WORK_START = 7   # ì˜¤ì „ 7ì‹œë¶€í„°
        WORK_END = 22    # ì˜¤í›„ 10ì‹œê¹Œì§€
        
        def parse_existing_schedule_time(schedule):
            """ê¸°ì¡´ ì¼ì • ì‹œê°„ íŒŒì‹±"""
            try:
                if 'start_time' in schedule and 'end_time' in schedule:
                    start_str = schedule['start_time']
                    end_str = schedule['end_time']
                    
                    if 'T' in start_str:
                        start_dt = datetime.strptime(start_str, '%Y-%m-%dT%H:%M:%S')
                        end_dt = datetime.strptime(end_str, '%Y-%m-%dT%H:%M:%S')
                    else:
                        start_dt = datetime.strptime(start_str, '%Y-%m-%d %H:%M:%S')
                        end_dt = datetime.strptime(end_str, '%Y-%m-%d %H:%M:%S')
                        
                    return start_dt, end_dt
                        
            except (ValueError, KeyError) as e:
                logger.warning(f"ì¼ì • ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {schedule}, ì˜¤ë¥˜: {e}")
                return None, None
            
            return None, None

        def is_conflicting(new_start, new_end, schedules):
            """ì‹œê°„ ê²¹ì¹¨ ê²€ì‚¬"""
            logger.info(f"ì¶©ëŒ ê²€ì‚¬: ìƒˆ ì¼ì • {new_start.strftime('%H:%M')}-{new_end.strftime('%H:%M')}")
            
            for i, schedule in enumerate(schedules):
                existing_start, existing_end = parse_existing_schedule_time(schedule)
                
                if existing_start is None or existing_end is None:
                    continue
                
                # ê°™ì€ ë‚ ì§œì¸ì§€ í™•ì¸
                if existing_start.date() != target_date:
                    continue
                    
                logger.info(f"ê¸°ì¡´ ì¼ì • {i+1}: {existing_start.strftime('%H:%M')}-{existing_end.strftime('%H:%M')}")
                
                # ê²¹ì¹˜ëŠ”ì§€ ê²€ì‚¬
                if (new_start < existing_end) and (existing_start < new_end):
                    logger.warning(f"âš ï¸ ì‹œê°„ ì¶©ëŒ ë°œê²¬! ìƒˆ ì¼ì •ê³¼ ê¸°ì¡´ ì¼ì • {i+1}ì´ ê²¹ì¹¨")
                    return True
                    
            return False

        def is_realistic_time(hour, duration):
            """í˜„ì‹¤ì ì¸ ì‹œê°„ì¸ì§€ í™•ì¸"""
            end_hour = hour + duration
            
            # ì—…ë¬´ì‹œê°„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
            if hour < WORK_START or end_hour > WORK_END:
                return False
                
            # ì ì‹¬ì‹œê°„ í”¼í•˜ê¸° (12-13ì‹œ)
            if hour <= 12 and end_hour >= 13:
                return False
                
            return True

        # ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œê°„ì´ í˜„ì‹¤ì ì¸ì§€ ë¨¼ì € í™•ì¸
        if start_hour < WORK_START or start_hour >= WORK_END:
            logger.warning(f"ì‚¬ìš©ì ì§€ì • ì‹œê°„ {start_hour}ì‹œê°€ ë¹„í˜„ì‹¤ì ì„. ì˜¤ì „ 9ì‹œë¡œ ì¡°ì •")
            start_hour = 9

        # í˜„ì‹¤ì ì¸ ì‹œê°„ëŒ€ì—ì„œë§Œ íƒìƒ‰
        realistic_hours = []
        
        # ì˜¤ì „ ì‹œê°„ëŒ€ (7-12ì‹œ)
        for h in range(7, 12):
            if h + duration_hours <= 12:  # ì ì‹¬ì‹œê°„ ì „ì— ëë‚˜ì•¼ í•¨
                realistic_hours.append(h)
        
        # ì˜¤í›„ ì‹œê°„ëŒ€ (13-22ì‹œ)
        for h in range(13, 22):
            if h + duration_hours <= 22:  # ì €ë… 10ì‹œ ì „ì— ëë‚˜ì•¼ í•¨
                realistic_hours.append(h)

        # ì‚¬ìš©ì ì„ í˜¸ ì‹œê°„ë¶€í„° ì‹œì‘í•˜ì—¬ íƒìƒ‰
        search_order = []
        if start_hour in realistic_hours:
            search_order.append(start_hour)
        
        # ì„ í˜¸ ì‹œê°„ ì£¼ë³€ë¶€í„° í™•ì¥ íƒìƒ‰
        for offset in range(1, 8):
            if start_hour + offset in realistic_hours and start_hour + offset not in search_order:
                search_order.append(start_hour + offset)
            if start_hour - offset in realistic_hours and start_hour - offset not in search_order:
                search_order.append(start_hour - offset)
        
        # ë‚˜ë¨¸ì§€ í˜„ì‹¤ì ì¸ ì‹œê°„ë“¤ ì¶”ê°€
        for h in realistic_hours:
            if h not in search_order:
                search_order.append(h)

        logger.info(f"í˜„ì‹¤ì ì¸ ì‹œê°„ëŒ€ íƒìƒ‰ ìˆœì„œ: {search_order}")

        # í˜„ì‹¤ì ì¸ ì‹œê°„ëŒ€ì—ì„œ ì¶©ëŒ ì—†ëŠ” ì‹œê°„ ì°¾ê¸°
        for attempt, hour in enumerate(search_order):
            try:
                candidate_start = datetime.combine(target_date, time(hour))
                candidate_end = candidate_start + timedelta(hours=duration_hours)
                
                logger.info(f"ì‹œë„ {attempt + 1}: {candidate_start.strftime('%H:%M')}-{candidate_end.strftime('%H:%M')}")
                
                if not is_conflicting(candidate_start, candidate_end, existing_schedules):
                    logger.info(f"âœ… ì¶©ëŒ ì—†ëŠ” í˜„ì‹¤ì ì¸ ì‹œê°„ ë°œê²¬: {candidate_start.strftime('%H:%M')}-{candidate_end.strftime('%H:%M')}")
                    return candidate_start, candidate_end
                    
            except Exception as e:
                logger.error(f"ì‹œê°„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                continue

        # ëª¨ë“  í˜„ì‹¤ì ì¸ ì‹œê°„ëŒ€ì— ì¶©ëŒì´ ìˆëŠ” ê²½ìš°
        logger.warning("ëª¨ë“  í˜„ì‹¤ì ì¸ ì‹œê°„ëŒ€ì— ì¶©ëŒ ë°œìƒ")
        
        # ê°€ì¥ ë¹ˆ ì‹œê°„ëŒ€ ì°¾ê¸°
        best_hour = 9  # ê¸°ë³¸ê°’
        min_conflicts = float('inf')
        
        for hour in realistic_hours:
            try:
                candidate_start = datetime.combine(target_date, time(hour))
                candidate_end = candidate_start + timedelta(hours=duration_hours)
                
                # ì´ ì‹œê°„ëŒ€ì˜ ì¶©ëŒ ê°œìˆ˜ ì„¸ê¸°
                conflict_count = 0
                for schedule in existing_schedules:
                    existing_start, existing_end = parse_existing_schedule_time(schedule)
                    if existing_start and existing_start.date() == target_date:
                        if (candidate_start < existing_end) and (existing_start < candidate_end):
                            conflict_count += 1
                
                if conflict_count < min_conflicts:
                    min_conflicts = conflict_count
                    best_hour = hour
                    
            except Exception as e:
                logger.error(f"ì¶©ëŒ ê³„ì‚° ì˜¤ë¥˜: {e}")
                continue
        
        # ìµœì†Œ ì¶©ëŒ ì‹œê°„ëŒ€ë¡œ ë°°ì •
        final_start = datetime.combine(target_date, time(best_hour))
        final_end = final_start + timedelta(hours=duration_hours)
        
        logger.info(f"ğŸ”„ ìµœì†Œ ì¶©ëŒ ì‹œê°„ìœ¼ë¡œ ë°°ì •: {final_start.strftime('%H:%M')}-{final_end.strftime('%H:%M')} (ì¶©ëŒ {min_conflicts}ê°œ)")
        return final_start, final_end

    def _extract_time_info(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ - í˜„ì‹¤ì ì¸ ì‹œê°„ë§Œ ë°˜í™˜"""
        import re
        start_hour = None
        duration_hours = 1

        is_pm = 'ì˜¤í›„' in text
        is_am = 'ì˜¤ì „' in text

        # "ì˜¤í›„ 3-5ì‹œ"ì™€ ê°™ì€ ê²½ìš° ì²˜ë¦¬
        time_range = re.search(r'(\d{1,2})\s*[-~]\s*(\d{1,2})\s*ì‹œ', text)
        if time_range:
            start = int(time_range.group(1))
            end = int(time_range.group(2))

            if is_pm:
                if start < 12:
                    start += 12
                if end < 12:
                    end += 12
            elif is_am:
                if start == 12:
                    start = 0
                if end == 12:
                    end = 0

            # í˜„ì‹¤ì ì¸ ì‹œê°„ì¸ì§€ í™•ì¸
            if 7 <= start <= 22 and 7 <= end <= 22:
                start_hour = start
                duration_hours = end - start
                return start_hour, duration_hours

        # "2ì‹œê°„"ë§Œ ìˆëŠ” ê²½ìš°
        dur_match = re.search(r'(\d{1,2})\s*ì‹œê°„', text)
        if dur_match:
            duration_hours = int(dur_match.group(1))

        # ë‹¨ì¼ ì‹œê°: "ì˜¤í›„ 3ì‹œ"
        single_time_match = re.search(r'(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2})\s*ì‹œ', text)
        if single_time_match:
            hour = int(single_time_match.group(2))
            if single_time_match.group(1) == 'ì˜¤í›„' and hour < 12:
                hour += 12
            elif single_time_match.group(1) == 'ì˜¤ì „' and hour == 12:
                hour = 0
            
            # í˜„ì‹¤ì ì¸ ì‹œê°„ì¸ì§€ í™•ì¸
            if 7 <= hour <= 22:
                start_hour = hour

        # ë¹„í˜„ì‹¤ì ì¸ ì‹œê°„ì´ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        if start_hour is not None and (start_hour < 7 or start_hour > 22):
            logger.warning(f"ë¹„í˜„ì‹¤ì ì¸ ì‹œê°„ {start_hour}ì‹œ ê°ì§€, ì˜¤ì „ 9ì‹œë¡œ ì¡°ì •")
            start_hour = 9

        return start_hour, duration_hours
    def _extract_schedule_title(self, request):
        """ìš”ì²­ì—ì„œ ê¸°ë³¸ ì¼ì • ì œëª© ì¶”ì¶œ (fallbackìš©)"""
        if 'ìš´ë™' in request:
            return 'ìš´ë™'
        elif 'ë¯¸íŒ…' in request or 'íšŒì˜' in request:
            return 'íŒ€ ë¯¸íŒ…'
        elif 'ê³µë¶€' in request or 'í•™ìŠµ' in request:
            return 'í•™ìŠµ ì‹œê°„'
        elif 'ì‘ì—…' in request or 'ì—…ë¬´' in request:
            return 'ì§‘ì¤‘ ì‘ì—…'
        elif 'ì•½ì†' in request:
            return 'ì•½ì†'
        else:
            return 'ìƒˆ ì¼ì •'

    def _extract_schedule_location(self, request):
        """ìš”ì²­ì—ì„œ ì¥ì†Œ ì¶”ì¶œ"""
        if 'ìš´ë™' in request or 'í—¬ìŠ¤' in request:
            return 'í—¬ìŠ¤ì¥'
        elif 'ë¯¸íŒ…' in request or 'íšŒì˜' in request:
            return 'íšŒì˜ì‹¤'
        elif 'ê³µë¶€' in request or 'í•™ìŠµ' in request:
            return 'ë„ì„œê´€'
        elif 'ì»¤í”¼' in request or 'ì¹´í˜' in request:
            return 'ì¹´í˜'
        else:
            return 'ì‚¬ë¬´ì‹¤'

    def _get_weekday_korean(self, date):
        """ìš”ì¼ì„ í•œêµ­ì–´ë¡œ ë°˜í™˜"""
        weekdays = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
        return weekdays[date.weekday()]


# ìˆ˜ë™ ì¼ì • ìƒì„± ë·° (ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸)
@api_view(['POST'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def create_schedule(request):
    """ìˆ˜ë™ìœ¼ë¡œ ì¼ì • ìƒì„± - /api/schedule/create/ ì—”ë“œí¬ì¸íŠ¸"""
    logger.info(f"ìˆ˜ë™ ì¼ì • ìƒì„± ìš”ì²­ - ì‚¬ìš©ì: {request.user.username}")
    
    try:
        data = request.data.copy()
        
        # ì œëª©ì´ ì—†ìœ¼ë©´ LLMì´ ìë™ ìƒì„±
        if not data.get('title'):
            description = data.get('description', '')
            if description:
                # ê°„ë‹¨í•œ ì œëª© ìƒì„± ë¡œì§
                try:
                    optimizer = ScheduleOptimizerBot()
                    title_prompt = f"""
                    ë‹¤ìŒ ì„¤ëª…ì—ì„œ ì ì ˆí•œ ì¼ì • ì œëª©ì„ 10ê¸€ì ì´ë‚´ë¡œ ê°„ë‹¨í•˜ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”:
                    "{description}"
                    
                    ì œëª©ë§Œ ë°˜í™˜í•˜ì„¸ìš” (ë”°ì˜´í‘œë‚˜ ì„¤ëª… ì—†ì´).
                    """
                    suggestions = optimizer.get_ai_suggestions(title_prompt, "title")
                    
                    # AI ì‘ë‹µì—ì„œ ì œëª© ì¶”ì¶œ
                    for key, response in suggestions.items():
                        if response and len(response.strip()) > 0:
                            lines = response.strip().split('\n')
                            for line in lines:
                                clean_line = line.strip().strip('"\'`').strip()
                                if 2 <= len(clean_line) <= 15:
                                    data['title'] = clean_line
                                    logger.info(f"ìë™ ìƒì„±ëœ ì œëª©: {clean_line}")
                                    break
                            if data.get('title'):
                                break
                    
                    # ì œëª© ìƒì„±ì— ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’
                    if not data.get('title'):
                        data['title'] = 'ìƒˆ ì¼ì •'
                        
                except Exception as e:
                    logger.error(f"ì œëª© ìë™ ìƒì„± ì‹¤íŒ¨: {e}")
                    data['title'] = 'ìƒˆ ì¼ì •'
            else:
                data['title'] = 'ìƒˆ ì¼ì •'
        
        data['user'] = request.user.id
        
        serializer = ScheduleSerializer(data=data)
        if serializer.is_valid():
            schedule = serializer.save(user=request.user)
            logger.info(f"ìˆ˜ë™ ì¼ì • ìƒì„± ì„±ê³µ: {schedule.id} - {schedule.title}")
            return Response({
                'message': 'ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'schedule': serializer.data
            }, status=status.HTTP_201_CREATED)
        else:
            logger.warning(f"ìˆ˜ë™ ì¼ì • ìƒì„± ì‹¤íŒ¨ - ìœ íš¨ì„± ê²€ì¦ ì˜¤ë¥˜: {serializer.errors}")
            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
            
    except Exception as e:
        logger.error(f"ìˆ˜ë™ ì¼ì • ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return Response({
            'error': f'ì¼ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ê¸°ì¡´ ë·°ë“¤...
@api_view(['POST'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def confirm_schedule(request, request_id):
    """AI ì œì•ˆëœ ì¼ì •ì„ í™•ì •í•˜ì—¬ ì‹¤ì œ ì¼ì •ìœ¼ë¡œ ìƒì„±"""
    logger.info(f"ì¼ì • í™•ì • ìš”ì²­ - ì‚¬ìš©ì: {request.user.username}, request_id: {request_id}")
    
    try:
        user = request.user
        ai_suggestion_data = request.data.get('ai_suggestion')
        
        if not ai_suggestion_data:
            return Response({
                'error': 'AI ì œì•ˆ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        is_multiple = ai_suggestion_data.get('is_multiple_schedule', False)
        
        if is_multiple and ai_suggestion_data.get('multiple_schedules'):
            # ì—¬ëŸ¬ ì¼ì • ì²˜ë¦¬
            created_schedules = []
            
            for schedule_data in ai_suggestion_data['multiple_schedules']:
                try:
                    suggested_date = schedule_data.get('suggested_date')
                    suggested_start_time = schedule_data.get('suggested_start_time', '09:00')
                    suggested_end_time = schedule_data.get('suggested_end_time', '10:00')
                    
                    start_datetime = datetime.strptime(
                        f"{suggested_date} {suggested_start_time}",
                        '%Y-%m-%d %H:%M'
                    )
                    end_datetime = datetime.strptime(
                        f"{suggested_date} {suggested_end_time}",
                        '%Y-%m-%d %H:%M'
                    )
                    
                    schedule = Schedule.objects.create(
                        user=user,
                        title=schedule_data.get('title', 'ìƒˆ ì¼ì •'),
                        description=schedule_data.get('description', 'AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤.'),
                        start_time=start_datetime,
                        end_time=end_datetime,
                        location=schedule_data.get('location', ''),
                        priority=schedule_data.get('priority', 'MEDIUM'),
                        attendees=json.dumps(schedule_data.get('attendees', []), ensure_ascii=False)
                    )
                    
                    created_schedules.append(schedule)
                    logger.info(f"ë‹¤ì¤‘ ì¼ì • ìƒì„± ì„±ê³µ: {schedule.id} - {schedule.title}")
                    
                except Exception as e:
                    logger.error(f"ê°œë³„ ì¼ì • ìƒì„± ì‹¤íŒ¨: {str(e)}")
                    continue
            
            if created_schedules:
                serializer = ScheduleSerializer(created_schedules, many=True)
                return Response({
                    'message': f'{len(created_schedules)}ê°œì˜ ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'schedules': serializer.data
                }, status=status.HTTP_201_CREATED)
            else:
                return Response({
                    'error': 'ì¼ì • ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_400_BAD_REQUEST)
        
        else:
            # ë‹¨ì¼ ì¼ì • ì²˜ë¦¬
            optimized_suggestion = ai_suggestion_data.get('optimized_suggestion')
            if not optimized_suggestion:
                return Response({
                    'error': 'ìµœì í™”ëœ ì œì•ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            try:
                suggested_date = optimized_suggestion.get('suggested_date')
                suggested_start_time = optimized_suggestion.get('suggested_start_time', '09:00')
                suggested_end_time = optimized_suggestion.get('suggested_end_time', '10:00')
                
                if 'T' in suggested_date:
                    suggested_date = suggested_date.split('T')[0]
                
                start_datetime = datetime.strptime(
                    f"{suggested_date} {suggested_start_time}",
                    '%Y-%m-%d %H:%M'
                )
                end_datetime = datetime.strptime(
                    f"{suggested_date} {suggested_end_time}",
                    '%Y-%m-%d %H:%M'
                )
                
            except (ValueError, TypeError) as e:
                logger.error(f"DateTime parsing error: {e}")
                now = datetime.now()
                start_datetime = now.replace(hour=9, minute=0, second=0, microsecond=0)
                end_datetime = now.replace(hour=10, minute=0, second=0, microsecond=0)
            
            schedule = Schedule.objects.create(
                user=user,
                title=optimized_suggestion.get('title', 'ìƒˆ ì¼ì •'),
                description=optimized_suggestion.get('description', 'AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤.'),
                start_time=start_datetime,
                end_time=end_datetime,
                location=optimized_suggestion.get('location', ''),
                priority=optimized_suggestion.get('priority', 'MEDIUM'),
                attendees=json.dumps(optimized_suggestion.get('attendees', []), ensure_ascii=False)
            )
            
            serializer = ScheduleSerializer(schedule)
            logger.info(f"ë‹¨ì¼ ì¼ì • ìƒì„± ì„±ê³µ: {schedule.id} - {schedule.title}")
            
            return Response({
                'message': 'AIì˜ ë¶„ì„ì„ í†µí•´ ìµœì í™”ëœ ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'schedule': serializer.data
            }, status=status.HTTP_201_CREATED)
            
    except Exception as e:
        logger.error(f"ì¼ì • í™•ì • ì‹¤íŒ¨: {str(e)}")
        return Response({
            'error': f'ì¼ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }, status=status.HTTP_400_BAD_REQUEST)


@api_view(['PUT', 'DELETE'])
@authentication_classes([DebugTokenAuthentication, SessionAuthentication])
@permission_classes([IsAuthenticated])
def manage_schedule(request, schedule_id):
    """ì¼ì • ìˆ˜ì • ë˜ëŠ” ì‚­ì œ"""
    try:
        schedule = get_object_or_404(Schedule, id=schedule_id, user=request.user)
        
        if request.method == 'PUT':
            # ì¼ì • ìˆ˜ì •
            serializer = ScheduleSerializer(schedule, data=request.data, partial=True)
            if serializer.is_valid():
                serializer.save()
                logger.info(f"ì¼ì • ìˆ˜ì • ì„±ê³µ: {schedule_id}")
                return Response(serializer.data)
            else:
                return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
        
        elif request.method == 'DELETE':
            # ì¼ì • ì‚­ì œ
            schedule.delete()
            logger.info(f"ì¼ì • ì‚­ì œ ì„±ê³µ: {schedule_id}")
            return Response({'message': 'ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'}, 
                          status=status.HTTP_204_NO_CONTENT)
            
    except Exception as e:
        logger.error(f"ì¼ì • ê´€ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return Response({
            'error': f'ì¼ì • ê´€ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def parse_date_from_request(request_text):
    """ìš”ì²­ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒŒì‹±"""
    try:
        korea_tz = pytz.timezone('Asia/Seoul')
        korea_now = datetime.now(korea_tz).date()

        if 'ì˜¤ëŠ˜' in request_text:
            return korea_now
        elif 'ë‚´ì¼' in request_text:
            return korea_now + timedelta(days=1)
        elif 'ëª¨ë ˆ' in request_text or 'ëª¨ë˜' in request_text:
            return korea_now + timedelta(days=2)
        elif 'ì´ë²ˆ ì£¼' in request_text:
            days_until_friday = (4 - korea_now.weekday()) % 7
            days_until_friday = 7 if days_until_friday == 0 else days_until_friday
            return korea_now + timedelta(days=days_until_friday)
        elif 'ë‹¤ìŒ ì£¼' in request_text:
            return korea_now + timedelta(days=7)
        else:
            return korea_now + timedelta(days=1)
    except Exception as e:
        logger.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return datetime.now().date()

def parse_multiple_schedules_backend(request_text):
    """ë°±ì—”ë“œì—ì„œ ì—¬ëŸ¬ ì¼ì • íŒŒì‹±"""
    try:
        separators = [',', 'ï¼Œ', 'ê·¸ë¦¬ê³ ', 'ë°', 'ì™€', 'ê³¼']
        
        parts = [request_text]
        for sep in separators:
            new_parts = []
            for part in parts:
                new_parts.extend(part.split(sep))
            parts = new_parts
        
        cleaned_requests = []
        for part in parts:
            cleaned = part.strip()
            if cleaned and len(cleaned) > 2:
                cleaned_requests.append(cleaned)
        
        return cleaned_requests if len(cleaned_requests) > 1 else [request_text]
    except Exception as e:
        logger.error(f"ì¼ì • íŒŒì‹± ì˜¤ë¥˜: {e}")
        return [request_text]


# ScheduleOptimizerBot í´ë˜ìŠ¤
class ScheduleOptimizerBot:
    """ì¼ì • ìµœì í™”ë¥¼ ìœ„í•œ AI ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self):
        logger.info("ScheduleOptimizerBot ì´ˆê¸°í™” ì‹œì‘")
        try:

            self.chatbots = {
                # OpenAI ìµœì‹ /ê²½ëŸ‰ ì±„íŒ… ëª¨ë¸ë¡œ êµì²´
                'gpt': RealChatBot(OPENAI_API_KEY, 'gpt-4o-mini', 'openai'),
                # Anthropic ëª¨ë¸ëª… ìµœì‹  alias ê¶Œì¥
                'claude': RealChatBot(ANTHROPIC_API_KEY, 'claude-3-5-haiku-latest', 'anthropic'),
                # GroqëŠ” llama-3.1-8b-instantë„ OK. mixtral ì“°ê³  ì‹¶ìœ¼ë©´ ëª¨ë¸ëª…ë§Œ ë³€ê²½
                'mixtral': RealChatBot(GROQ_API_KEY, 'llama-3.1-8b-instant', 'groq'),
            }
            logger.info("ChatBot í´ë˜ìŠ¤ ë¡œë“œ ì„±ê³µ")
        except ImportError as e:
            logger.warning(f"ChatBot í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}. ë”ë¯¸ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.chatbots = {
                'gpt': DummyChatBot(),
                'claude': DummyChatBot(),
                'mixtral': DummyChatBot(),
            }
        except Exception as e:
            logger.error(f"ChatBot ì´ˆê¸°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            self.chatbots = {
                'gpt': DummyChatBot(),
                'claude': DummyChatBot(),
                'mixtral': DummyChatBot(),
            }
    
    def get_ai_suggestions(self, prompt, suggestion_type="schedule"):
        """ì—¬ëŸ¬ AI ëª¨ë¸ë¡œë¶€í„° ì œì•ˆë°›ê¸°"""
        logger.info(f"AI ì œì•ˆ ìš”ì²­ ì‹œì‘ - íƒ€ì…: {suggestion_type}")
        suggestions = {}
        
        for model_name, chatbot in self.chatbots.items():
            try:
                logger.info(f"{model_name} AI ëª¨ë¸ ìš”ì²­ ì‹œì‘")
                if hasattr(chatbot, 'chat'):
                    response = chatbot.chat(prompt)
                    logger.info(f"{model_name} AI ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}")
                else:
                    response = f"ë”ë¯¸ ì‘ë‹µ: {model_name}ì—ì„œ {suggestion_type} ë¶„ì„ ì™„ë£Œ"
                    logger.info(f"{model_name} ë”ë¯¸ ì‘ë‹µ ì‚¬ìš©")
                suggestions[f"{model_name}_suggestion"] = response
            except Exception as e:
                logger.error(f"{model_name} AI ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
                suggestions[f"{model_name}_suggestion"] = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        
        logger.info(f"AI ì œì•ˆ ì™„ë£Œ: {len(suggestions)}ê°œ ì‘ë‹µ")
        return suggestions
    
    def analyze_and_optimize_suggestions(self, suggestions, query, selected_models=['GPT', 'Claude', 'Mixtral']):
        """ì—¬ëŸ¬ AI ì œì•ˆì„ ë¶„ì„í•˜ì—¬ ìµœì í™”ëœ ê²°ê³¼ ìƒì„±"""
        logger.info("AI ì œì•ˆ ë¶„ì„ ë° ìµœì í™” ì‹œì‘")
        try:
            optimized = self._extract_first_valid_suggestion(suggestions)
            confidence = 0.85
            
            logger.info("AI ì œì•ˆ ë¶„ì„ ë° ìµœì í™” ì™„ë£Œ")
            return {
                "optimized_suggestion": optimized,
                "confidence_score": confidence,
                "ai_analysis": {
                    "analysis_summary": "AI ëª¨ë¸ë“¤ì˜ ì œì•ˆì„ ì¢…í•© ë¶„ì„í–ˆìŠµë‹ˆë‹¤.",
                    "reasoning": "ì—¬ëŸ¬ ëª¨ë¸ì˜ ê³µí†µì ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.",
                    "models_used": selected_models
                },
                "individual_suggestions": self._parse_individual_suggestions(suggestions)
            }
            
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            current_datetime = get_current_datetime()
            return {
                "optimized_suggestion": {
                    "title": "ìƒˆ ì¼ì •",
                    "description": "AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤",
                    "suggested_date": current_datetime.strftime('%Y-%m-%d'),
                    "suggested_start_time": "09:00",
                    "suggested_end_time": "10:00",
                    "location": "",
                    "priority": "MEDIUM",
                    "attendees": [],
                    "reasoning": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                },
                "confidence_score": 0.5,
                "ai_analysis": {
                    "analysis_summary": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                    "reasoning": "ê¸°ë³¸ ì œì•ˆì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
                    "models_used": []
                },
                "individual_suggestions": []
            }
    
    def _extract_first_valid_suggestion(self, suggestions):
        """ì²« ë²ˆì§¸ ìœ íš¨í•œ ì œì•ˆ ì¶”ì¶œ"""
        logger.info("ìœ íš¨í•œ ì œì•ˆ ì¶”ì¶œ ì‹œì‘")
        current_datetime = get_current_datetime()
        
        for key, suggestion in suggestions.items():
            try:
                logger.info(f"{key}ì—ì„œ JSON ì¶”ì¶œ ì‹œë„")
                json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
                if json_match:
                    parsed_json = json.loads(json_match.group())
                    logger.info(f"{key}ì—ì„œ JSON íŒŒì‹± ì„±ê³µ")
                    
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
                    parsed_json.setdefault('suggested_date', current_datetime.strftime('%Y-%m-%d'))
                    parsed_json.setdefault('suggested_start_time', "09:00")
                    parsed_json.setdefault('suggested_end_time', "10:00")
                    parsed_json.setdefault('title', "ìƒˆ ì¼ì •")
                    parsed_json.setdefault('description', "AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤")
                    parsed_json.setdefault('priority', "MEDIUM")
                    parsed_json.setdefault('attendees', [])
                    parsed_json.setdefault('location', "")
                    parsed_json.setdefault('reasoning', "AI ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.")
                        
                    logger.info("ìœ íš¨í•œ ì œì•ˆ ì¶”ì¶œ ì„±ê³µ")
                    return parsed_json
            except (json.JSONDecodeError, AttributeError, KeyError) as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨ ({key}): {str(e)}")
                continue
            except Exception as e:
                logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ íŒŒì‹± ì˜¤ë¥˜ ({key}): {str(e)}")
                continue
        
        # ëª¨ë“  íŒŒì‹±ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
        logger.warning("ëª¨ë“  AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
        return {
            "title": "ìƒˆ ì¼ì •",
            "description": "AIê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤",
            "suggested_date": current_datetime.strftime('%Y-%m-%d'),
            "suggested_start_time": "09:00",
            "suggested_end_time": "10:00",
            "location": "",
            "priority": "MEDIUM",
            "attendees": [],
            "reasoning": "ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ì œì•ˆì„ ì¢…í•©í•œ ê²°ê³¼ì…ë‹ˆë‹¤."
        }
    
    def _parse_individual_suggestions(self, suggestions):
        """ê°œë³„ ì œì•ˆë“¤ì„ íŒŒì‹±"""
        logger.info("ê°œë³„ ì œì•ˆ íŒŒì‹± ì‹œì‘")
        parsed = []
        current_datetime = get_current_datetime()
        
        for key, suggestion in suggestions.items():
            try:
                json_match = re.search(r'\{.*\}', suggestion, re.DOTALL)
                if json_match:
                    parsed_suggestion = json.loads(json_match.group())
                    parsed_suggestion['source'] = key.replace('_suggestion', '')
                    
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
                    parsed_suggestion.setdefault('suggested_date', current_datetime.strftime('%Y-%m-%d'))
                    parsed_suggestion.setdefault('suggested_start_time', "09:00")
                    parsed_suggestion.setdefault('suggested_end_time', "10:00")
                    parsed_suggestion.setdefault('title', "ìƒˆ ì¼ì •")
                    parsed_suggestion.setdefault('description', f"{key.replace('_suggestion', '').upper()}ê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤")
                    parsed_suggestion.setdefault('priority', "MEDIUM")
                    parsed_suggestion.setdefault('attendees', [])
                    parsed_suggestion.setdefault('location', "")
                    parsed_suggestion.setdefault('reasoning', f"{key.replace('_suggestion', '').upper()} ëª¨ë¸ì˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.")
                        
                    parsed.append(parsed_suggestion)
                    logger.info(f"{key} ê°œë³„ ì œì•ˆ íŒŒì‹± ì„±ê³µ")
            except (json.JSONDecodeError, AttributeError, KeyError) as e:
                logger.warning(f"ê°œë³„ ì œì•ˆ íŒŒì‹± ì‹¤íŒ¨ ({key}): {str(e)}")
                continue
            except Exception as e:
                logger.error(f"ê°œë³„ ì œì•ˆ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({key}): {str(e)}")
                continue
        
        # íŒŒì‹±ëœ ì œì•ˆì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì œì•ˆ ìƒì„±
        if not parsed:
            logger.warning("ëª¨ë“  ê°œë³„ ì œì•ˆ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ì œì•ˆ ìƒì„±")
            for model in ['gpt', 'claude', 'mixtral']:
                parsed.append({
                    "title": "ìƒˆ ì¼ì •",
                    "description": f"{model.upper()}ê°€ ì œì•ˆí•œ ì¼ì •ì…ë‹ˆë‹¤",
                    "suggested_date": current_datetime.strftime('%Y-%m-%d'),
                    "suggested_start_time": "09:00",
                    "suggested_end_time": "10:00",
                    "location": "",
                    "priority": "MEDIUM",
                    "attendees": [],
                    "reasoning": f"{model.upper()} ëª¨ë¸ì˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.",
                    "source": model
                })
        
        logger.info(f"ê°œë³„ ì œì•ˆ íŒŒì‹± ì™„ë£Œ: {len(parsed)}ê°œ")
        return parsed
        
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from datetime import datetime
from collections import Counter

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


# views.py - ê¶Œí•œ ì„¤ì • ì¶”ê°€
# views.py - ëª¨ë“  APIViewì— ê¶Œí•œ ì„¤ì • ì¶”ê°€




# views.py - ì§„í–‰ë¥  ì¶”ì  ê°œì„  ë²„ì „
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


# ì „ì—­ ì§„í–‰ë¥  ì¶”ì 
analysis_progress_tracker = {}


#  
# views.py - ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€

import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# ì „ì—­ ì§„í–‰ë¥  ì¶”ì  (ê¸°ì¡´ê³¼ ë™ì¼)
analysis_progress_tracker = {}
import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# ì „ì—­ ì§„í–‰ë¥  ì¶”ì  (ê¸°ì¡´ê³¼ ë™ì¼)
analysis_progress_tracker = {}

class AnalysisProgressTracker:
    """ë¶„ì„ ì§„í–‰ë¥  ì¶”ì  í´ë˜ìŠ¤ - ê³ ê¸‰ ë¶„ì„ ë‹¨ê³„ ì¶”ê°€"""
    
    def __init__(self):
        self.progress_data = {}
    
    def start_tracking(self, video_id, total_frames=0, analysis_type='enhanced'):
        """ë¶„ì„ ì¶”ì  ì‹œì‘"""
        self.progress_data[video_id] = {
            'progress': 0,
            'currentStep': 'ë¶„ì„ ì¤€ë¹„ì¤‘',
            'startTime': datetime.now().isoformat(),
            'processedFrames': 0,
            'totalFrames': total_frames,
            'estimatedTime': None,
            'analysisType': analysis_type,
            'steps': [],
            'currentFeature': '',
            'completedFeatures': [],
            'totalFeatures': self._get_total_features(analysis_type)
        }
    
    def update_progress(self, video_id, progress=None, step=None, processed_frames=None, current_feature=None):
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ - ê³ ê¸‰ ë¶„ì„ ì •ë³´ í¬í•¨"""
        if video_id not in self.progress_data:
            return
        
        data = self.progress_data[video_id]
        
        if progress is not None:
            data['progress'] = min(100, max(0, progress))
        
        if step is not None:
            data['currentStep'] = step
            data['steps'].append({
                'step': step,
                'timestamp': datetime.now().isoformat()
            })
        
        if current_feature is not None:
            data['currentFeature'] = current_feature
            if current_feature not in data['completedFeatures']:
                data['completedFeatures'].append(current_feature)
        
        if processed_frames is not None:
            data['processedFrames'] = processed_frames
            
            # ì§„í–‰ë¥  ìë™ ê³„ì‚° (í”„ë ˆì„ ê¸°ë°˜ + ê¸°ëŠ¥ ê¸°ë°˜)
            if data['totalFrames'] > 0:
                frame_progress = (processed_frames / data['totalFrames']) * 80  # í”„ë ˆì„ ë¶„ì„ 80%
                feature_progress = (len(data['completedFeatures']) / data['totalFeatures']) * 20  # í›„ì²˜ë¦¬ 20%
                calculated_progress = frame_progress + feature_progress
                data['progress'] = min(100, calculated_progress)
        
        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (ê³ ê¸‰ ë¶„ì„ ê³ ë ¤)
        if data['progress'] > 5:
            elapsed = (datetime.now() - datetime.fromisoformat(data['startTime'])).total_seconds()
            
            # ë¶„ì„ íƒ€ì…ë³„ ì‹œê°„ ê°€ì¤‘ì¹˜
            time_weights = {
                'basic': 1.0,
                'enhanced': 2.0,
                'comprehensive': 4.0,
                'custom': 2.5
            }
            
            weight = time_weights.get(data['analysisType'], 2.0)
            estimated_total = (elapsed / data['progress']) * 100 * weight
            remaining = estimated_total - elapsed
            data['estimatedTime'] = max(0, remaining)
    
    def _get_total_features(self, analysis_type):
        """ë¶„ì„ íƒ€ì…ë³„ ì´ ê¸°ëŠ¥ ìˆ˜"""
        feature_counts = {
            'basic': 2,  # ê°ì²´ê°ì§€, ê¸°ë³¸ìº¡ì…˜
            'enhanced': 4,  # ê°ì²´ê°ì§€, CLIP, OCR, ê³ ê¸‰ìº¡ì…˜
            'comprehensive': 6,  # ëª¨ë“  ê¸°ëŠ¥
            'custom': 4  # í‰ê· ê°’
        }
        return feature_counts.get(analysis_type, 4)
    
    def get_progress(self, video_id):
        """ì§„í–‰ë¥  ì¡°íšŒ"""
        return self.progress_data.get(video_id, {})
    
    def finish_tracking(self, video_id, success=True):
        """ë¶„ì„ ì™„ë£Œ"""
        if video_id in self.progress_data:
            self.progress_data[video_id]['progress'] = 100
            self.progress_data[video_id]['currentStep'] = 'ë¶„ì„ ì™„ë£Œ' if success else 'ë¶„ì„ ì‹¤íŒ¨'
            self.progress_data[video_id]['success'] = success
            # ì™„ë£Œ í›„ 10ë¶„ ë’¤ ë°ì´í„° ì‚­ì œ
            threading.Timer(600, lambda: self.progress_data.pop(video_id, None)).start()

# ì „ì—­ íŠ¸ë˜ì»¤ ì¸ìŠ¤í„´ìŠ¤
progress_tracker = AnalysisProgressTracker()

# views.py - EnhancedAnalyzeVideoView í´ë˜ìŠ¤ ì™„ì „ ìˆ˜ì •
import threading
import time
import json
import cv2
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
# views.py ìƒë‹¨ import ë¶€ë¶„ - ìˆ˜ì •ë¨

import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

# ëª¨ë¸ imports
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient

# âœ… ì¤‘ìš”: get_video_analyzer í•¨ìˆ˜ import ì¶”ê°€
from .video_analyzer import get_video_analyzer, VideoAnalyzer

# âœ… ì¶”ê°€: ê¸°íƒ€ í•„ìš”í•œ í•¨ìˆ˜ë“¤ë„ import
try:
    from .video_analyzer import (
        EnhancedVideoAnalyzer, 
        ColorAnalyzer, 
        SceneClassifier, 
        AdvancedSceneAnalyzer,
        log_once  # ë¡œê·¸ ì¤‘ë³µ ë°©ì§€ í•¨ìˆ˜
    )
    print("âœ… video_analyzer ëª¨ë“ˆì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ import ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ video_analyzer import ë¶€ë¶„ ì‹¤íŒ¨: {e}")
    # Fallback - ê¸°ë³¸ í´ë˜ìŠ¤ë§Œ import
    try:
        from .video_analyzer import get_video_analyzer, VideoAnalyzer, log_once
        print("âœ… video_analyzer ëª¨ë“ˆì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ import ì„±ê³µ")
    except ImportError as e:
        print(f"âš ï¸ video_analyzer import ë¶€ë¶„ ì‹¤íŒ¨: {e}")
        get_video_analyzer = None
        VideoAnalyzer = None
        log_once = None

# views.py - ì‹¤ì œ AI ë¶„ì„ì„ ì‚¬ìš©í•˜ëŠ” EnhancedAnalyzeVideoView

import os
import json
import time
import threading
from datetime import datetime
from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny

# ë¹„ë””ì˜¤ ë¶„ì„ê¸° import
try:
    from .services.video_analysis_service import get_video_analyzer, get_analyzer_status, VIDEO_ANALYZER_AVAILABLE
    from .db_builder import get_video_rag_system
    VIDEO_ANALYZER_AVAILABLE = True
    print("âœ… video_analyzer ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e:
    VIDEO_ANALYZER_AVAILABLE = False
    print(f"âŒ video_analyzer import ì‹¤íŒ¨: {e}")

# Django ëª¨ë¸ import
from .models import Video, VideoAnalysis, Scene, Frame

@method_decorator(csrf_exempt, name='dispatch')
class EnhancedAnalyzeVideoView(APIView):
    """ì‹¤ì œ AI ë¶„ì„ì„ ì‚¬ìš©í•˜ëŠ” ê³ ê¸‰ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):
        try:
            print(f"ğŸš€ ì‹¤ì œ AI ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: video_id={video_id}")
            
            analysis_type = request.data.get('analysisType', 'enhanced')
            analysis_config = request.data.get('analysisConfig', {})
            enhanced_analysis = request.data.get('enhancedAnalysis', True)
            
            print(f"ğŸ“‹ ë¶„ì„ ìš”ì²­ ì •ë³´:")
            print(f"  - ë¹„ë””ì˜¤ ID: {video_id}")
            print(f"  - ë¶„ì„ íƒ€ì…: {analysis_type}")
            print(f"  - ê³ ê¸‰ ë¶„ì„: {enhanced_analysis}")
            print(f"  - ë¶„ì„ ì„¤ì •: {analysis_config}")
            
            # ë¹„ë””ì˜¤ ì¡´ì¬ í™•ì¸
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # ì´ë¯¸ ë¶„ì„ ì¤‘ì¸ì§€ í™•ì¸
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'ì´ë¯¸ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.',
                    'current_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # AI ë¶„ì„ê¸° ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not VIDEO_ANALYZER_AVAILABLE:
                return Response({
                    'error': 'AI ë¶„ì„ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.',
                    'fallback': 'basic_analysis'
                }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
            
            # ë¶„ì„ê¸° ìƒíƒœ í™•ì¸
            analyzer_status = get_analyzer_status()
            print(f"ğŸ” ë¶„ì„ê¸° ìƒíƒœ: {analyzer_status}")
            
            # ë¹„ë””ì˜¤ ë¶„ì„ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            from .services.video_analysis_service import get_video_analysis_service
            video_service = get_video_analysis_service()
            
            # ë¹„ë””ì˜¤ ë¶„ì„ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ë¶„ì„ ì‹œì‘
            result = video_service.analyze_video(
                video_id=video.id,
                analysis_type=analysis_type
            )
            
            if result.get('success', False):
                return Response({
                    'success': True,
                    'message': f'{self._get_analysis_type_name(analysis_type)} AI ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'video_id': video.id,
                    'analysis_type': analysis_type,
                    'enhanced_analysis': enhanced_analysis,
                    'estimated_time': self._get_estimated_time_real(analysis_type),
                    'status': 'processing',
                    'ai_features': analyzer_status.get('features', {}),
                    'analysis_method': 'real_ai_analysis'
                })
            else:
                return Response({
                    'error': result.get('error', 'ë¶„ì„ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'),
                    'fallback': 'basic_analysis'
                }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
        except Exception as e:
            print(f"âŒ AI ë¶„ì„ ì‹œì‘ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return Response({
                'error': f'AI ë¶„ì„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _run_real_ai_analysis(self, video, analysis_type, analysis_config, enhanced_analysis):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ì‹¤ì œ AI ë¶„ì„ í•¨ìˆ˜"""
        start_time = time.time()
        
        try:
            print(f"ğŸš€ ë¹„ë””ì˜¤ {video.id} ì‹¤ì œ AI ë¶„ì„ ì‹œì‘ - íƒ€ì…: {analysis_type}")
            
            # 1. VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            analyzer = get_video_analyzer()
            if not analyzer:
                raise Exception("VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            print(f"âœ… VideoAnalyzer ë¡œë“œ ì™„ë£Œ: {type(analyzer).__name__}")
            
            # 2. ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            analysis_results_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            os.makedirs(analysis_results_dir, exist_ok=True)
            
            # 3. JSON íŒŒì¼ëª… ìƒì„±
            timestamp = int(time.time())
            json_filename = f"real_analysis_{video.id}_{analysis_type}_{timestamp}.json"
            json_filepath = os.path.join(analysis_results_dir, json_filename)
            
            print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {json_filepath}")
            
            # 4. ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ ì •ì˜
            def progress_callback(progress, message):
                print(f"ğŸ“Š ë¶„ì„ ì§„í–‰ë¥ : {progress:.1f}% - {message}")
                # í•„ìš”ì‹œ ì›¹ì†Œì¼“ì´ë‚˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
            
            # 5. ì‹¤ì œ AI ë¶„ì„ ìˆ˜í–‰
            print("ğŸ§  ì‹¤ì œ AI ë¶„ì„ ì‹œì‘...")
            analysis_results = analyzer.analyze_video_comprehensive(
                video=video,
                analysis_type=analysis_type,
                progress_callback=progress_callback
            )
            
            if not analysis_results.get('success', False):
                raise Exception(f"AI ë¶„ì„ ì‹¤íŒ¨: {analysis_results.get('error', 'Unknown error')}")
            
            print(f"âœ… AI ë¶„ì„ ì™„ë£Œ: {analysis_results.get('total_frames_analyzed', 0)}ê°œ í”„ë ˆì„ ì²˜ë¦¬")
            
            # 6. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            analysis_results['metadata'] = {
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_type': analysis_type,
                'analysis_config': analysis_config,
                'enhanced_analysis': enhanced_analysis,
                'json_file_path': json_filepath,
                'analysis_timestamp': datetime.now().isoformat(),
                'total_frames': getattr(video, 'total_frames', 0),
                'video_duration': getattr(video, 'duration', 0),
                'fps': getattr(video, 'fps', 30),
                'processing_time_seconds': time.time() - start_time,
                'analysis_method': 'real_ai_enhanced',
                'ai_features_used': analysis_results.get('analysis_config', {}).get('features_enabled', {})
            }
            
            # 7. JSON íŒŒì¼ ì €ì¥
            try:
                with open(json_filepath, 'w', encoding='utf-8') as f:
                    json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
                print(f"âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: {json_filepath}")
            except Exception as json_error:
                print(f"âš ï¸ JSON ì €ì¥ ì‹¤íŒ¨: {json_error}")
                # JSON ì €ì¥ ì‹¤íŒ¨í•´ë„ DBëŠ” ì €ì¥í•˜ë„ë¡ ê³„ì† ì§„í–‰
            
            # 8. Django ëª¨ë¸ì— ë¶„ì„ ê²°ê³¼ ì €ì¥
            self._save_analysis_to_db(video, analysis_results, enhanced_analysis, json_filepath)
            
            # 9. RAG ì‹œìŠ¤í…œì— ë¶„ì„ ê²°ê³¼ ë“±ë¡
            self._register_to_rag_system(video.id, json_filepath)
            
            # 10. ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.save()
            
            processing_time = time.time() - start_time
            print(f"ğŸ‰ ë¹„ë””ì˜¤ {video.id} ì‹¤ì œ AI ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“Š ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
            print(f"ğŸ“Š ìµœì¢… í†µê³„: {analysis_results.get('total_frames_analyzed', 0)}ê°œ í”„ë ˆì„ ë¶„ì„")
            
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ {video.id} AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            
            # ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            try:
                video.analysis_status = 'failed'
                video.save()
            except Exception as save_error:
                print(f"âš ï¸ ì˜¤ë¥˜ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {save_error}")


    def _save_frame_image(self, video, frame_data):
        """í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ media/imagesì— ì €ì¥"""
        print(f"ğŸ–¼ï¸ í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì‹œì‘: video_id={video.id}, image_id={frame_data.get('image_id')}")
        
        images_dir = os.path.join(settings.MEDIA_ROOT, 'images')
        os.makedirs(images_dir, exist_ok=True)
        print(f"ğŸ“ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {images_dir}")

        video_path = video.file_path
        print(f"ğŸ¥ ë¹„ë””ì˜¤ ê²½ë¡œ (ì›ë³¸): {video_path}")
        
        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not os.path.isabs(video_path):
            video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            print(f"ğŸ¥ ë¹„ë””ì˜¤ ê²½ë¡œ (ì ˆëŒ€): {video_path}")
        
        if not video_path or not os.path.exists(video_path):
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {video_path}")
            return None

        # timestamp(ì´ˆ) â†’ ms ë‹¨ìœ„ë¡œ í”„ë ˆì„ ìœ„ì¹˜ ì´ë™
        timestamp = frame_data.get('timestamp', 0)
        print(f"â° íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}ì´ˆ")
        
        cap = cv2.VideoCapture(video_path)
        cap.set(cv2.CAP_PROP_POS_MSEC, timestamp * 1000)
        ret, frame = cap.read()
        cap.release()
        
        print(f"ğŸ“¸ í”„ë ˆì„ ì½ê¸° ê²°ê³¼: ret={ret}, frame_shape={frame.shape if frame is not None else 'None'}")

        if ret and frame is not None:
            filename = f"video{video.id}_frame{frame_data.get('image_id', 0)}.jpg"
            filepath = os.path.join(images_dir, filename)
            print(f"ğŸ’¾ ì €ì¥í•  íŒŒì¼: {filepath}")
            
            success = cv2.imwrite(filepath, frame)
            print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ ê²°ê³¼: {success}")
            
            if success:
                relative_path = os.path.relpath(filepath, settings.MEDIA_ROOT)
                print(f"ğŸ”— ìƒëŒ€ ê²½ë¡œ: {relative_path}")
                return relative_path
            else:
                print(f"âŒ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨")
                return None
        else:
            print(f"âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
            return None
    def _save_analysis_to_db(self, video, analysis_results, enhanced_analysis, json_filepath):
        """ë¶„ì„ ê²°ê³¼ë¥¼ Django DBì— ì €ì¥"""
        try:
            print("ğŸ’¾ ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥ ì¤‘...")

            video_summary = analysis_results.get('video_summary', {})
            frame_results = (
                analysis_results.get('frame_results')
                or analysis_results.get('frames')
                or []
            )
            analysis_config = analysis_results.get('analysis_config', {})
            metadata = analysis_results.get('metadata', {})

            # VideoAnalysis ìƒì„±
            VideoAnalysis.objects.create(
                video=video,
                enhanced_analysis=enhanced_analysis,
                success_rate=95.0,
                processing_time_seconds=metadata.get('processing_time_seconds', 0),
                analysis_statistics={
                    'total_frames_analyzed': analysis_results.get('total_frames_analyzed', 0),
                    'unique_objects': len(video_summary.get('dominant_objects', [])),
                    'analysis_method': 'real_ai_enhanced',
                    'ai_features_used': analysis_config.get('features_enabled', {}),
                    'scene_types': video_summary.get('scene_types', []),
                    'text_extracted': bool(video_summary.get('text_content')),
                    'json_file_path': json_filepath,
                    'dominant_objects': video_summary.get('dominant_objects', []),
                    'analysis_quality_metrics': video_summary.get('analysis_quality_metrics', {}),
                    'processing_statistics': video_summary.get('processing_statistics', {})
                },
                caption_statistics={
                    'frames_with_caption': len([f for f in frame_results if f.get('final_caption')]),
                    'enhanced_captions': len([f for f in frame_results if f.get('enhanced_caption')]),
                    'text_content_length': len(video_summary.get('text_content', '')),
                    'average_confidence': video_summary.get('analysis_quality_metrics', {}).get(
                        'average_detection_confidence', 0.8
                    )
                }
            )

            # Scene ì €ì¥ (í•˜ì´ë¼ì´íŠ¸ í”„ë ˆì„ ê¸°ë°˜)
            highlight_frames = video_summary.get('highlight_frames', [])
            scene_duration = video.duration / max(len(highlight_frames), 1) if video.duration > 0 else 1

            for i, highlight in enumerate(highlight_frames[:10]):
                Scene.objects.create(
                    video=video,
                    scene_id=i + 1,
                    start_time=max(0, highlight.get('timestamp', 0) - scene_duration/2),
                    end_time=min(video.duration, highlight.get('timestamp', 0) + scene_duration/2),
                    duration=scene_duration,
                    frame_count=60,
                    dominant_objects=video_summary.get('dominant_objects', [])[:5],
                    enhanced_captions_count=1 if highlight.get('object_count', 0) > 0 else 0
                )

            # Frame ì €ì¥ (í”„ë ˆì„ ID ê¸°ì¤€ìœ¼ë¡œ ì „ë¶€ ì €ì¥)
            important_frames = [f for f in frame_results if f.get('image_id') is not None]

            for frame_data in important_frames[:50]:
                try:
                    # âœ… ì´ë¯¸ì§€ ì €ì¥
                    image_path = self._save_frame_image(video, frame_data)
                    
                    if image_path:
                        print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ ì„±ê³µ: {image_path}")
                    else:
                        print(f"âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨, í”„ë ˆì„ ì •ë³´ë§Œ ì €ì¥")

                    # âœ… persons ë°ì´í„°ë¥¼ detected_objectsì— ì €ì¥
                    persons_data = frame_data.get("persons", [])
                    
                    # âœ… attributes ì•ˆì—ì„œ êº¼ë‚´ê¸°
                    attrs = frame_data.get("attributes", {})

                    detected = {
                        'persons': persons_data,  # YOLOë¡œ ê°ì§€ëœ ì‚¬ëŒ ê°ì²´ë“¤
                        'clothing': attrs.get('detailed_clothing', {}),
                        'color': attrs.get('clothing_color', {}),
                        'accessories': attrs.get('accessories', {}),
                        'posture': attrs.get('posture', {}),
                        'hair_style': attrs.get('hair_style', {}),
                        'facial_attributes': attrs.get('facial_attributes', {})
                    }

                    Frame.objects.create(
                        video=video,
                        image_id=frame_data.get('image_id', 0),
                        timestamp=frame_data.get('timestamp', 0),
                        caption=frame_data.get('caption', ''),  # ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
                        enhanced_caption=frame_data.get('enhanced_caption', ''),
                        final_caption=frame_data.get('final_caption', ''),
                        detected_objects=detected,
                        comprehensive_features={
                            "crop_quality": frame_data.get("crop_quality", {}),
                            "pose_analysis": attrs.get("pose_analysis", {}),
                            "facial_details": attrs.get("facial_details", {})
                        },
                        image=image_path if image_path else None  # âœ… ì €ì¥ëœ ê²½ë¡œ ì—°ê²° (None í—ˆìš©)
                    )
                except Exception as frame_error:
                    print(f"âš ï¸ í”„ë ˆì„ {frame_data.get('image_id', 'unknown')} ì €ì¥ ì‹¤íŒ¨: {frame_error}")
                    continue

            print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {len(important_frames)}ê°œ í”„ë ˆì„, {len(highlight_frames)}ê°œ ì”¬")

        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ğŸ” DB ì €ì¥ ì˜¤ë¥˜ ìƒì„¸:\n{traceback.format_exc()}")


    def _register_to_rag_system(self, video_id, json_filepath):
        """RAG ì‹œìŠ¤í…œì— ë¶„ì„ ê²°ê³¼ ë“±ë¡"""
        try:
            print(f"ğŸ” RAG ì‹œìŠ¤í…œì— ë¹„ë””ì˜¤ {video_id} ë“±ë¡ ì¤‘...")
            
            rag_system = get_video_rag_system()
            if not rag_system:
                print("âš ï¸ RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return
            
            success = rag_system.process_video_analysis_json(json_filepath, str(video_id))
            
            if success:
                print(f"âœ… RAG ì‹œìŠ¤í…œ ë“±ë¡ ì™„ë£Œ: ë¹„ë””ì˜¤ {video_id}")
            else:
                print(f"âš ï¸ RAG ì‹œìŠ¤í…œ ë“±ë¡ ì‹¤íŒ¨: ë¹„ë””ì˜¤ {video_id}")
                
        except Exception as e:
            print(f"âŒ RAG ì‹œìŠ¤í…œ ë“±ë¡ ì˜¤ë¥˜: {e}")
    
    def _get_analysis_type_name(self, analysis_type):
        """ë¶„ì„ íƒ€ì… ì´ë¦„ ë°˜í™˜"""
        type_names = {
            'basic': 'ê¸°ë³¸ AI ë¶„ì„',
            'enhanced': 'í–¥ìƒëœ AI ë¶„ì„',
            'comprehensive': 'ì¢…í•© AI ë¶„ì„',
            'custom': 'ì‚¬ìš©ì ì •ì˜ AI ë¶„ì„'
        }
        return type_names.get(analysis_type, 'í–¥ìƒëœ AI ë¶„ì„')
    
    def _get_estimated_time_real(self, analysis_type):
        """ì‹¤ì œ AI ë¶„ì„ íƒ€ì…ë³„ ì˜ˆìƒ ì‹œê°„"""
        time_estimates = {
            'basic': '5-15ë¶„',
            'enhanced': '10-30ë¶„', 
            'comprehensive': '20-60ë¶„',
            'custom': 'ìƒí™©ì— ë”°ë¼ ë‹¤ë¦„'
        }
        return time_estimates.get(analysis_type, '10-30ë¶„')
    
    def get(self, request, video_id):
        """ë¶„ì„ ìƒíƒœ ì¡°íšŒ"""
        try:
            video = Video.objects.get(id=video_id)
            
            analyzer_status = get_analyzer_status() if VIDEO_ANALYZER_AVAILABLE else {'status': 'unavailable'}
            
            return Response({
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_status': video.analysis_status,
                'is_analyzed': video.is_analyzed,
                'analyzer_available': VIDEO_ANALYZER_AVAILABLE,
                'analyzer_status': analyzer_status,
                'last_updated': video.updated_at.isoformat() if hasattr(video, 'updated_at') else None
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
# ğŸ†• ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
class VideoQAAnalyticsView(APIView):
    """ë¹„ë””ì˜¤ QA ë¶„ì„ ë° í†µê³„ ë·°"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.rag_system = get_video_rag_system()
        self.enhanced_qa = EnhancedVideoQASystem(self.rag_system, LLMClient())
    
    def get(self, request, video_id=None):
        """QA í†µê³„ ì¡°íšŒ"""
        try:
            if video_id:
                # íŠ¹ì • ë¹„ë””ì˜¤ì˜ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
                context = self.enhanced_qa.get_conversation_context(str(video_id))
                
                # ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                category_stats = {}
                for conv in context:
                    category = conv.get('category', 'unknown')
                    category_stats[category] = category_stats.get(category, 0) + 1
                
                return Response({
                    'video_id': video_id,
                    'total_conversations': len(context),
                    'category_statistics': category_stats,
                    'recent_conversations': context[-5:],  # ìµœê·¼ 5ê°œ
                    'success': True
                })
            else:
                # ì „ì²´ ì‹œìŠ¤í…œ í†µê³„
                total_videos = len(self.enhanced_qa.context_memory)
                total_conversations = sum(len(convs) for convs in self.enhanced_qa.context_memory.values())
                
                return Response({
                    'total_videos_with_conversations': total_videos,
                    'total_conversations': total_conversations,
                    'videos': list(self.enhanced_qa.context_memory.keys()),
                    'success': True
                })
                
        except Exception as e:
            return Response({
                'error': f'í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}',
                'success': False
            }, status=500)
    
    def delete(self, request, video_id=None):
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì‚­ì œ"""
        try:
            if video_id:
                self.enhanced_qa.clear_context(str(video_id))
                return Response({
                    'message': f'ë¹„ë””ì˜¤ {video_id}ì˜ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'success': True
                })
            else:
                self.enhanced_qa.clear_context()
                return Response({
                    'message': 'ëª¨ë“  ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'success': True
                })
                
        except Exception as e:
            return Response({
                'error': f'ì»¨í…ìŠ¤íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {str(e)}',
                'success': False
            }, status=500)


class VideoQAUtils:
    """ë¹„ë””ì˜¤ QA ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤"""
    
    @staticmethod
    def categorize_questions_batch(questions: List[str]) -> Dict[str, List[str]]:
        """ì§ˆë¬¸ë“¤ì„ ë°°ì¹˜ë¡œ ì¹´í…Œê³ ë¦¬í™”"""
        categories = {
            'object_detection': [],
            'people_analysis': [],
            'scene_analysis': [],
            'action_analysis': [],
            'summary': [],
            'specific_search': [],
            'general': []
        }
        
        for question in questions:
            category = VideoQAUtils.classify_single_question(question)
            categories[category].append(question)
        
        return categories
    
    @staticmethod
    def classify_single_question(question: str) -> str:
        """ë‹¨ì¼ ì§ˆë¬¸ ë¶„ë¥˜"""
        question_lower = question.lower()
        
        patterns = {
            'object_detection': ['ë¬´ì—‡ì´', 'ë­ê°€', 'ê°ì²´', 'ì‚¬ë¬¼', 'ë‚˜ì˜¤ëŠ”', 'ë³´ì´ëŠ”'],
            'people_analysis': ['ì‚¬ëŒ', 'ì¸ë¬¼', 'ì–¼êµ´', 'ì„±ë³„', 'ë‚˜ì´', 'ì˜·'],
            'scene_analysis': ['ì¥ë©´', 'ë°°ê²½', 'í™˜ê²½', 'ì¥ì†Œ', 'ìœ„ì¹˜', 'ì‹œê°„'],
            'action_analysis': ['í–‰ë™', 'ë™ì‘', 'í•˜ê³ ìˆ', 'ì›€ì§ì„', 'í™œë™'],
            'summary': ['ìš”ì•½', 'ì •ë¦¬', 'ì „ì²´', 'ë‚´ìš©', 'ì¤„ê±°ë¦¬'],
            'specific_search': ['ì°¾ì•„', 'ê²€ìƒ‰', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëª‡ ë²ˆì§¸']
        }
        
        for category, keywords in patterns.items():
            if any(keyword in question_lower for keyword in keywords):
                return category
        
        return 'general'
    
    @staticmethod
    def generate_question_suggestions(video_analysis_data: Dict) -> List[str]:
        """ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ê°ì²´ ê¸°ë°˜ ì§ˆë¬¸
        if 'objects' in video_analysis_data:
            objects = video_analysis_data['objects'][:3]
            suggestions.extend([
                f"{obj}ê°€ ì–¸ì œ ë‚˜ì˜¤ë‚˜ìš”?" for obj in objects
            ])
        
        # ì¥ë©´ ê¸°ë°˜ ì§ˆë¬¸
        suggestions.extend([
            "ë¹„ë””ì˜¤ì˜ ì£¼ìš” ì¥ë©´ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
            "ì–´ë–¤ ì‚¬ëŒë“¤ì´ ë‚˜ì˜¤ë‚˜ìš”?",
            "ì£¼ìš” í–‰ë™ì´ë‚˜ í™œë™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë°°ê²½ì´ë‚˜ ì¥ì†ŒëŠ” ì–´ë””ì¸ê°€ìš”?"
        ])
        
        return suggestions[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜


# ğŸ†• ìºì‹± ì‹œìŠ¤í…œ (ì„ íƒì‚¬í•­)
from django.core.cache import cache
from hashlib import md5

class QACache:
    """QA ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def get_cache_key(video_id: str, question: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{video_id}:{question}"
        return f"qa_cache:{md5(content.encode()).hexdigest()}"
    
    @staticmethod
    def get_cached_response(video_id: str, question: str) -> Optional[Dict]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        cache_key = QACache.get_cache_key(video_id, question)
        return cache.get(cache_key)
    
    @staticmethod
    def cache_response(video_id: str, question: str, response: Dict, timeout: int = 300):
        """ì‘ë‹µ ìºì‹±"""
        cache_key = QACache.get_cache_key(video_id, question)
        cache.set(cache_key, response, timeout)
    
    @staticmethod
    def clear_video_cache(video_id: str):
        """íŠ¹ì • ë¹„ë””ì˜¤ì˜ ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        # ìºì‹œ íŒ¨í„´ìœ¼ë¡œ ì‚­ì œ (Redis ì‚¬ìš© ì‹œ)
        pattern = f"qa_cache:*{video_id}*"
        # êµ¬í˜„ì€ ì‚¬ìš©í•˜ëŠ” ìºì‹œ ë°±ì—”ë“œì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
        
# ìƒˆë¡œìš´ ë·° ì¶”ê°€: AnalysisCapabilitiesView ì™„ì „ êµ¬í˜„
class AnalysisCapabilitiesView(APIView):
    """ì‹œìŠ¤í…œ ë¶„ì„ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸ - ì™„ì „ êµ¬í˜„"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("ğŸ” AnalysisCapabilitiesView: ë¶„ì„ ê¸°ëŠ¥ ìƒíƒœ ìš”ì²­")
            
            # VideoAnalyzer ìƒíƒœ í™•ì¸
            analyzer_status = self._check_video_analyzer()
            
            # MultiLLM ìƒíƒœ í™•ì¸
            multi_llm_status = self._check_multi_llm_analyzer()
            
            # ì‹œìŠ¤í…œ ê¸°ëŠ¥ ìƒíƒœ
            capabilities = {
                'system_status': {
                    'analyzer_available': analyzer_status['available'],
                    'multi_llm_available': multi_llm_status['available'],
                    'device': analyzer_status.get('device', 'unknown'),
                    'timestamp': datetime.now().isoformat()
                },
                'core_features': {
                    'object_detection': {
                        'name': 'ê°ì²´ ê°ì§€',
                        'available': analyzer_status.get('yolo_available', False),
                        'description': 'YOLO ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì²´ ê°ì§€',
                        'icon': 'ğŸ¯'
                    },
                    'enhanced_captions': {
                        'name': 'ê³ ê¸‰ ìº¡ì…˜ ìƒì„±',
                        'available': True,
                        'description': 'AI ê¸°ë°˜ ìƒì„¸ ìº¡ì…˜ ìƒì„±',
                        'icon': 'ğŸ’¬'
                    }
                },
                'advanced_features': {
                    'clip_analysis': {
                        'name': 'CLIP ë¶„ì„',
                        'available': analyzer_status.get('clip_available', False),
                        'description': 'OpenAI CLIP ëª¨ë¸ ê¸°ë°˜ ì”¬ ì´í•´',
                        'icon': 'ğŸ–¼ï¸'
                    },
                    'ocr_text_extraction': {
                        'name': 'OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ',
                        'available': analyzer_status.get('ocr_available', False),
                        'description': 'EasyOCR ê¸°ë°˜ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì‹',
                        'icon': 'ğŸ“'
                    },
                    'vqa_analysis': {
                        'name': 'VQA ì§ˆë¬¸ë‹µë³€',
                        'available': analyzer_status.get('vqa_available', False),
                        'description': 'BLIP ëª¨ë¸ ê¸°ë°˜ ì‹œê°ì  ì§ˆë¬¸ ë‹µë³€',
                        'icon': 'â“'
                    },
                    'scene_graph': {
                        'name': 'Scene Graph',
                        'available': analyzer_status.get('scene_graph_available', False),
                        'description': 'NetworkX ê¸°ë°˜ ê°ì²´ ê´€ê³„ ë¶„ì„',
                        'icon': 'ğŸ•¸ï¸'
                    }
                },
                'multi_llm_features': {
                    'gpt4v': {
                        'name': 'GPT-4V',
                        'available': multi_llm_status.get('gpt4v_available', False),
                        'description': 'OpenAI GPT-4 Vision',
                        'icon': 'ğŸŸ¢'
                    },
                    'claude': {
                        'name': 'Claude-3.5',
                        'available': multi_llm_status.get('claude_available', False),
                        'description': 'Anthropic Claude-3.5 Sonnet',
                        'icon': 'ğŸŸ '
                    },
                    'gemini': {
                        'name': 'Gemini Pro',
                        'available': multi_llm_status.get('gemini_available', False),
                        'description': 'Google Gemini Pro Vision',
                        'icon': 'ğŸ”µ'
                    },
                    'groq': {
                        'name': 'Groq Llama',
                        'available': multi_llm_status.get('groq_available', False),
                        'description': 'Groq Llama-3.1-70B',
                        'icon': 'âš¡'
                    }
                },
                'api_status': {
                    'openai_available': multi_llm_status.get('openai_api_key', False),
                    'anthropic_available': multi_llm_status.get('anthropic_api_key', False),
                    'google_available': multi_llm_status.get('google_api_key', False),
                    'groq_available': multi_llm_status.get('groq_api_key', False)
                }
            }
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ ìˆ˜ ê³„ì‚°
            total_features = (len(capabilities['core_features']) + 
                            len(capabilities['advanced_features']) + 
                            len(capabilities['multi_llm_features']))
            
            available_features = sum(1 for features in [
                capabilities['core_features'], 
                capabilities['advanced_features'],
                capabilities['multi_llm_features']
            ] for feature in features.values() if feature.get('available', False))
            
            capabilities['summary'] = {
                'total_features': total_features,
                'available_features': available_features,
                'availability_rate': (available_features / total_features * 100) if total_features > 0 else 0,
                'system_ready': analyzer_status['available'] and available_features > 0,
                'multi_llm_ready': multi_llm_status['available'] and multi_llm_status['model_count'] > 0
            }
            
            print(f"âœ… ë¶„ì„ ê¸°ëŠ¥ ìƒíƒœ: {available_features}/{total_features} ì‚¬ìš© ê°€ëŠ¥")
            
            return Response(capabilities)
            
        except Exception as e:
            print(f"âŒ AnalysisCapabilitiesView ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return Response({
                'system_status': {
                    'analyzer_available': False,
                    'multi_llm_available': False,
                    'device': 'error',
                    'error': str(e)
                },
                'summary': {
                    'system_ready': False,
                    'error': str(e)
                }
            }, status=500)
    
    def _check_video_analyzer(self):
        """VideoAnalyzer ìƒíƒœ í™•ì¸"""
        try:
            analyzer = get_video_analyzer()
            return {
                'available': True,
                'device': getattr(analyzer, 'device', 'cpu'),
                'yolo_available': getattr(analyzer, 'model', None) is not None,
                'clip_available': getattr(analyzer, 'clip_available', False),
                'ocr_available': getattr(analyzer, 'ocr_available', False),
                'vqa_available': getattr(analyzer, 'vqa_available', False),
                'scene_graph_available': getattr(analyzer, 'scene_graph_available', False)
            }
        except Exception as e:
            print(f"âŒ VideoAnalyzer ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'available': False, 'error': str(e)}
    
    def _check_multi_llm_analyzer(self):
        """MultiLLM ìƒíƒœ í™•ì¸"""
        try:
            multi_llm = get_multi_llm_analyzer()
            available_models = getattr(multi_llm, 'available_models', [])
            
            return {
                'available': len(available_models) > 0,
                'model_count': len(available_models),
                'available_models': available_models,
                'gpt4v_available': 'gpt-4v' in available_models,
                'claude_available': 'claude-3.5' in available_models,
                'gemini_available': 'gemini-pro' in available_models,
                'groq_available': 'groq-llama' in available_models,
                'openai_api_key': bool(os.getenv("OPENAI_API_KEY")),
                'anthropic_api_key': bool(os.getenv("ANTHROPIC_API_KEY")),
                'google_api_key': bool(os.getenv("GOOGLE_API_KEY")),
                'groq_api_key': bool(os.getenv("GROQ_API_KEY"))
            }
        except Exception as e:
            print(f"âŒ MultiLLM ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'available': False, 'error': str(e)}


# ìƒˆë¡œìš´ ë·°: MultiLLM ì „ìš© ì±„íŒ… ë·°
class MultiLLMChatView(APIView):
    """ë©€í‹° LLM ì „ìš© ì±„íŒ… ë·°"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.multi_llm_analyzer = get_multi_llm_analyzer()
    
    def post(self, request):
        try:
            user_query = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            analysis_mode = request.data.get('analysis_mode', 'comparison')
            
            if not user_query:
                return Response({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}, status=400)
            
            # ë¹„ë””ì˜¤ê°€ ì—†ì–´ë„ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
            video = None
            video_context = {}
            frame_images = []
            
            if video_id:
                try:
                    video = Video.objects.get(id=video_id)
                    video_context = self._prepare_video_context(video)
                    frame_images = self._extract_frames_safely(video)
                except Video.DoesNotExist:
                    pass  # ë¹„ë””ì˜¤ ì—†ì´ë„ ì§„í–‰
            
            # ë©€í‹° LLM ë¶„ì„ ì‹¤í–‰
            multi_responses = self.multi_llm_analyzer.analyze_video_multi_llm(
                frame_images, user_query, video_context
            )
            
            comparison_result = self.multi_llm_analyzer.compare_responses(multi_responses)
            
            return Response({
                'response_type': 'multi_llm_result',
                'query': user_query,
                'video_info': {'id': video.id, 'name': video.original_name} if video else None,
                'llm_responses': {
                    model: {
                        'response': resp.response_text,
                        'confidence': resp.confidence_score,
                        'processing_time': resp.processing_time,
                        'success': resp.success,
                        'error': resp.error
                    }
                    for model, resp in multi_responses.items()
                },
                'comparison_analysis': comparison_result['comparison'],
                'recommendation': comparison_result['comparison']['recommendation']
            })
            
        except Exception as e:
            print(f"âŒ MultiLLM ì±„íŒ… ì˜¤ë¥˜: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _prepare_video_context(self, video):
        """ë¹„ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        context = {
            'duration': video.duration,
            'filename': video.original_name
        }
        
        if hasattr(video, 'analysis') and video.analysis:
            try:
                stats = video.analysis.analysis_statistics
                context.update({
                    'detected_objects': stats.get('dominant_objects', []),
                    'scene_types': stats.get('scene_types', [])
                })
            except:
                pass
        
        return context
    
    def _extract_frames_safely(self, video):
        """ì•ˆì „í•œ í”„ë ˆì„ ì¶”ì¶œ"""
        try:
            # EnhancedVideoChatViewì˜ ë©”ì„œë“œ ì¬ì‚¬ìš©
            view = EnhancedVideoChatView()
            return view._extract_key_frames_for_llm(video, max_frames=2)
        except:
            return []


# LLM í†µê³„ ë·° ì¶”ê°€
class LLMStatsView(APIView):
    """LLM ì„±ëŠ¥ í†µê³„ ë·°"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            # ê°„ë‹¨í•œ í†µê³„ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìˆ˜ì§‘)
            stats = {
                'total_requests': 0,
                'model_usage': {
                    'gpt-4v': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'claude-3.5': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'gemini-pro': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'groq-llama': {'count': 0, 'avg_time': 0, 'success_rate': 0}
                },
                'average_response_time': 0,
                'overall_success_rate': 0,
                'last_updated': datetime.now().isoformat()
            }
            
            return Response(stats)
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)

class VideoListView(APIView):
    """ë¹„ë””ì˜¤ ëª©ë¡ ì¡°íšŒ - ê³ ê¸‰ ë¶„ì„ ì •ë³´ í¬í•¨"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("ğŸ” VideoListView: ë¹„ë””ì˜¤ ëª©ë¡ ìš”ì²­ (ê³ ê¸‰ ë¶„ì„ ì •ë³´ í¬í•¨)")
            videos = Video.objects.all()
            video_list = []
            
            for video in videos:
                video_data = {
                    'id': video.id,
                    'filename': video.filename,
                    'original_name': video.original_name,
                    'duration': video.duration,
                    'is_analyzed': video.is_analyzed,
                    'analysis_status': video.analysis_status,
                    'uploaded_at': video.uploaded_at,
                    'file_size': video.file_size
                }
                
                # ê³ ê¸‰ ë¶„ì„ ì •ë³´ ì¶”ê°€
                if hasattr(video, 'analysis'):
                    analysis = video.analysis
                    stats = analysis.analysis_statistics
                    
                    video_data.update({
                        'enhanced_analysis': analysis.enhanced_analysis,
                        'success_rate': analysis.success_rate,
                        'processing_time': analysis.processing_time_seconds,
                        'analysis_type': stats.get('analysis_type', 'basic'),
                        'advanced_features_used': {
                            'clip': stats.get('clip_analysis', False),
                            'ocr': stats.get('text_extracted', False),
                            'vqa': stats.get('vqa_analysis', False),
                            'scene_graph': stats.get('scene_graph_analysis', False)
                        },
                        'scene_types': stats.get('scene_types', []),
                        'unique_objects': stats.get('unique_objects', 0)
                    })
                
                # ì§„í–‰ë¥  ì •ë³´ ì¶”ê°€ (ë¶„ì„ ì¤‘ì¸ ê²½ìš°)
                if video.analysis_status == 'processing':
                    progress_info = progress_tracker.get_progress(video.id)
                    if progress_info:
                        video_data['progress_info'] = progress_info
                
                video_list.append(video_data)
            
            print(f"âœ… VideoListView: {len(video_list)}ê°œ ë¹„ë””ì˜¤ ë°˜í™˜ (ê³ ê¸‰ ë¶„ì„ ì •ë³´ í¬í•¨)")
            return Response({
                'videos': video_list,
                'total_count': len(video_list),
                'analysis_capabilities': self._get_system_capabilities()
            })
            
        except Exception as e:
            print(f"âŒ VideoListView ì˜¤ë¥˜: {e}")
            return Response({
                'error': f'ë¹„ë””ì˜¤ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _get_system_capabilities(self):
        """ì‹œìŠ¤í…œ ë¶„ì„ ê¸°ëŠ¥ ìƒíƒœ"""
        try:
            # âœ… ìˆ˜ì •: ì „ì—­ VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            analyzer = get_video_analyzer()
            return {
                'clip_available': analyzer.clip_available,
                'ocr_available': analyzer.ocr_available,
                'vqa_available': analyzer.vqa_available,
                'scene_graph_available': analyzer.scene_graph_available
            }
        except:
            return {
                'clip_available': False,
                'ocr_available': False,
                'vqa_available': False,
                'scene_graph_available': False
            }

class AnalysisStatusView(APIView):
    """ë¶„ì„ ìƒíƒœ í™•ì¸ - ì§„í–‰ë¥  ì •ë³´ í¬í•¨"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            response_data = {
                'status': video.analysis_status,
                'video_filename': video.filename,
                'is_analyzed': video.is_analyzed
            }
            
            # ì§„í–‰ë¥  ì •ë³´ ì¶”ê°€
            if video.analysis_status == 'processing':
                progress_info = progress_tracker.get_progress(video.id)
                response_data.update(progress_info)
            
            # ë¶„ì„ ì™„ë£Œëœ ê²½ìš° ìƒì„¸ ì •ë³´ ì¶”ê°€
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                response_data.update({
                    'enhanced_analysis': analysis.enhanced_analysis,
                    'success_rate': analysis.success_rate,
                    'processing_time': analysis.processing_time_seconds,
                    'stats': {
                        'objects': analysis.analysis_statistics.get('unique_objects', 0),
                        'scenes': Scene.objects.filter(video=video).count(),
                        'captions': analysis.caption_statistics.get('frames_with_caption', 0)
                    }
                })
            
            return Response(response_data)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
import threading
import time
import json
import cv2
import os
import base64
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory, CostAnalysis
from .llm_client import LLMClient

# âœ… ì•ˆì „í•œ import
try:
    from .video_analyzer import get_video_analyzer
except ImportError:
    print("âš ï¸ video_analyzer import ì‹¤íŒ¨")
    get_video_analyzer = None

try:
    from .multi_llm_service import get_multi_llm_analyzer
except ImportError:
    print("âš ï¸ multi_llm_service import ì‹¤íŒ¨")
    get_multi_llm_analyzer = None

# âœ… ìˆ˜ì •ëœ AnalyzeVideoView - URL íŒŒë¼ë¯¸í„° ì²˜ë¦¬
class AnalyzeVideoView(APIView):
    """ê¸°ë³¸ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):  # âœ… video_id íŒŒë¼ë¯¸í„° ì¶”ê°€
        try:
            print(f"ğŸ”¬ ê¸°ë³¸ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: video_id={video_id}")
            
            enable_enhanced = request.data.get('enable_enhanced_analysis', False)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # ì´ë¯¸ ë¶„ì„ ì¤‘ì¸ì§€ í™•ì¸
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'ì´ë¯¸ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ë¶„ì„ ìƒíƒœ ì—…ë°ì´íŠ¸
            video.analysis_status = 'processing'
            video.save()
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹œì‘
            analysis_thread = threading.Thread(
                target=self._run_basic_analysis,
                args=(video, enable_enhanced),
                daemon=True
            )
            analysis_thread.start()
            
            return Response({
                'success': True,
                'message': 'ê¸°ë³¸ ë¹„ë””ì˜¤ ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'video_id': video.id,
                'enhanced_analysis': enable_enhanced,
                'estimated_time': '5-10ë¶„'
            })
            
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ë¶„ì„ ì‹œì‘ ì˜¤ë¥˜: {e}")
            return Response({
                'error': f'ë¶„ì„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _run_basic_analysis(self, video, enable_enhanced):
        """ë°±ê·¸ë¼ìš´ë“œ ê¸°ë³¸ ë¶„ì„"""
        try:
            print(f"ğŸ”¬ ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰: {video.original_name}")
            
            # ê°„ë‹¨í•œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
            time.sleep(2)  # ì‹¤ì œë¡œëŠ” ë¶„ì„ ë¡œì§ ìˆ˜í–‰
            
            # VideoAnalysis ìƒì„±
            analysis = VideoAnalysis.objects.create(
                video=video,
                enhanced_analysis=enable_enhanced,
                success_rate=85.0,
                processing_time_seconds=120,
                analysis_statistics={
                    'analysis_type': 'basic',
                    'unique_objects': 8,
                    'total_detections': 45,
                    'scene_types': ['outdoor', 'urban']
                },
                caption_statistics={
                    'frames_with_caption': 25,
                    'average_confidence': 0.8
                }
            )
            
            # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
            video.analysis_status = 'completed'
            video.is_analyzed = True
            video.save()
            
            print(f"âœ… ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ: {video.original_name}")
            
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            video.analysis_status = 'failed'
            video.save()

class AnalysisProgressView(APIView):
    """ë¶„ì„ ì§„í–‰ë¥  ì „ìš© API"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            progress_info = progress_tracker.get_progress(video_id)
            
            if not progress_info:
                return Response({
                    'error': 'ì§„í–‰ ì¤‘ì¸ ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤'
                }, status=status.HTTP_404_NOT_FOUND)
            
            return Response(progress_info)
            
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ê¸°ì¡´ì˜ ë‹¤ë¥¸ View í´ë˜ìŠ¤ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
class VideoUploadView(APIView):
    """ë¹„ë””ì˜¤ ì—…ë¡œë“œ"""
    permission_classes = [AllowAny]
    parser_classes = (MultiPartParser, FormParser)
    
    def post(self, request):
        try:
            if 'video' not in request.FILES:
                return Response({
                    'error': 'ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            video_file = request.FILES['video']
            
            if not video_file.name.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.webm')):
                return Response({
                    'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Generate unique filename
            timestamp = int(time.time())
            filename = f"upload_{timestamp}_{video_file.name}"
            
            # Save file
            file_path = default_storage.save(
                f'uploads/{filename}',
                ContentFile(video_file.read())
            )
            
            # Create Video model instance
            video = Video.objects.create(
                filename=filename,
                original_name=video_file.name,
                file_path=file_path,
                file_size=video_file.size,
                analysis_status='pending'
            )
            
            return Response({
                'success': True,
                'video_id': video.id,
                'filename': filename,
                'message': f'ë¹„ë””ì˜¤ "{video_file.name}"ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.'
            })
            
        except Exception as e:
            return Response({
                'error': f'ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class APIStatusView(APIView):
    """API ìƒíƒœ í™•ì¸"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        # print("ğŸ” APIStatusView: API ìƒíƒœ ìš”ì²­ ë°›ìŒ")
        try:
            llm_client = LLMClient()
            status_info = llm_client.get_api_status()
            
            response_data = {
                'groq': status_info.get('groq', {'available': False}),
                'openai': status_info.get('openai', {'available': False}),
                'anthropic': status_info.get('anthropic', {'available': False}),
                'fallback_enabled': True,
                'timestamp': datetime.now().isoformat(),
                'server_status': 'running',
                'active_analyses': len([k for k, v in progress_tracker.progress_data.items() 
                                     if v.get('progress', 0) < 100])
            }
            
            # print(f"âœ… APIStatusView: ìƒíƒœ ì •ë³´ ë°˜í™˜ - {response_data}")
            return Response(response_data)
        except Exception as e:
            print(f"âŒ APIStatusView ì˜¤ë¥˜: {e}")
            return Response({
                'error': str(e),
                'server_status': 'error'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



class VideoChatView(APIView):
    """ë¹„ë””ì˜¤ ê´€ë ¨ ì±„íŒ… API - ê¸°ì¡´ ChatViewì™€ êµ¬ë¶„"""
    permission_classes = [AllowAny]  # ğŸ”§ ê¶Œí•œ ì„¤ì • ì¶”ê°€
    
    def __init__(self):
        super().__init__()
        self.llm_client = LLMClient()
        self.video_analyzer = VideoAnalyzer()
    
    def post(self, request):
        try:
            user_message = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            
            if not user_message:
                return Response({'response': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})
            
            print(f"ğŸ’¬ ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}")
            
            # Get current video
            if video_id:
                try:
                    current_video = Video.objects.get(id=video_id)
                except Video.DoesNotExist:
                    current_video = Video.objects.filter(is_analyzed=True).first()
            else:
                current_video = Video.objects.filter(is_analyzed=True).first()
            
            if not current_video:
                return Response({
                    'response': 'ë¶„ì„ëœ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.'
                })
            
            # Get video info
            video_info = self._get_video_info(current_video)
            
            # Determine if multi-LLM should be used
            use_multi_llm = "compare" in user_message.lower() or "ë¹„êµ" in user_message or "ë¶„ì„" in user_message
            
            # Handle different query types
            if self._is_search_query(user_message):
                return self._handle_search_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_highlight_query(user_message):
                return self._handle_highlight_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_summary_query(user_message):
                return self._handle_summary_query(user_message, current_video, video_info, use_multi_llm)
            
            elif self._is_info_query(user_message):
                return self._handle_info_query(user_message, current_video, video_info, use_multi_llm)
            
            else:
                # General conversation
                bot_response = self.llm_client.generate_smart_response(
                    user_query=user_message,
                    search_results=None,
                    video_info=video_info,
                    use_multi_llm=use_multi_llm
                )
                return Response({'response': bot_response})
                
        except Exception as e:
            print(f"âŒ Chat error: {e}")
            error_response = self.llm_client.generate_smart_response(
                user_query="ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë„ì›€ì„ ìš”ì²­í•©ë‹ˆë‹¤.",
                search_results=None,
                video_info=None
            )
            return Response({'response': error_response})
    

class FrameView(APIView):
    """ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œí•´ ë°”ë¡œ JPEGë¡œ ì‘ë‹µ"""
    permission_classes = [AllowAny]

    def get(self, request, video_id, frame_number, frame_type='normal'):
        # 1) ë¹„ë””ì˜¤ ì¡°íšŒ
        try:
            video = Video.objects.get(id=video_id)
        except Video.DoesNotExist:
            return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}, status=404)

        # 2) ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì•ˆì „í•˜ê²Œ í•´ì„ (file_path ìš°ì„ , ê·¸ ë‹¤ìŒ MEDIA_ROOT/uploads|videos)
        candidates = []

        fp = getattr(video, 'file_path', None)
        if fp:
            abs_fp = fp if os.path.isabs(fp) else os.path.join(settings.MEDIA_ROOT, fp)
            candidates.append(abs_fp)

        candidates.append(os.path.join(settings.MEDIA_ROOT, 'uploads', video.filename))
        candidates.append(os.path.join(settings.MEDIA_ROOT, 'videos', video.filename))

        video_path = next((p for p in candidates if p and os.path.exists(p)), None)
        if not video_path:
            return Response({'error': 'ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤', 'candidates': candidates}, status=404)

        # 3) OpenCVë¡œ ì—´ê¸°
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return Response({'error': 'ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}, status=500)

        # 4) frame_number ë˜ëŠ” t(ì´ˆ) â†’ í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚°
        try:
            t = request.GET.get('t')
            if t is not None:
                fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
                frame_idx = int(float(t) * float(fps))
            else:
                frame_idx = int(frame_number)  # URLì—ì„œ intë¡œ ë°›ëŠ”ê²Œ ìµœì„  (urls.py ì°¸ê³ )
        except Exception:
            cap.release()
            return Response({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë ˆì„ ë²ˆí˜¸'}, status=400)

        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
        if total and frame_idx >= total:
            frame_idx = max(0, total - 1)
        if frame_idx < 0:
            frame_idx = 0

        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        cap.release()

        if not ret or frame is None:
            return Response({'error': f'í”„ë ˆì„({frame_idx})ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}, status=500)

        # 5) ì£¼ì„ ëª¨ë“œ(ì„ íƒ): í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ë§Œ ì‹œë„
        if frame_type == 'annotated' and hasattr(self, '_annotate_frame'):
            try:
                target_class = (request.GET.get('class') or '').lower()
                frame = self._annotate_frame(frame, video, frame_idx, target_class)
            except Exception:
                # ì£¼ì„ ì‹¤íŒ¨í•´ë„ ì›ë³¸ í”„ë ˆì„ì€ ë°˜í™˜
                pass

        # 6) ë¦¬ì‚¬ì´ì¦ˆ (ì˜µì…˜)
        h, w = frame.shape[:2]
        try:
            max_w = int(request.GET.get('max_w', 800))
        except Exception:
            max_w = 800

        if w > max_w:
            ratio = max_w / float(w)
            frame = cv2.resize(frame, (max_w, int(h * ratio)))

        # 7) JPEG ì¸ì½”ë”© í›„ ë©”ëª¨ë¦¬ë¡œ ì‘ë‹µ
        ok, buf = cv2.imencode('.jpg', frame, [int(cv2.IMWRITE_JPEG_QUALITY), 90])
        if not ok:
            return Response({'error': 'ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨'}, status=500)

        return HttpResponse(buf.tobytes(), content_type='image/jpeg')

class ScenesView(APIView):
    """Scene ëª©ë¡ ì¡°íšŒ"""
    permission_classes = [AllowAny]  # ğŸ”§ ê¶Œí•œ ì„¤ì • ì¶”ê°€
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            scene_list = []
            for scene in scenes:
                scene_data = {
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects,
                    'caption_type': 'enhanced' if scene.enhanced_captions_count > 0 else 'basic'
                }
                scene_list.append(scene_data)
            
            return Response({
                'scenes': scene_list,
                'total_scenes': len(scene_list),
                'analysis_type': 'enhanced' if hasattr(video, 'analysis') and video.analysis.enhanced_analysis else 'basic'
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        



import os
import json
import time
import cv2
import numpy as np
from django.conf import settings
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework.permissions import AllowAny
from datetime import datetime, timedelta
from collections import Counter
import threading
import queue

from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient


class AnalysisFeaturesView(APIView):
    """ë¶„ì„ ê¸°ëŠ¥ë³„ ìƒì„¸ ì •ë³´ ì œê³µ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            analyzer = VideoAnalyzer()
            
            features = {
                'object_detection': {
                    'name': 'ê°ì²´ ê°ì§€',
                    'description': 'YOLO ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì²´ ê°ì§€ ë° ë¶„ë¥˜',
                    'available': True,
                    'processing_time_factor': 1.0,
                    'icon': 'ğŸ¯',
                    'details': 'ë¹„ë””ì˜¤ ë‚´ ì‚¬ëŒ, ì°¨ëŸ‰, ë™ë¬¼ ë“± ë‹¤ì–‘í•œ ê°ì²´ë¥¼ ì •í™•í•˜ê²Œ ê°ì§€í•©ë‹ˆë‹¤.'
                },
                'clip_analysis': {
                    'name': 'CLIP ì”¬ ë¶„ì„',
                    'description': 'OpenAI CLIP ëª¨ë¸ì„ í™œìš©í•œ ê³ ê¸‰ ì”¬ ì´í•´',
                    'available': analyzer.clip_available,
                    'processing_time_factor': 1.5,
                    'icon': 'ğŸ–¼ï¸',
                    'details': 'ì´ë¯¸ì§€ì˜ ì˜ë¯¸ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ì—¬ ì”¬ ë¶„ë¥˜ ë° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'
                },
                'ocr': {
                    'name': 'OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ',
                    'description': 'EasyOCRì„ ì‚¬ìš©í•œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì‹',
                    'available': analyzer.ocr_available,
                    'processing_time_factor': 1.2,
                    'icon': 'ğŸ“',
                    'details': 'ë¹„ë””ì˜¤ ë‚´ í•œê¸€, ì˜ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.'
                },
                'vqa': {
                    'name': 'VQA ì§ˆë¬¸ë‹µë³€',
                    'description': 'BLIP ëª¨ë¸ ê¸°ë°˜ ì‹œê°ì  ì§ˆë¬¸ ë‹µë³€',
                    'available': analyzer.vqa_available,
                    'processing_time_factor': 2.0,
                    'icon': 'â“',
                    'details': 'ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ë‹µë³€í•˜ì—¬ ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.'
                },
                'scene_graph': {
                    'name': 'Scene Graph',
                    'description': 'ê°ì²´ê°„ ê´€ê³„ ë° ìƒí˜¸ì‘ìš© ë¶„ì„',
                    'available': analyzer.scene_graph_available,
                    'processing_time_factor': 3.0,
                    'icon': 'ğŸ•¸ï¸',
                    'details': 'ê°ì²´ë“¤ ì‚¬ì´ì˜ ê´€ê³„ì™€ ìƒí˜¸ì‘ìš©ì„ ë¶„ì„í•˜ì—¬ ë³µì¡í•œ ì”¬ì„ ì´í•´í•©ë‹ˆë‹¤.'
                },
                'enhanced_caption': {
                    'name': 'ê³ ê¸‰ ìº¡ì…˜ ìƒì„±',
                    'description': 'ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•œ ìƒì„¸ ìº¡ì…˜',
                    'available': True,
                    'processing_time_factor': 1.1,
                    'icon': 'ğŸ’¬',
                    'details': 'ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìƒì„¸í•˜ê³  ì •í™•í•œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.'
                }
            }
            
            return Response({
                'features': features,
                'device': analyzer.device,
                'total_available': sum(1 for f in features.values() if f['available']),
                'recommended_configs': {
                    'basic': ['object_detection', 'enhanced_caption'],
                    'enhanced': ['object_detection', 'clip_analysis', 'ocr', 'enhanced_caption'],
                    'comprehensive': list(features.keys())
                }
            })
            
        except Exception as e:
            return Response({
                'error': f'ë¶„ì„ ê¸°ëŠ¥ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AdvancedVideoSearchView(APIView):
    """ê³ ê¸‰ ë¹„ë””ì˜¤ ê²€ìƒ‰ API"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = VideoAnalyzer()
        self.llm_client = LLMClient()
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            query = request.data.get('query', '').strip()
            search_options = request.data.get('search_options', {})
            
            if not query:
                return Response({
                    'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            video = Video.objects.get(id=video_id)
            
            # ê³ ê¸‰ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.video_analyzer.search_comprehensive(video, query)
            
            # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ê°€ í¬í•¨ëœ í”„ë ˆì„ë“¤ì— ëŒ€í•´ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
            enhanced_results = []
            for result in search_results[:10]:
                frame_id = result.get('frame_id')
                try:
                    frame = Frame.objects.get(video=video, image_id=frame_id)
                    enhanced_result = dict(result)
                    
                    # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    comprehensive_features = frame.comprehensive_features or {}
                    
                    if search_options.get('include_clip_analysis') and 'clip_features' in comprehensive_features:
                        enhanced_result['clip_analysis'] = comprehensive_features['clip_features']
                    
                    if search_options.get('include_ocr_text') and 'ocr_text' in comprehensive_features:
                        enhanced_result['ocr_text'] = comprehensive_features['ocr_text']
                    
                    if search_options.get('include_vqa_results') and 'vqa_results' in comprehensive_features:
                        enhanced_result['vqa_insights'] = comprehensive_features['vqa_results']
                    
                    if search_options.get('include_scene_graph') and 'scene_graph' in comprehensive_features:
                        enhanced_result['scene_graph'] = comprehensive_features['scene_graph']
                    
                    enhanced_results.append(enhanced_result)
                    
                except Frame.DoesNotExist:
                    enhanced_results.append(result)
            
            # AI ê¸°ë°˜ ê²€ìƒ‰ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            search_insights = self._generate_search_insights(query, enhanced_results, video)
            
            return Response({
                'search_results': enhanced_results,
                'query': query,
                'insights': search_insights,
                'total_matches': len(search_results),
                'search_type': 'advanced',
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'analysis_type': getattr(video, 'analysis_type', 'basic')
                }
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _generate_search_insights(self, query, results, video):
        """ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            if not results:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            insights_prompt = f"""
            ê²€ìƒ‰ì–´: "{query}"
            ë¹„ë””ì˜¤: {video.original_name}
            ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë§¤ì¹­
            
            ì£¼ìš” ë°œê²¬ì‚¬í•­:
            {json.dumps(results[:3], ensure_ascii=False, indent=2)}
            
            ì´ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ê°„ë‹¨í•˜ê³  ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            
            insights = self.llm_client.generate_smart_response(
                user_query=insights_prompt,
                search_results=results[:5],
                video_info=f"ë¹„ë””ì˜¤: {video.original_name}",
                use_multi_llm=False
            )
            
            return insights
            
        except Exception as e:
            return f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


class EnhancedFrameView(APIView):
    """ê³ ê¸‰ ë¶„ì„ ì •ë³´ê°€ í¬í•¨ëœ í”„ë ˆì„ ë°ì´í„° ì œê³µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id, frame_number):
        try:
            video = Video.objects.get(id=video_id)
            
            # í”„ë ˆì„ ë°ì´í„° ì¡°íšŒ
            try:
                frame = Frame.objects.get(video=video, image_id=frame_number)
                
                frame_data = {
                    'frame_id': frame.image_id,
                    'timestamp': frame.timestamp,
                    'caption': frame.caption,
                    'enhanced_caption': frame.enhanced_caption,
                    'final_caption': frame.final_caption,
                    'detected_objects': frame.detected_objects,
                    'comprehensive_features': frame.comprehensive_features,
                    'analysis_quality': frame.comprehensive_features.get('caption_quality', 'basic')
                }
                
                # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ ë¶„í•´
                if frame.comprehensive_features:
                    features = frame.comprehensive_features
                    
                    frame_data['advanced_analysis'] = {
                        'clip_analysis': features.get('clip_features', {}),
                        'ocr_text': features.get('ocr_text', {}),
                        'vqa_results': features.get('vqa_results', {}),
                        'scene_graph': features.get('scene_graph', {}),
                        'scene_complexity': features.get('scene_complexity', 0)
                    }
                
                return Response(frame_data)
                
            except Frame.DoesNotExist:
                # í”„ë ˆì„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¯¸ì§€ë§Œ ë°˜í™˜
                return Response({
                    'frame_id': frame_number,
                    'message': 'í”„ë ˆì„ ë°ì´í„°ëŠ” ì—†ì§€ë§Œ ì´ë¯¸ì§€ëŠ” ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.',
                    'image_url': f'/frame/{video_id}/{frame_number}/'
                })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'í”„ë ˆì„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class EnhancedScenesView(APIView):
    """ê³ ê¸‰ ë¶„ì„ ì •ë³´ê°€ í¬í•¨ëœ ì”¬ ë°ì´í„° ì œê³µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            enhanced_scenes = []
            for scene in scenes:
                scene_data = {
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects,
                    'enhanced_captions_count': scene.enhanced_captions_count,
                    'caption_type': 'enhanced' if scene.enhanced_captions_count > 0 else 'basic'
                }
                
                # ì”¬ ë‚´ í”„ë ˆì„ë“¤ì˜ ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ ì§‘ê³„
                scene_frames = Frame.objects.filter(
                    video=video,
                    timestamp__gte=scene.start_time,
                    timestamp__lte=scene.end_time
                )
                
                if scene_frames.exists():
                    # ê³ ê¸‰ ê¸°ëŠ¥ ì‚¬ìš© í†µê³„
                    clip_count = sum(1 for f in scene_frames if f.comprehensive_features.get('clip_features'))
                    ocr_count = sum(1 for f in scene_frames if f.comprehensive_features.get('ocr_text', {}).get('texts'))
                    vqa_count = sum(1 for f in scene_frames if f.comprehensive_features.get('vqa_results'))
                    
                    scene_data['advanced_features'] = {
                        'clip_analysis_frames': clip_count,
                        'ocr_text_frames': ocr_count,
                        'vqa_analysis_frames': vqa_count,
                        'total_frames': scene_frames.count()
                    }
                    
                    # ì”¬ ë³µì¡ë„ í‰ê· 
                    complexities = [f.comprehensive_features.get('scene_complexity', 0) for f in scene_frames]
                    scene_data['average_complexity'] = sum(complexities) / len(complexities) if complexities else 0
                
                enhanced_scenes.append(scene_data)
            
            return Response({
                'scenes': enhanced_scenes,
                'total_scenes': len(enhanced_scenes),
                'analysis_type': 'enhanced' if any(s.get('advanced_features') for s in enhanced_scenes) else 'basic',
                'video_info': {
                    'id': video.id,
                    'name': video.original_name
                }
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ê³ ê¸‰ ì”¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisResultsView(APIView):
    """ì¢…í•© ë¶„ì„ ê²°ê³¼ ì œê³µ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ì•„ì§ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            analysis = video.analysis
            scenes = Scene.objects.filter(video=video)
            frames = Frame.objects.filter(video=video)
            
            # ì¢…í•© ë¶„ì„ ê²°ê³¼
            results = {
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'duration': video.duration,
                    'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                    'processing_time': analysis.processing_time_seconds,
                    'success_rate': analysis.success_rate
                },
                'analysis_summary': {
                    'total_scenes': scenes.count(),
                    'total_frames_analyzed': frames.count(),
                    'unique_objects': analysis.analysis_statistics.get('unique_objects', 0),
                    'features_used': analysis.analysis_statistics.get('features_used', []),
                    'scene_types': analysis.analysis_statistics.get('scene_types', [])
                },
                'advanced_features': {
                    'clip_analysis': analysis.analysis_statistics.get('clip_analysis', False),
                    'ocr_text_extracted': analysis.analysis_statistics.get('text_extracted', False),
                    'vqa_analysis': analysis.analysis_statistics.get('vqa_analysis', False),
                    'scene_graph_analysis': analysis.analysis_statistics.get('scene_graph_analysis', False)
                },
                'content_insights': {
                    'dominant_objects': analysis.analysis_statistics.get('dominant_objects', []),
                    'text_content_length': analysis.caption_statistics.get('text_content_length', 0),
                    'enhanced_captions_count': analysis.caption_statistics.get('enhanced_captions', 0),
                    'average_confidence': analysis.caption_statistics.get('average_confidence', 0)
                }
            }
            
            # í”„ë ˆì„ë³„ ê³ ê¸‰ ë¶„ì„ í†µê³„
            if frames.exists():
                clip_frames = sum(1 for f in frames if f.comprehensive_features.get('clip_features'))
                ocr_frames = sum(1 for f in frames if f.comprehensive_features.get('ocr_text', {}).get('texts'))
                vqa_frames = sum(1 for f in frames if f.comprehensive_features.get('vqa_results'))
                
                results['frame_statistics'] = {
                    'total_frames': frames.count(),
                    'clip_analyzed_frames': clip_frames,
                    'ocr_processed_frames': ocr_frames,
                    'vqa_analyzed_frames': vqa_frames,
                    'coverage': {
                        'clip': (clip_frames / frames.count()) * 100 if frames.count() > 0 else 0,
                        'ocr': (ocr_frames / frames.count()) * 100 if frames.count() > 0 else 0,
                        'vqa': (vqa_frames / frames.count()) * 100 if frames.count() > 0 else 0
                    }
                }
            
            return Response(results)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisSummaryView(APIView):
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì œê³µ"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.llm_client = LLMClient()
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ì•„ì§ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ë¶„ì„ ê²°ê³¼ ë°ì´í„° ìˆ˜ì§‘
            analysis = video.analysis
            frames = Frame.objects.filter(video=video)[:10]  # ìƒìœ„ 10ê°œ í”„ë ˆì„
            
            # AI ê¸°ë°˜ ìš”ì•½ ìƒì„±
            summary_data = {
                'video_name': video.original_name,
                'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                'features_used': analysis.analysis_statistics.get('features_used', []),
                'dominant_objects': analysis.analysis_statistics.get('dominant_objects', []),
                'scene_types': analysis.analysis_statistics.get('scene_types', []),
                'processing_time': analysis.processing_time_seconds
            }
            
            # ëŒ€í‘œ í”„ë ˆì„ë“¤ì˜ ìº¡ì…˜ ìˆ˜ì§‘
            sample_captions = []
            for frame in frames:
                if frame.final_caption:
                    sample_captions.append(frame.final_caption)
            
            summary_prompt = f"""
            ë‹¤ìŒ ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  ìœ ìš©í•œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
            
            ë¹„ë””ì˜¤: {video.original_name}
            ë¶„ì„ ìœ í˜•: {summary_data['analysis_type']}
            ì‚¬ìš©ëœ ê¸°ëŠ¥: {', '.join(summary_data['features_used'])}
            ì£¼ìš” ê°ì²´: {', '.join(summary_data['dominant_objects'][:5])}
            ì”¬ ìœ í˜•: {', '.join(summary_data['scene_types'][:3])}
            
            ëŒ€í‘œ ìº¡ì…˜ë“¤:
            {chr(10).join(sample_captions[:5])}
            
            ì´ ë¹„ë””ì˜¤ì˜ ì£¼ìš” ë‚´ìš©, íŠ¹ì§•, í™œìš© ë°©ì•ˆì„ í¬í•¨í•˜ì—¬ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
            """
            
            ai_summary = self.llm_client.generate_smart_response(
                user_query=summary_prompt,
                search_results=None,
                video_info=f"ë¹„ë””ì˜¤: {video.original_name}",
                use_multi_llm=True  # ê³ í’ˆì§ˆ ìš”ì•½ì„ ìœ„í•´ ë‹¤ì¤‘ LLM ì‚¬ìš©
            )
            
            return Response({
                'video_id': video.id,
                'video_name': video.original_name,
                'ai_summary': ai_summary,
                'analysis_data': summary_data,
                'key_insights': {
                    'total_objects': len(summary_data['dominant_objects']),
                    'scene_variety': len(summary_data['scene_types']),
                    'analysis_depth': len(summary_data['features_used']),
                    'processing_efficiency': f"{summary_data['processing_time']}ì´ˆ"
                },
                'generated_at': datetime.now().isoformat()
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class AnalysisExportView(APIView):
    """ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ì•„ì§ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            export_format = request.GET.get('format', 'json')
            
            # ì „ì²´ ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
            analysis = video.analysis
            scenes = Scene.objects.filter(video=video)
            frames = Frame.objects.filter(video=video)
            
            export_data = {
                'export_info': {
                    'video_id': video.id,
                    'video_name': video.original_name,
                    'export_date': datetime.now().isoformat(),
                    'export_format': export_format
                },
                'video_metadata': {
                    'filename': video.filename,
                    'duration': video.duration,
                    'file_size': video.file_size,
                    'uploaded_at': video.uploaded_at.isoformat()
                },
                'analysis_metadata': {
                    'analysis_type': analysis.analysis_statistics.get('analysis_type', 'basic'),
                    'enhanced_analysis': analysis.enhanced_analysis,
                    'success_rate': analysis.success_rate,
                    'processing_time_seconds': analysis.processing_time_seconds,
                    'features_used': analysis.analysis_statistics.get('features_used', [])
                },
                'scenes': [
                    {
                        'scene_id': scene.scene_id,
                        'start_time': scene.start_time,
                        'end_time': scene.end_time,
                        'duration': scene.duration,
                        'frame_count': scene.frame_count,
                        'dominant_objects': scene.dominant_objects
                    }
                    for scene in scenes
                ],
                'frames': [
                    {
                        'frame_id': frame.image_id,
                        'timestamp': frame.timestamp,
                        'caption': frame.caption,
                        'enhanced_caption': frame.enhanced_caption,
                        'final_caption': frame.final_caption,
                        'detected_objects': frame.detected_objects,
                        'comprehensive_features': frame.comprehensive_features
                    }
                    for frame in frames
                ],
                'statistics': {
                    'total_scenes': scenes.count(),
                    'total_frames': frames.count(),
                    'unique_objects': analysis.analysis_statistics.get('unique_objects', 0),
                    'scene_types': analysis.analysis_statistics.get('scene_types', []),
                    'dominant_objects': analysis.analysis_statistics.get('dominant_objects', [])
                }
            }
            
            if export_format == 'json':
                response = JsonResponse(export_data, json_dumps_params={'ensure_ascii': False, 'indent': 2})
                response['Content-Disposition'] = f'attachment; filename="{video.original_name}_analysis.json"'
                return response
            
            elif export_format == 'csv':
                # CSV í˜•íƒœë¡œ í”„ë ˆì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
                import csv
                from io import StringIO
                
                output = StringIO()
                writer = csv.writer(output)
                
                # í—¤ë”
                writer.writerow(['frame_id', 'timestamp', 'caption', 'enhanced_caption', 'objects_count', 'scene_complexity'])
                
                # ë°ì´í„°
                for frame_data in export_data['frames']:
                    writer.writerow([
                        frame_data['frame_id'],
                        frame_data['timestamp'],
                        frame_data.get('caption', ''),
                        frame_data.get('enhanced_caption', ''),
                        len(frame_data.get('detected_objects', [])),
                        frame_data.get('comprehensive_features', {}).get('scene_complexity', 0)
                    ])
                
                response = HttpResponse(output.getvalue(), content_type='text/csv')
                response['Content-Disposition'] = f'attachment; filename="{video.original_name}_analysis.csv"'
                return response
            
            else:
                return Response({
                    'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‚´ë³´ë‚´ê¸° í˜•ì‹ì…ë‹ˆë‹¤. json ë˜ëŠ” csvë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ê²€ìƒ‰ ê´€ë ¨ ë·°ë“¤
class ObjectSearchView(APIView):
    """ê°ì²´ë³„ ê²€ìƒ‰"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            object_type = request.GET.get('object', '')
            video_id = request.GET.get('video_id')
            
            if not object_type:
                return Response({
                    'error': 'ê²€ìƒ‰í•  ê°ì²´ íƒ€ì…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # íŠ¹ì • ë¹„ë””ì˜¤ ë˜ëŠ” ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ ê²€ìƒ‰
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                frames = Frame.objects.filter(video=video)
                
                for frame in frames:
                    for obj in frame.detected_objects:
                        if object_type.lower() in obj.get('class', '').lower():
                            results.append({
                                'video_id': video.id,
                                'video_name': video.original_name,
                                'frame_id': frame.image_id,
                                'timestamp': frame.timestamp,
                                'object_class': obj.get('class'),
                                'confidence': obj.get('confidence'),
                                'caption': frame.final_caption or frame.caption
                            })
            
            return Response({
                'search_query': object_type,
                'results': results[:50],  # ìµœëŒ€ 50ê°œ ê²°ê³¼
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'ê°ì²´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TextSearchView(APIView):
    """í…ìŠ¤íŠ¸ ê²€ìƒ‰ (OCR ê²°ê³¼ ê¸°ë°˜)"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            search_text = request.GET.get('text', '')
            video_id = request.GET.get('video_id')
            
            if not search_text:
                return Response({
                    'error': 'ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # íŠ¹ì • ë¹„ë””ì˜¤ ë˜ëŠ” ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ ê²€ìƒ‰
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                frames = Frame.objects.filter(video=video)
                
                for frame in frames:
                    ocr_data = frame.comprehensive_features.get('ocr_text', {})
                    if 'full_text' in ocr_data and search_text.lower() in ocr_data['full_text'].lower():
                        results.append({
                            'video_id': video.id,
                            'video_name': video.original_name,
                            'frame_id': frame.image_id,
                            'timestamp': frame.timestamp,
                            'extracted_text': ocr_data['full_text'],
                            'text_details': ocr_data.get('texts', []),
                            'caption': frame.final_caption or frame.caption
                        })
            
            return Response({
                'search_query': search_text,
                'results': results[:50],
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class SceneSearchView(APIView):
    """ì”¬ íƒ€ì…ë³„ ê²€ìƒ‰"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            scene_type = request.GET.get('scene', '')
            video_id = request.GET.get('video_id')
            
            if not scene_type:
                return Response({
                    'error': 'ê²€ìƒ‰í•  ì”¬ íƒ€ì…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # íŠ¹ì • ë¹„ë””ì˜¤ ë˜ëŠ” ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ ê²€ìƒ‰
            if video_id:
                videos = Video.objects.filter(id=video_id, is_analyzed=True)
            else:
                videos = Video.objects.filter(is_analyzed=True)
            
            results = []
            for video in videos:
                if hasattr(video, 'analysis'):
                    scene_types = video.analysis.analysis_statistics.get('scene_types', [])
                    if any(scene_type.lower() in st.lower() for st in scene_types):
                        results.append({
                            'video_id': video.id,
                            'video_name': video.original_name,
                            'scene_types': scene_types,
                            'analysis_type': video.analysis.analysis_statistics.get('analysis_type', 'basic'),
                            'dominant_objects': video.analysis.analysis_statistics.get('dominant_objects', [])
                        })
            
            return Response({
                'search_query': scene_type,
                'results': results,
                'total_matches': len(results)
            })
            
        except Exception as e:
            return Response({
                'error': f'ì”¬ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.shortcuts import get_object_or_404
from django.db import transaction
import json
import logging
import os
import time

logger = logging.getLogger(__name__)

@csrf_exempt
@require_http_methods(["DELETE"])
def delete_video(request, video_id):
    """ê°œì„ ëœ ë¹„ë””ì˜¤ ì‚­ì œ - ìƒì„¸ ë¡œê¹… ë° ê²€ì¦ í¬í•¨"""
    
    logger.info(f"ğŸ—‘ï¸ ë¹„ë””ì˜¤ ì‚­ì œ ìš”ì²­ ì‹œì‘: ID={video_id}")
    
    try:
        # 1ë‹¨ê³„: ë¹„ë””ì˜¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            video = get_object_or_404(Video, id=video_id)
            logger.info(f"âœ… ë¹„ë””ì˜¤ ì°¾ìŒ: {video.original_name} (íŒŒì¼: {video.file_path})")
        except Video.DoesNotExist:
            logger.warning(f"âŒ ë¹„ë””ì˜¤ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: ID={video_id}")
            return JsonResponse({
                'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'video_id': video_id,
                'deleted': False
            }, status=404)
        
        # 2ë‹¨ê³„: ì‚­ì œ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if video.analysis_status == 'processing':
            logger.warning(f"âŒ ë¶„ì„ ì¤‘ì¸ ë¹„ë””ì˜¤ ì‚­ì œ ì‹œë„: ID={video_id}")
            return JsonResponse({
                'error': 'ë¶„ì„ ì¤‘ì¸ ë¹„ë””ì˜¤ëŠ” ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'video_id': video_id,
                'status': video.analysis_status,
                'deleted': False
            }, status=400)
        
        # 3ë‹¨ê³„: íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì•ˆì „í•œ ì‚­ì œ ì²˜ë¦¬
        video_info = {
            'id': video_id,
            'name': video.original_name,
            'file_path': video.file_path,
            'has_analysis': hasattr(video, 'analysis_results') and video.analysis_results.exists(),
            'has_scenes': hasattr(video, 'scenes') and video.scenes.exists()
        }
        
        with transaction.atomic():
            logger.info(f"ğŸ”„ íŠ¸ëœì­ì…˜ ì‹œì‘: ë¹„ë””ì˜¤ {video_id} ì‚­ì œ")
            
            # ê´€ë ¨ ë°ì´í„° ë¨¼ì € ì‚­ì œ
            deleted_analysis_count = 0
            deleted_scenes_count = 0
            
            if hasattr(video, 'analysis_results'):
                deleted_analysis_count = video.analysis_results.count()
                video.analysis_results.all().delete()
                logger.info(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ ì‚­ì œ: {deleted_analysis_count}ê°œ")
            
            if hasattr(video, 'scenes'):
                deleted_scenes_count = video.scenes.count()
                video.scenes.all().delete()
                logger.info(f"ğŸ¬ ì”¬ ë°ì´í„° ì‚­ì œ: {deleted_scenes_count}ê°œ")
            
            # íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ íŒŒì¼ ì‚­ì œ
            file_deleted = False
            if video.file_path and os.path.exists(video.file_path):
                try:
                    os.remove(video.file_path)
                    file_deleted = True
                    logger.info(f"ğŸ“ íŒŒì¼ ì‚­ì œ ì„±ê³µ: {video.file_path}")
                except Exception as file_error:
                    logger.error(f"âŒ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {video.file_path} - {str(file_error)}")
                    # íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨í•´ë„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œëŠ” ì‚­ì œ ì§„í–‰
                    file_deleted = False
            else:
                logger.info(f"ğŸ“ ì‚­ì œí•  íŒŒì¼ ì—†ìŒ: {video.file_path}")
                file_deleted = True  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì‚­ì œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¹„ë””ì˜¤ ë ˆì½”ë“œ ì‚­ì œ
            video.delete()
            logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¹„ë””ì˜¤ ì‚­ì œ ì™„ë£Œ: ID={video_id}")
            
            # íŠ¸ëœì­ì…˜ ì»¤ë°‹ í›„ ì ì‹œ ëŒ€ê¸° (ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™”)
            time.sleep(0.1)
        
        # 4ë‹¨ê³„: ì‚­ì œ ê²€ì¦
        try:
            verification_video = Video.objects.get(id=video_id)
            # ë¹„ë””ì˜¤ê°€ ì—¬ì „íˆ ì¡´ì¬í•˜ë©´ ì˜¤ë¥˜
            logger.error(f"âŒ ì‚­ì œ ê²€ì¦ ì‹¤íŒ¨: ë¹„ë””ì˜¤ê°€ ì—¬ì „íˆ ì¡´ì¬í•¨ ID={video_id}")
            return JsonResponse({
                'error': 'ë¹„ë””ì˜¤ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œê±°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'video_id': video_id,
                'deleted': False,
                'verification_failed': True
            }, status=500)
        except Video.DoesNotExist:
            # ë¹„ë””ì˜¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì‚­ì œ ì„±ê³µ
            logger.info(f"âœ… ì‚­ì œ ê²€ì¦ ì„±ê³µ: ë¹„ë””ì˜¤ê°€ ì™„ì „íˆ ì œê±°ë¨ ID={video_id}")
        
        # 5ë‹¨ê³„: ì„±ê³µ ì‘ë‹µ
        response_data = {
            'success': True,
            'message': f'ë¹„ë””ì˜¤ "{video_info["name"]}"ì´(ê°€) ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'video_id': video_id,
            'deleted': True,
            'details': {
                'file_deleted': file_deleted,
                'analysis_results_deleted': deleted_analysis_count,
                'scenes_deleted': deleted_scenes_count,
                'file_path': video_info['file_path']
            }
        }
        
        logger.info(f"âœ… ë¹„ë””ì˜¤ ì‚­ì œ ì™„ë£Œ: {json.dumps(response_data, ensure_ascii=False)}")
        return JsonResponse(response_data)
        
    except Exception as e:
        logger.error(f"âŒ ë¹„ë””ì˜¤ ì‚­ì œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: ID={video_id}, ì˜¤ë¥˜={str(e)}")
        return JsonResponse({
            'error': f'ë¹„ë””ì˜¤ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
            'video_id': video_id,
            'deleted': False,
            'exception': str(e)
        }, status=500)

@csrf_exempt
@require_http_methods(["GET"])  
def video_detail(request, video_id):
    """ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì¡´ì¬ ì—¬ë¶€ í™•ì¸ìš©)"""
    try:
        video = get_object_or_404(Video, id=video_id)
        return JsonResponse({
            'id': video.id,
            'original_name': video.original_name,
            'analysis_status': video.analysis_status,
            'exists': True
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
            'video_id': video_id,
            'exists': False
        }, status=404)

# ì‚­ì œ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ ë³„ë„ ì—”ë“œí¬ì¸íŠ¸
@csrf_exempt
@require_http_methods(["GET"])
def check_video_exists(request, video_id):
    """ë¹„ë””ì˜¤ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸"""
    try:
        Video.objects.get(id=video_id)
        return JsonResponse({
            'exists': True,
            'video_id': video_id
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'exists': False,
            'video_id': video_id
        })

# views.pyì— ì¶”ê°€í•  ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° View í´ë˜ìŠ¤ë“¤

class AdvancedVideoSearchView(APIView):
    """ê³ ê¸‰ ë¹„ë””ì˜¤ ê²€ìƒ‰ View - ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ í¬í•¨"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = get_video_analyzer()
        self.llm_client = LLMClient()
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            query = request.data.get('query', '').strip()
            search_options = request.data.get('search_options', {})
            
            print(f"ğŸ” ê³ ê¸‰ ë¹„ë””ì˜¤ ê²€ìƒ‰: ë¹„ë””ì˜¤={video_id}, ì¿¼ë¦¬='{query}'")
            
            if not query:
                return Response({
                    'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({
                    'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # ê³ ê¸‰ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self._perform_advanced_search(video, query, search_options)
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì¶”ê°€
            enhanced_results = self._add_bbox_info(search_results, video)
            
            # AI ê¸°ë°˜ ê²€ìƒ‰ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            search_insights = self._generate_search_insights(query, enhanced_results, video)
            
            print(f"âœ… ê³ ê¸‰ ê²€ìƒ‰ ì™„ë£Œ: {len(enhanced_results)}ê°œ ê²°ê³¼")
            
            return Response({
                'search_results': enhanced_results,
                'query': query,
                'insights': search_insights,
                'total_matches': len(search_results),
                'search_type': 'advanced_search',
                'has_bbox_annotations': any(r.get('bbox_annotations') for r in enhanced_results),
                'video_info': {
                    'id': video.id,
                    'name': video.original_name,
                    'analysis_type': getattr(video, 'analysis_type', 'basic')
                }
            })
            
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return Response({
                'error': f'ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _perform_advanced_search(self, video, query, search_options):
        """ì‹¤ì œ ê³ ê¸‰ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            # EnhancedVideoChatViewì˜ ê²€ìƒ‰ ë¡œì§ ì¬ì‚¬ìš©
            chat_view = EnhancedVideoChatView()
            video_info = chat_view._get_enhanced_video_info(video)
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            response = chat_view._handle_enhanced_search(query, video, video_info)
            
            if hasattr(response, 'data') and 'search_results' in response.data:
                return response.data['search_results']
            else:
                return []
                
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ ê²€ìƒ‰ ìˆ˜í–‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _add_bbox_info(self, search_results, video):
        """ê²€ìƒ‰ ê²°ê³¼ì— ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì¶”ê°€"""
        enhanced_results = []
        
        for result in search_results:
            enhanced_result = dict(result)
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ì •ë³´ í™•ì¸ ë° ì¶”ê°€
            if 'matches' in result:
                bbox_annotations = []
                for match in result['matches']:
                    if match.get('type') == 'object' and 'bbox' in match:
                        bbox_annotations.append({
                            'match': match['match'],
                            'confidence': match['confidence'],
                            'bbox': match['bbox'],
                            'colors': match.get('colors', []),
                            'color_description': match.get('color_description', '')
                        })
                
                enhanced_result['bbox_annotations'] = bbox_annotations
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ì´ë¯¸ì§€ URL ì¶”ê°€
                if bbox_annotations:
                    bbox_url = f"/frame/{video.id}/{result['frame_id']}/bbox/"
                    enhanced_result['bbox_image_url'] = bbox_url
            
            enhanced_results.append(enhanced_result)
        
        return enhanced_results
    
    def _generate_search_insights(self, query, results, video):
        """ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            if not results:
                return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
            
            bbox_count = sum(1 for r in results if r.get('bbox_annotations'))
            total_objects = sum(len(r.get('bbox_annotations', [])) for r in results)
            
            insights_prompt = f"""
            ê²€ìƒ‰ì–´: "{query}"
            ë¹„ë””ì˜¤: {video.original_name}
            ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë§¤ì¹­
            ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œ ê°€ëŠ¥: {bbox_count}ê°œ í”„ë ˆì„
            ì´ ê°ì§€ëœ ê°ì²´: {total_objects}ê°œ
            
            ì£¼ìš” ë°œê²¬ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
            ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œ ê¸°ëŠ¥ì— ëŒ€í•œ ì•ˆë‚´ë„ í¬í•¨í•´ì£¼ì„¸ìš”.
            """
            
            insights = self.llm_client.generate_smart_response(
                user_query=insights_prompt,
                search_results=results[:3],
                video_info=f"ë¹„ë””ì˜¤: {video.original_name}",
                use_multi_llm=False
            )
            
            return insights
            
        except Exception as e:
            return f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# ê¸°ì¡´ FrameView í´ë˜ìŠ¤ì— ë°”ìš´ë”© ë°•ìŠ¤ ì˜µì…˜ ì¶”ê°€
class EnhancedFrameView(FrameView):
    """ê¸°ì¡´ FrameViewë¥¼ í™•ì¥í•œ ê³ ê¸‰ í”„ë ˆì„ View"""
    
    def get(self, request, video_id, frame_number):
        try:
            # ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œ ì˜µì…˜ í™•ì¸
            show_bbox = request.GET.get('bbox', '').lower() in ['true', '1', 'yes']
            
            if show_bbox:
                # ë°”ìš´ë”© ë°•ìŠ¤ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ ë°˜í™˜
                bbox_view = FrameWithBboxView()
                return bbox_view.get(request, video_id, frame_number)
            else:
                # ê¸°ë³¸ í”„ë ˆì„ ë°˜í™˜
                return super().get(request, video_id, frame_number)
                
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ í”„ë ˆì„ ë·° ì˜¤ë¥˜: {e}")
            return super().get(request, video_id, frame_number)

# chat/views.pyì— ë‹¤ìŒ í´ë˜ìŠ¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”

class AnalysisCapabilitiesView(APIView):
    """ì‹œìŠ¤í…œ ë¶„ì„ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            print("ğŸ” AnalysisCapabilitiesView: ë¶„ì„ ê¸°ëŠ¥ ìƒíƒœ ìš”ì²­")
            
            # VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            try:
                analyzer = get_video_analyzer()
                analyzer_available = True
                print("âœ… VideoAnalyzer ì¸ìŠ¤í„´ìŠ¤ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ VideoAnalyzer ë¡œë”© ì‹¤íŒ¨: {e}")
                analyzer = None
                analyzer_available = False
            
            # ì‹œìŠ¤í…œ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸
            capabilities = {
                'system_status': {
                    'analyzer_available': analyzer_available,
                    'device': getattr(analyzer, 'device', 'unknown') if analyzer else 'none',
                    'timestamp': datetime.now().isoformat()
                },
                'core_features': {
                    'object_detection': {
                        'name': 'ê°ì²´ ê°ì§€',
                        'available': analyzer.model is not None if analyzer else False,
                        'description': 'YOLO ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì²´ ê°ì§€',
                        'icon': 'ğŸ¯'
                    },
                    'enhanced_captions': {
                        'name': 'ê³ ê¸‰ ìº¡ì…˜ ìƒì„±',
                        'available': True,
                        'description': 'AI ê¸°ë°˜ ìƒì„¸ ìº¡ì…˜ ìƒì„±',
                        'icon': 'ğŸ’¬'
                    }
                },
                'advanced_features': {
                    'clip_analysis': {
                        'name': 'CLIP ë¶„ì„',
                        'available': getattr(analyzer, 'clip_available', False) if analyzer else False,
                        'description': 'OpenAI CLIP ëª¨ë¸ ê¸°ë°˜ ì”¬ ì´í•´',
                        'icon': 'ğŸ–¼ï¸'
                    },
                    'ocr_text_extraction': {
                        'name': 'OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ',
                        'available': getattr(analyzer, 'ocr_available', False) if analyzer else False,  
                        'description': 'EasyOCR ê¸°ë°˜ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì‹',
                        'icon': 'ğŸ“'
                    },
                    'vqa_analysis': {
                        'name': 'VQA ì§ˆë¬¸ë‹µë³€',
                        'available': getattr(analyzer, 'vqa_available', False) if analyzer else False,
                        'description': 'BLIP ëª¨ë¸ ê¸°ë°˜ ì‹œê°ì  ì§ˆë¬¸ ë‹µë³€',
                        'icon': 'â“'
                    },
                    'scene_graph': {
                        'name': 'Scene Graph',
                        'available': getattr(analyzer, 'scene_graph_available', False) if analyzer else False,
                        'description': 'NetworkX ê¸°ë°˜ ê°ì²´ ê´€ê³„ ë¶„ì„',
                        'icon': 'ğŸ•¸ï¸'
                    }
                },
                'api_status': {
                    'groq_available': True,  # LLMClientì—ì„œ í™•ì¸ í•„ìš”
                    'openai_available': True,
                    'anthropic_available': True
                }
            }
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ ìˆ˜ ê³„ì‚°
            total_features = len(capabilities['core_features']) + len(capabilities['advanced_features'])
            available_features = sum(1 for features in [capabilities['core_features'], capabilities['advanced_features']] 
                                   for feature in features.values() if feature.get('available', False))
            
            capabilities['summary'] = {
                'total_features': total_features,
                'available_features': available_features,
                'availability_rate': (available_features / total_features * 100) if total_features > 0 else 0,
                'system_ready': analyzer_available and available_features > 0
            }
            
            print(f"âœ… ë¶„ì„ ê¸°ëŠ¥ ìƒíƒœ: {available_features}/{total_features} ì‚¬ìš© ê°€ëŠ¥")
            
            return Response(capabilities)
            
        except Exception as e:
            print(f"âŒ AnalysisCapabilitiesView ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ ìƒíƒœ ë°˜í™˜
            error_response = {
                'system_status': {
                    'analyzer_available': False,
                    'device': 'error',
                    'error': str(e)
                },
                'core_features': {},
                'advanced_features': {},
                'api_status': {},
                'summary': {
                    'total_features': 0,
                    'available_features': 0,
                    'availability_rate': 0,
                    'system_ready': False,
                    'error': str(e)
                }
            }
            
            return Response(error_response, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# views.pyì— ì¶”ê°€í•  ê³ ê¸‰ ê²€ìƒ‰ API í´ë˜ìŠ¤ë“¤

class CrossVideoSearchView(APIView):
    """ì˜ìƒ ê°„ ê²€ìƒ‰ - ì—¬ëŸ¬ ë¹„ë””ì˜¤ì—ì„œ ì¡°ê±´ ê²€ìƒ‰"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            search_filters = request.data.get('filters', {})
            
            if not query:
                return Response({'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}, status=400)
            
            # ì¿¼ë¦¬ ë¶„ì„ - ë‚ ì”¨, ì‹œê°„ëŒ€, ì¥ì†Œ ë“± ì¶”ì¶œ
            query_analysis = self._analyze_query(query)
            
            # ë¶„ì„ëœ ë¹„ë””ì˜¤ë“¤ ì¤‘ì—ì„œ ê²€ìƒ‰
            videos = Video.objects.filter(is_analyzed=True)
            matching_videos = []
            
            for video in videos:
                match_score = self._calculate_video_match_score(video, query_analysis, search_filters)
                if match_score > 0.3:  # ì„ê³„ê°’
                    matching_videos.append({
                        'video_id': video.id,
                        'video_name': video.original_name,
                        'match_score': match_score,
                        'match_reasons': self._get_match_reasons(video, query_analysis),
                        'metadata': self._get_video_metadata(video),
                        'thumbnail_url': f'/api/frame/{video.id}/100/',
                    })
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            matching_videos.sort(key=lambda x: x['match_score'], reverse=True)
            
            return Response({
                'query': query,
                'total_matches': len(matching_videos),
                'results': matching_videos[:20],  # ìƒìœ„ 20ê°œ
                'query_analysis': query_analysis,
                'search_type': 'cross_video'
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _analyze_query(self, query):
        """ì¿¼ë¦¬ì—ì„œ ë‚ ì”¨, ì‹œê°„ëŒ€, ì¥ì†Œ ë“± ì¶”ì¶œ"""
        analysis = {
            'weather': None,
            'time_of_day': None,
            'location': None,
            'objects': [],
            'activities': []
        }
        
        query_lower = query.lower()
        
        # ë‚ ì”¨ í‚¤ì›Œë“œ
        weather_keywords = {
            'ë¹„': 'rainy', 'ë¹„ê°€': 'rainy', 'ìš°ì²œ': 'rainy',
            'ë§‘ì€': 'sunny', 'í™”ì°½í•œ': 'sunny', 'í–‡ë¹›': 'sunny',
            'íë¦°': 'cloudy', 'êµ¬ë¦„': 'cloudy'
        }
        
        # ì‹œê°„ëŒ€ í‚¤ì›Œë“œ
        time_keywords = {
            'ë°¤': 'night', 'ì•¼ê°„': 'night', 'ì €ë…': 'evening',
            'ë‚®': 'day', 'ì˜¤í›„': 'afternoon', 'ì•„ì¹¨': 'morning'
        }
        
        # ì¥ì†Œ í‚¤ì›Œë“œ
        location_keywords = {
            'ì‹¤ë‚´': 'indoor', 'ê±´ë¬¼': 'indoor', 'ë°©': 'indoor',
            'ì‹¤ì™¸': 'outdoor', 'ë„ë¡œ': 'outdoor', 'ê±°ë¦¬': 'outdoor'
        }
        
        for keyword, value in weather_keywords.items():
            if keyword in query_lower:
                analysis['weather'] = value
                break
        
        for keyword, value in time_keywords.items():
            if keyword in query_lower:
                analysis['time_of_day'] = value
                break
                
        for keyword, value in location_keywords.items():
            if keyword in query_lower:
                analysis['location'] = value
                break
        
        return analysis
    
    def _calculate_video_match_score(self, video, query_analysis, filters):
        """ë¹„ë””ì˜¤ì™€ ì¿¼ë¦¬ ê°„ì˜ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        try:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
            if hasattr(video, 'analysis'):
                stats = video.analysis.analysis_statistics
                scene_types = stats.get('scene_types', [])
                
                # ë‚ ì”¨ ë§¤ì¹­
                if query_analysis['weather']:
                    weather_scenes = [s for s in scene_types if query_analysis['weather'] in s.lower()]
                    if weather_scenes:
                        score += 0.4
                
                # ì‹œê°„ëŒ€ ë§¤ì¹­
                if query_analysis['time_of_day']:
                    time_scenes = [s for s in scene_types if query_analysis['time_of_day'] in s.lower()]
                    if time_scenes:
                        score += 0.3
                
                # ì¥ì†Œ ë§¤ì¹­
                if query_analysis['location']:
                    location_scenes = [s for s in scene_types if query_analysis['location'] in s.lower()]
                    if location_scenes:
                        score += 0.3
            
            return min(score, 1.0)
            
        except Exception:
            return 0.0
    
    def _get_match_reasons(self, video, query_analysis):
        """ë§¤ì¹­ ì´ìœ  ìƒì„±"""
        reasons = []
        
        if query_analysis['weather']:
            reasons.append(f"{query_analysis['weather']} ë‚ ì”¨ ì¡°ê±´")
        if query_analysis['time_of_day']:
            reasons.append(f"{query_analysis['time_of_day']} ì‹œê°„ëŒ€")
        if query_analysis['location']:
            reasons.append(f"{query_analysis['location']} í™˜ê²½")
            
        return reasons
    
    def _get_video_metadata(self, video):
        """ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        metadata = {
            'duration': video.duration,
            'file_size': video.file_size,
            'uploaded_at': video.uploaded_at.isoformat(),
            'analysis_type': 'basic'
        }
        
        if hasattr(video, 'analysis'):
            stats = video.analysis.analysis_statistics
            metadata.update({
                'analysis_type': stats.get('analysis_type', 'basic'),
                'scene_types': stats.get('scene_types', []),
                'dominant_objects': stats.get('dominant_objects', [])
            })
        
        return metadata

# views.py - ê³ ê¸‰ ê²€ìƒ‰ ê´€ë ¨ ë·° ìˆ˜ì •ëœ ë²„ì „
# views.py - IntraVideoTrackingView í–¥ìƒëœ ë²„ì „ (ë”ë¯¸ ë°ì´í„° ì§€ì›)

@method_decorator(csrf_exempt, name='dispatch')
class FrameVisualizationView(APIView):
    """í”„ë ˆì„ì— ë°”ìš´ë”© ë°•ìŠ¤ ì‹œê°í™”"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            frame_id = request.data.get('frame_id')
            objects = request.data.get('objects', [])
            
            if not video_id or not frame_id:
                return Response({'error': 'ë¹„ë””ì˜¤ IDì™€ í”„ë ˆì„ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
                frame = Frame.objects.filter(video=video, image_id=frame_id).first()
                
                if not frame or not frame.image:
                    return Response({'error': 'í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
                
                # detected_objectsì—ì„œ persons ë°°ì—´ ì¶”ì¶œ
                detected_objects = frame.detected_objects
                if isinstance(detected_objects, dict) and 'persons' in detected_objects:
                    persons = detected_objects['persons']
                else:
                    persons = []
                
                # ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                image_path = self._draw_bounding_boxes(frame, persons)
                
                return Response({
                    'success': True,
                    'image_url': image_path,
                    'objects_count': len(persons)
                })
                
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
                
        except Exception as e:
            logger.error(f"âŒ ë°”ìš´ë”© ë°•ìŠ¤ ì‹œê°í™” ì˜¤ë¥˜: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _draw_bounding_boxes(self, frame, objects):
        """í”„ë ˆì„ ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°"""
        import cv2
        import numpy as np
        import os
        from django.conf import settings
        
        try:
            # ì´ë¯¸ì§€ ê²½ë¡œ
            image_path = os.path.join(settings.MEDIA_ROOT, str(frame.image))
            
            if not os.path.exists(image_path):
                return None
            
            # ì´ë¯¸ì§€ ì½ê¸°
            image = cv2.imread(image_path)
            if image is None:
                return None
            
            height, width = image.shape[:2]
            
            # ê° ê°ì²´ì— ëŒ€í•´ ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            for i, obj in enumerate(objects):
                bbox = obj.get('bbox', [])
                if len(bbox) != 4:
                    continue
                
                # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                x1 = int(bbox[0] * width)
                y1 = int(bbox[1] * height)
                x2 = int(bbox[2] * width)
                y2 = int(bbox[3] * height)
                
                # ìƒ‰ìƒ ì„¤ì • (ê°ì²´ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒ)
                colors = [
                    (0, 255, 0),    # ì´ˆë¡
                    (255, 0, 0),    # íŒŒë‘
                    (0, 0, 255),    # ë¹¨ê°•
                    (255, 255, 0),  # ì‹œì•ˆ
                    (255, 0, 255),  # ë§ˆì  íƒ€
                    (0, 255, 255),  # ë…¸ë‘
                ]
                color = colors[i % len(colors)]
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
                
                # ë¼ë²¨ ì¶”ê°€
                label = obj.get('description', f'Object {i+1}')
                confidence = obj.get('confidence', 0)
                label_text = f'{label} ({confidence:.2f})'
                
                # ë¼ë²¨ ë°°ê²½
                (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
                cv2.rectangle(image, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                cv2.putText(image, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
            output_dir = os.path.join(settings.MEDIA_ROOT, 'visualized_frames')
            os.makedirs(output_dir, exist_ok=True)
            
            output_filename = f'video{frame.video.id}_frame{frame.image_id}_visualized.jpg'
            output_path = os.path.join(output_dir, output_filename)
            
            cv2.imwrite(output_path, image)
            
            # ìƒëŒ€ ê²½ë¡œ ë°˜í™˜
            relative_path = os.path.relpath(output_path, settings.MEDIA_ROOT)
            return relative_path
            
        except Exception as e:
            logger.error(f"âŒ ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            return None


class IntraVideoTrackingView(APIView):
    """ì˜ìƒ ë‚´ ê°ì²´ ì¶”ì  - í–¥ìƒëœ ë²„ì „ (ë”ë¯¸ ë°ì´í„° ì§€ì›)"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            tracking_target = request.data.get('tracking_target', '').strip()
            time_range = request.data.get('time_range', {})
            
            logger.info(f"ğŸ¯ ê°ì²´ ì¶”ì  ìš”ì²­: ë¹„ë””ì˜¤={video_id}, ëŒ€ìƒ='{tracking_target}', ì‹œê°„ë²”ìœ„={time_range}")
            
            if not video_id or not tracking_target:
                return Response({'error': 'ë¹„ë””ì˜¤ IDì™€ ì¶”ì  ëŒ€ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # Frame ë°ì´í„° í™•ì¸ ë° ìƒì„±
            self._ensure_frame_data(video)
            
            # íƒ€ê²Ÿ ë¶„ì„ (ìƒ‰ìƒ, ê°ì²´ íƒ€ì… ë“± ì¶”ì¶œ)
            target_analysis = self._analyze_tracking_target(tracking_target)
            logger.info(f"ğŸ“‹ íƒ€ê²Ÿ ë¶„ì„ ê²°ê³¼: {target_analysis}")
            
            # í”„ë ˆì„ë³„ ì¶”ì  ê²°ê³¼
            tracking_results = self._perform_object_tracking(video, target_analysis, time_range)
            
            logger.info(f"âœ… ê°ì²´ ì¶”ì  ì™„ë£Œ: {len(tracking_results)}ê°œ ê²°ê³¼")
            
            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë” ê´€ëŒ€í•œ ê²€ìƒ‰ ìˆ˜í–‰
            if not tracking_results:
                logger.info("ğŸ”„ ê´€ëŒ€í•œ ê²€ìƒ‰ ëª¨ë“œë¡œ ì¬ì‹œë„...")
                tracking_results = self._perform_lenient_tracking(video, target_analysis, time_range)
            
            return Response({
                'video_id': video_id,
                'tracking_target': tracking_target,
                'target_analysis': target_analysis,
                'tracking_results': tracking_results,
                'total_detections': len(tracking_results),
                'search_type': 'object_tracking'
            })
            
        except Exception as e:
            logger.error(f"âŒ ê°ì²´ ì¶”ì  ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return Response({'error': str(e)}, status=500)
    
    def _ensure_frame_data(self, video):
        """Frame ë°ì´í„° í™•ì¸ ë° ìƒì„±"""
        try:
            frame_count = video.frames.count()
            if frame_count == 0:
                logger.warning(f"âš ï¸ ë¹„ë””ì˜¤ {video.original_name}ì— Frame ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                from .models import create_dummy_frame_data
                create_dummy_frame_data(video, frame_count=30)
                logger.info(f"âœ… ë”ë¯¸ Frame ë°ì´í„° ìƒì„± ì™„ë£Œ: 30ê°œ")
                return True
            else:
                logger.info(f"ğŸ“Š ê¸°ì¡´ Frame ë°ì´í„° í™•ì¸: {frame_count}ê°œ")
                return False
        except Exception as e:
            logger.error(f"âŒ Frame ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _analyze_tracking_target(self, target):
        """ì¶”ì  ëŒ€ìƒ ë¶„ì„ - í–¥ìƒëœ ë²„ì „"""
        analysis = {
            'object_type': None,
            'colors': [],
            'gender': None,
            'clothing': [],
            'keywords': target.lower().split(),
            'original_target': target
        }
        
        target_lower = target.lower()
        
        # YOLO ê°ì²´ íƒ€ì… ë§¤í•‘ í™•ì¥ (COCO ë°ì´í„°ì…‹ ê¸°ë°˜)
        object_mappings = {
            # ì‚¬ëŒ ê´€ë ¨
            ('ì‚¬ëŒ', 'ë‚¨ì„±', 'ì—¬ì„±', 'ì¸ë¬¼', 'person'): 'person',
            
            # ê°€ë°© ê´€ë ¨
            ('ê°€ë°©', 'handbag', 'ë°±íŒ©', 'í•¸ë“œë°±', 'bag', 'backpack'): 'bag',
            
            # ì „ìê¸°ê¸°
            ('tv', 'í‹°ë¹„', 'í…”ë ˆë¹„ì „', 'television'): 'tv',
            ('ë…¸íŠ¸ë¶', 'ì»´í“¨í„°', 'laptop'): 'laptop',
            ('í•¸ë“œí°', 'íœ´ëŒ€í°', 'í°', 'phone', 'cell_phone'): 'cell_phone',
            ('í‚¤ë³´ë“œ', 'keyboard'): 'keyboard',
            ('ë§ˆìš°ìŠ¤', 'mouse'): 'mouse',
            ('ë¦¬ëª¨ì»¨', 'remote'): 'remote',
            
            # ê°€êµ¬
            ('ì˜ì', 'chair'): 'chair',
            ('ì¹¨ëŒ€', 'bed'): 'bed',
            ('ì†ŒíŒŒ', 'couch'): 'couch',
            ('ì‹íƒ', 'dining_table'): 'dining_table',
            ('í™”ì¥ëŒ€', 'toilet'): 'toilet',
            
            # êµí†µìˆ˜ë‹¨
            ('ì°¨', 'ìë™ì°¨', 'ì°¨ëŸ‰', 'ìŠ¹ìš©ì°¨', 'car'): 'car',
            ('ìì „ê±°', 'bicycle'): 'bicycle',
            ('ì˜¤í† ë°”ì´', 'motorcycle'): 'motorcycle',
            ('ë²„ìŠ¤', 'bus'): 'bus',
            ('íŠ¸ëŸ­', 'truck'): 'truck',
            ('ê¸°ì°¨', 'train'): 'train',
            ('ë¹„í–‰ê¸°', 'airplane'): 'airplane',
            ('ë³´íŠ¸', 'boat'): 'boat',
            
            # ë™ë¬¼
            ('ê°œ', 'ê°•ì•„ì§€', 'ë©ë©ì´', 'dog'): 'dog',
            ('ê³ ì–‘ì´', 'ëƒ¥ì´', 'cat'): 'cat',
            ('ë§', 'horse'): 'horse',
            ('ì†Œ', 'cow'): 'cow',
            ('ì–‘', 'sheep'): 'sheep',
            ('ìƒˆ', 'bird'): 'bird',
            ('ê³°', 'bear'): 'bear',
            ('ì½”ë¼ë¦¬', 'elephant'): 'elephant',
            ('ê¸°ë¦°', 'giraffe'): 'giraffe',
            ('ì–¼ë£©ë§', 'zebra'): 'zebra',
            
            # ìŒì‹
            ('ë°”ë‚˜ë‚˜', 'banana'): 'banana',
            ('ì‚¬ê³¼', 'apple'): 'apple',
            ('ìƒŒë“œìœ„ì¹˜', 'sandwich'): 'sandwich',
            ('ì˜¤ë Œì§€', 'orange'): 'orange',
            ('ë¸Œë¡œì½œë¦¬', 'broccoli'): 'broccoli',
            ('ë‹¹ê·¼', 'carrot'): 'carrot',
            ('í•«ë„ê·¸', 'hot_dog'): 'hot_dog',
            ('í”¼ì', 'pizza'): 'pizza',
            ('ë„ë„›', 'donut'): 'donut',
            ('ì¼€ì´í¬', 'cake'): 'cake',
            
            # ìŠ¤í¬ì¸ /ë†€ì´
            ('ê³µ', 'ball'): 'sports_ball',
            ('ì•¼êµ¬ë°°íŠ¸', 'baseball_bat'): 'baseball_bat',
            ('ì•¼êµ¬ê¸€ëŸ¬ë¸Œ', 'baseball_glove'): 'baseball_glove',
            ('í…Œë‹ˆìŠ¤ë¼ì¼“', 'tennis_racket'): 'tennis_racket',
            ('ìŠ¤í‚¤', 'skis'): 'skis',
            ('ìŠ¤ë…¸ë³´ë“œ', 'snowboard'): 'snowboard',
            ('í”„ë¦¬ìŠ¤ë¹„', 'frisbee'): 'frisbee',
            ('í‚¤íŠ¸', 'kite'): 'kite',
            ('ì•¼êµ¬ë°©ë§ì´', 'baseball_bat'): 'baseball_bat',
            
            # ë„êµ¬/ë¬¼ê±´
            ('ê°€ìœ„', 'scissors'): 'scissors',
            ('ìš°ì‚°', 'umbrella'): 'umbrella',
            ('í•¸ë“œë°±', 'handbag'): 'handbag',
            ('ë„¥íƒ€ì´', 'tie'): 'tie',
            ('ê°€ë°©', 'suitcase'): 'suitcase',
            ('í”„ë¦¬ìŠ¤ë¹„', 'frisbee'): 'frisbee',
            ('ìŠ¤í‚¤', 'skis'): 'skis',
            ('ìŠ¤ë…¸ë³´ë“œ', 'snowboard'): 'snowboard',
            ('ìŠ¤í¬ì¸ ê³µ', 'sports_ball'): 'sports_ball',
            ('í‚¤íŠ¸', 'kite'): 'kite',
            ('ì•¼êµ¬ë°©ë§ì´', 'baseball_bat'): 'baseball_bat',
            ('ì•¼êµ¬ê¸€ëŸ¬ë¸Œ', 'baseball_glove'): 'baseball_glove',
            ('í…Œë‹ˆìŠ¤ë¼ì¼“', 'tennis_racket'): 'tennis_racket',
            ('ë³‘', 'bottle'): 'bottle',
            ('ì™€ì¸ì”', 'wine_glass'): 'wine_glass',
            ('ì»µ', 'cup'): 'cup',
            ('í¬í¬', 'fork'): 'fork',
            ('ë‚˜ì´í”„', 'knife'): 'knife',
            ('ìˆŸê°€ë½', 'spoon'): 'spoon',
            ('ê·¸ë¦‡', 'bowl'): 'bowl',
            
            # ê¸°íƒ€
            ('ì±…', 'book'): 'book',
            ('ì‹œê³„', 'clock'): 'clock',
            ('ê°€ìœ„', 'scissors'): 'scissors',
            ('í…Œë””ë² ì–´', 'teddy_bear'): 'teddy_bear',
            ('í—¤ì–´ë“œë¼ì´ì–´', 'hair_drier'): 'hair_drier',
            ('ì¹«ì†”', 'toothbrush'): 'toothbrush'
        }
        
        
        for keywords, obj_type in object_mappings.items():
            if any(keyword in target_lower for keyword in keywords):
                analysis['object_type'] = obj_type
                break
        
        # ìƒ‰ìƒ ì¶”ì¶œ í™•ì¥
        color_keywords = {
            'ë¹¨ê°„': 'red', 'ë¹¨ê°•': 'red', 'ì ìƒ‰': 'red',
            'ì£¼í™©': 'orange', 'ì˜¤ë Œì§€': 'orange',
            'ë…¸ë€': 'yellow', 'ë…¸ë‘': 'yellow', 'í™©ìƒ‰': 'yellow',
            'ì´ˆë¡': 'green', 'ë…¹ìƒ‰': 'green',
            'íŒŒë€': 'blue', 'íŒŒë‘': 'blue', 'ì²­ìƒ‰': 'blue',
            'ë³´ë¼': 'purple', 'ìì£¼': 'purple',
            'ê²€ì€': 'black', 'ê²€ì •': 'black',
            'í°': 'white', 'í•˜ì–€': 'white', 'ë°±ìƒ‰': 'white',
            'íšŒìƒ‰': 'gray', 'ê·¸ë ˆì´': 'gray',
            'í•‘í¬': 'pink','ë¶„í™': 'pink',
            'ê°ˆìƒ‰': 'brown', 'ë¸Œë¼ìš´': 'brown',
        }
        
        for keyword, color in color_keywords.items():
            if keyword in target_lower:
                analysis['colors'].append(color)
        
        # ì„±ë³„ ë° ì˜ìƒ ì •ë³´
        if any(word in target_lower for word in ['ë‚¨ì„±', 'ë‚¨ì', 'ì•„ì €ì”¨']):
            analysis['gender'] = 'male'
        elif any(word in target_lower for word in ['ì—¬ì„±', 'ì—¬ì', 'ì•„ì£¼ë¨¸ë‹ˆ']):
            analysis['gender'] = 'female'
        
        if any(word in target_lower for word in ['ìƒì˜', 'í‹°ì…”ì¸ ', 'ì…”ì¸ ', 'ì˜·']):
            analysis['clothing'].append('top')
        if any(word in target_lower for word in ['ëª¨ì', 'ìº¡', 'í–‡']):
            analysis['clothing'].append('hat')
        
        return analysis
    
    def _calculate_person_match_score(self, person, target_analysis):
        """PersonDetectionê³¼ ê²€ìƒ‰ ì¡°ê±´ì˜ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        match_score = 0.0
        match_reasons = []
        
        # ê¸°ë³¸ person ë§¤ì¹­ (bag ê²€ìƒ‰ë„ personì—ì„œ ì°¾ê¸°)
        if target_analysis.get('object_type') in ['person', 'bag']:
            match_score += 0.3
            match_reasons.append("ì‚¬ëŒ ê°ì²´ ë§¤ì¹­")
        
        # ìƒ‰ìƒ ë§¤ì¹­
        target_colors = target_analysis.get('colors', [])
        if target_colors:
            person_colors = self._extract_person_colors(person)
            color_matches = set(target_colors) & set(person_colors)
            if color_matches:
                match_score += 0.4
                match_reasons.append(f"ìƒ‰ìƒ ë§¤ì¹­: {', '.join(color_matches)}")
        
        # ì„±ë³„ ë§¤ì¹­
        target_gender = target_analysis.get('gender')
        if target_gender and person.gender_estimation:
            if target_gender in person.gender_estimation.lower():
                match_score += 0.2
                match_reasons.append(f"ì„±ë³„ ë§¤ì¹­: {person.gender_estimation}")
        
        # ì•¡ì„¸ì„œë¦¬ ë§¤ì¹­ (ê°€ë°© ë“±) - ë” ê´€ëŒ€í•˜ê²Œ
        keywords = target_analysis.get('keywords', [])
        if any(word in keywords for word in ['ê°€ë°©', 'ë°±íŒ©', 'bag', 'backpack']):
            # ê°€ë°© ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¼ë‹¨ ë§¤ì¹­
            match_score += 0.3
            match_reasons.append("ê°€ë°© ì•¡ì„¸ì„œë¦¬ ë§¤ì¹­")
        
        return match_score, match_reasons
    
    def _extract_person_colors(self, person):
        """PersonDetectionì—ì„œ ìƒ‰ìƒ ì •ë³´ ì¶”ì¶œ"""
        colors = []
        
        # ìƒì˜ ìƒ‰ìƒ
        upper_color = person.upper_body_color.lower()
        if 'red' in upper_color or 'ë¹¨ê°„' in upper_color:
            colors.append('red')
        elif 'blue' in upper_color or 'íŒŒë€' in upper_color:
            colors.append('blue')
        elif 'green' in upper_color or 'ì´ˆë¡' in upper_color:
            colors.append('green')
        elif 'pink' in upper_color or 'ë¶„í™' in upper_color:
            colors.append('pink')
        elif 'black' in upper_color or 'ê²€ì€' in upper_color:
            colors.append('black')
        elif 'white' in upper_color or 'í°' in upper_color:
            colors.append('white')
        elif 'brown' in upper_color or 'ê°ˆìƒ‰' in upper_color:
            colors.append('brown')
        elif 'orange' in upper_color or 'ì£¼í™©' in upper_color:
            colors.append('orange')
        elif 'yellow' in upper_color or 'ë…¸ë€' in upper_color:
            colors.append('yellow')
        
        # í•˜ì˜ ìƒ‰ìƒ
        lower_color = person.lower_body_color.lower()
        if 'red' in lower_color or 'ë¹¨ê°„' in lower_color:
            colors.append('red')
        elif 'blue' in lower_color or 'íŒŒë€' in lower_color:
            colors.append('blue')
        elif 'green' in lower_color or 'ì´ˆë¡' in lower_color:
            colors.append('green')
        elif 'pink' in lower_color or 'ë¶„í™' in lower_color:
            colors.append('pink')
        elif 'black' in lower_color or 'ê²€ì€' in lower_color:
            colors.append('black')
        elif 'white' in lower_color or 'í°' in lower_color:
            colors.append('white')
        elif 'brown' in lower_color or 'ê°ˆìƒ‰' in lower_color:
            colors.append('brown')
        elif 'orange' in lower_color or 'ì£¼í™©' in lower_color:
            colors.append('orange')
        elif 'yellow' in lower_color or 'ë…¸ë€' in lower_color:
            colors.append('yellow')
        
        return colors
    
    def _generate_person_description(self, person):
        """PersonDetection ê¸°ë°˜ ì„¤ëª… ìƒì„±"""
        parts = []
        
        if person.gender_estimation and person.gender_estimation != 'unknown':
            parts.append(person.gender_estimation)
        
        if person.age_group and person.age_group != 'unknown':
            parts.append(person.age_group)
        
        if person.upper_body_color and person.upper_body_color != 'unknown':
            color = person.upper_body_color.replace('wearing ', '').replace(' clothes', '')
            parts.append(f"{color} ìƒì˜")
        
        if person.lower_body_color and person.lower_body_color != 'unknown':
            color = person.lower_body_color.replace('wearing ', '').replace(' clothes', '')
            parts.append(f"{color} í•˜ì˜")
        
        return ' '.join(parts) if parts else 'person'
    
    def _perform_bag_tracking(self, video, target_analysis, time_range):
        """ê°€ë°© ì „ìš© ì¶”ì  - ì‹¤ì œ ê°€ë°© ê°ì²´ê°€ ê°ì§€ëœ í”„ë ˆì„ë§Œ ê²€ìƒ‰"""
        tracking_results = []
        
        try:
            from .models import Frame
            import json
            
            # Frameì—ì„œ ê°€ë°© ê°ì²´ê°€ ê°ì§€ëœ í”„ë ˆì„ ì°¾ê¸°
            frames_query = Frame.objects.filter(video=video)
            
            # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
            
            frames = list(frames_query)
            logger.info(f"ğŸ“Š ê°€ë°© ê²€ìƒ‰í•  í”„ë ˆì„ ìˆ˜: {len(frames)}ê°œ")
            
            bag_frames = []
            
            for frame in frames:
                if frame.detected_objects:
                    try:
                        objects = json.loads(frame.detected_objects) if isinstance(frame.detected_objects, str) else frame.detected_objects
                        
                        # ê°€ë°© ê°ì²´ ì°¾ê¸° (backpack: 24, handbag: 26)
                        for obj in objects:
                            if isinstance(obj, dict):
                                class_id = obj.get('class_id')
                                class_name = obj.get('class', '').lower()
                                
                                # YOLO í´ë˜ìŠ¤ ID ë˜ëŠ” ì´ë¦„ìœ¼ë¡œ ê°€ë°© í™•ì¸
                                if (class_id in [24, 26] or 
                                    'bag' in class_name or 'handbag' in class_name or 'backpack' in class_name):
                                    
                                    # í•´ë‹¹ í”„ë ˆì„ì˜ ì‚¬ëŒë“¤ ì°¾ê¸°
                                    from .models import PersonDetection
                                    persons = PersonDetection.objects.filter(frame=frame)
                                    
                                    for person in persons:
                                        # ê°€ë°©ê³¼ ì‚¬ëŒì˜ ê±°ë¦¬ ê³„ì‚° (ê°„ë‹¨í•œ ê·¼ì‚¬)
                                        bag_bbox = obj.get('bbox', [])
                                        person_bbox = [person.bbox_x1, person.bbox_y1, person.bbox_x2, person.bbox_y2]
                                        
                                        # ê°€ë°©ê³¼ ì‚¬ëŒì´ ê°€ê¹Œì´ ìˆìœ¼ë©´ ë§¤ì¹­
                                        if self._is_bag_near_person(bag_bbox, person_bbox):
                                            match_score = 0.8  # ê°€ë°©ì´ ì‹¤ì œë¡œ ê°ì§€ëœ ê²½ìš° ë†’ì€ ì ìˆ˜
                                            match_reasons = [f"ê°€ë°© ê°ì²´ ê°ì§€: {class_name}"]
                                            
                                            tracking_results.append({
                                                'frame_id': frame.image_id,
                                                'timestamp': frame.timestamp,
                                                'confidence': match_score,
                                                'bbox': person_bbox,
                                                'description': f"{self._generate_person_description(person)} (ê°€ë°© ì†Œì§€)",
                                                'tracking_id': person.track_id or f"person_{person.id}",
                                                'match_reasons': match_reasons,
                                                'bag_info': {
                                                    'class': class_name,
                                                    'class_id': class_id,
                                                    'bag_bbox': bag_bbox,
                                                    'confidence': obj.get('confidence', 0.0)
                                                },
                                                'person_attributes': {
                                                    'gender': person.gender_estimation,
                                                    'age': person.age_group,
                                                    'upper_color': person.upper_body_color,
                                                    'lower_color': person.lower_body_color,
                                                    'posture': person.posture
                                                }
                                            })
                                    
                                    bag_frames.append(frame.image_id)
                                    
                    except Exception as e:
                        logger.warning(f"âš ï¸ í”„ë ˆì„ {frame.image_id} ê°ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            
            logger.info(f"ğŸ’ ê°€ë°©ì´ ê°ì§€ëœ í”„ë ˆì„: {len(bag_frames)}ê°œ")
            
            # ì‹ ë¢°ë„ìˆœ ì •ë ¬
            tracking_results.sort(key=lambda x: x['confidence'], reverse=True)
            
            return tracking_results
            
        except Exception as e:
            logger.error(f"âŒ ê°€ë°© ì¶”ì  ì˜¤ë¥˜: {e}")
            return []
    
    def _is_bag_near_person(self, bag_bbox, person_bbox):
        """ê°€ë°©ê³¼ ì‚¬ëŒì´ ê°€ê¹Œì´ ìˆëŠ”ì§€ í™•ì¸"""
        if not bag_bbox or not person_bbox or len(bag_bbox) < 4 or len(person_bbox) < 4:
            return False
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì  ê³„ì‚°
        bag_center_x = (bag_bbox[0] + bag_bbox[2]) / 2
        bag_center_y = (bag_bbox[1] + bag_bbox[3]) / 2
        person_center_x = (person_bbox[0] + person_bbox[2]) / 2
        person_center_y = (person_bbox[1] + person_bbox[3]) / 2
        
        # ê±°ë¦¬ ê³„ì‚°
        distance = ((bag_center_x - person_center_x) ** 2 + (bag_center_y - person_center_y) ** 2) ** 0.5
        
        # ì„ê³„ê°’ (ì´ë¯¸ì§€ í¬ê¸°ì— ë¹„ë¡€)
        threshold = 0.3  # ì •ê·œí™”ëœ ì¢Œí‘œ ê¸°ì¤€
        
        return distance < threshold
    
    def _perform_yolo_object_tracking(self, video, target_analysis, time_range):
        """YOLO ê°ì²´ ì „ìš© ì¶”ì  - ëª¨ë“  YOLO ê°ì²´ íƒ€ì… ì§€ì›"""
        tracking_results = []
        
        try:
            from .models import Frame
            import json
            
            # Frameì—ì„œ í•´ë‹¹ ê°ì²´ê°€ ê°ì§€ëœ í”„ë ˆì„ ì°¾ê¸°
            frames_query = Frame.objects.filter(video=video)
            
            # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
            
            frames = list(frames_query)
            logger.info(f"ğŸ“Š YOLO ê°ì²´ ê²€ìƒ‰í•  í”„ë ˆì„ ìˆ˜: {len(frames)}ê°œ")
            
            target_object_type = target_analysis.get('object_type', '').lower()
            target_colors = target_analysis.get('colors', [])
            
            # YOLO í´ë˜ìŠ¤ ë§¤í•‘
            yolo_class_mapping = {
                'car': ['car', 'truck', 'bus', 'motorcycle', 'bicycle'],
                'ìë™ì°¨': ['car', 'truck', 'bus', 'motorcycle', 'bicycle'],
                'ì°¨': ['car', 'truck', 'bus', 'motorcycle', 'bicycle'],
                'dog': ['dog'],
                'ê°œ': ['dog'],
                'cat': ['cat'],
                'ê³ ì–‘ì´': ['cat'],
                'phone': ['cell phone'],
                'í•¸ë“œí°': ['cell phone'],
                'ì „í™”': ['cell phone'],
                'laptop': ['laptop'],
                'ë…¸íŠ¸ë¶': ['laptop'],
                'book': ['book'],
                'ì±…': ['book'],
                'chair': ['chair'],
                'ì˜ì': ['chair'],
                'table': ['dining table'],
                'í…Œì´ë¸”': ['dining table'],
                'cup': ['cup'],
                'ì»µ': ['cup'],
                'bottle': ['bottle'],
                'ë³‘': ['bottle'],
                'keyboard': ['keyboard'],
                'í‚¤ë³´ë“œ': ['keyboard'],
                'mouse': ['mouse'],
                'ë§ˆìš°ìŠ¤': ['mouse'],
                'tv': ['tv'],
                'í…”ë ˆë¹„ì „': ['tv'],
                'remote': ['remote'],
                'ë¦¬ëª¨ì»¨': ['remote'],
                'umbrella': ['umbrella'],
                'ìš°ì‚°': ['umbrella'],
                'suitcase': ['suitcase'],
                'ì—¬í–‰ê°€ë°©': ['suitcase'],
                'frisbee': ['frisbee'],
                'ì›ë°˜': ['frisbee'],
                'skis': ['skis'],
                'ìŠ¤í‚¤': ['skis'],
                'snowboard': ['snowboard'],
                'ìŠ¤ë…¸ë³´ë“œ': ['snowboard'],
                'sports ball': ['sports ball'],
                'ê³µ': ['sports ball'],
                'kite': ['kite'],
                'ì—°': ['kite'],
                'baseball bat': ['baseball bat'],
                'ì•¼êµ¬ë°°íŠ¸': ['baseball bat'],
                'baseball glove': ['baseball glove'],
                'ì•¼êµ¬ì¥ê°‘': ['baseball glove'],
                'skateboard': ['skateboard'],
                'ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ': ['skateboard'],
                'surfboard': ['surfboard'],
                'ì„œí•‘ë³´ë“œ': ['surfboard'],
                'tennis racket': ['tennis racket'],
                'í…Œë‹ˆìŠ¤ë¼ì¼“': ['tennis racket'],
                'wine glass': ['wine glass'],
                'ì™€ì¸ì”': ['wine glass'],
                'fork': ['fork'],
                'í¬í¬': ['fork'],
                'knife': ['knife'],
                'ì¹¼': ['knife'],
                'spoon': ['spoon'],
                'ìˆŸê°€ë½': ['spoon'],
                'bowl': ['bowl'],
                'ê·¸ë¦‡': ['bowl'],
                'banana': ['banana'],
                'ë°”ë‚˜ë‚˜': ['banana'],
                'apple': ['apple'],
                'ì‚¬ê³¼': ['apple'],
                'sandwich': ['sandwich'],
                'ìƒŒë“œìœ„ì¹˜': ['sandwich'],
                'orange': ['orange'],
                'ì˜¤ë Œì§€': ['orange'],
                'broccoli': ['broccoli'],
                'ë¸Œë¡œì½œë¦¬': ['broccoli'],
                'carrot': ['carrot'],
                'ë‹¹ê·¼': ['carrot'],
                'hot dog': ['hot dog'],
                'í•«ë„ê·¸': ['hot dog'],
                'pizza': ['pizza'],
                'í”¼ì': ['pizza'],
                'donut': ['donut'],
                'ë„ë„›': ['donut'],
                'cake': ['cake'],
                'ì¼€ì´í¬': ['cake'],
                'couch': ['couch'],
                'ì†ŒíŒŒ': ['couch'],
                'bed': ['bed'],
                'ì¹¨ëŒ€': ['bed'],
                'toilet': ['toilet'],
                'í™”ì¥ì‹¤': ['toilet'],
                'laptop': ['laptop'],
                'ë…¸íŠ¸ë¶': ['laptop'],
                'mouse': ['mouse'],
                'ë§ˆìš°ìŠ¤': ['mouse'],
                'remote': ['remote'],
                'ë¦¬ëª¨ì»¨': ['remote'],
                'keyboard': ['keyboard'],
                'í‚¤ë³´ë“œ': ['keyboard'],
                'cell phone': ['cell phone'],
                'í•¸ë“œí°': ['cell phone'],
                'microwave': ['microwave'],
                'ì „ìë ˆì¸ì§€': ['microwave'],
                'oven': ['oven'],
                'ì˜¤ë¸': ['oven'],
                'toaster': ['toaster'],
                'í† ìŠ¤í„°': ['toaster'],
                'sink': ['sink'],
                'ì‹±í¬ëŒ€': ['sink'],
                'refrigerator': ['refrigerator'],
                'ëƒ‰ì¥ê³ ': ['refrigerator'],
                'book': ['book'],
                'ì±…': ['book'],
                'clock': ['clock'],
                'ì‹œê³„': ['clock'],
                'vase': ['vase'],
                'í™”ë¶„': ['vase'],
                'scissors': ['scissors'],
                'ê°€ìœ„': ['scissors'],
                'teddy bear': ['teddy bear'],
                'ê³°ì¸í˜•': ['teddy bear'],
                'hair drier': ['hair drier'],
                'ë“œë¼ì´ì–´': ['hair drier'],
                'toothbrush': ['toothbrush'],
                'ì¹«ì†”': ['toothbrush']
            }
            
            # ê²€ìƒ‰í•  YOLO í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
            search_classes = []
            if target_object_type in yolo_class_mapping:
                search_classes = yolo_class_mapping[target_object_type]
            else:
                # ì§ì ‘ ë§¤ì¹­ ì‹œë„
                search_classes = [target_object_type]
            
            logger.info(f"ğŸ” ê²€ìƒ‰í•  YOLO í´ë˜ìŠ¤: {search_classes}")
            
            object_frames = []
            
            for frame in frames:
                if frame.detected_objects:
                    try:
                        objects = json.loads(frame.detected_objects) if isinstance(frame.detected_objects, str) else frame.detected_objects
                        
                        # í•´ë‹¹ ê°ì²´ ì°¾ê¸°
                        for obj in objects:
                            if isinstance(obj, dict):
                                class_name = obj.get('class', '').lower()
                                class_id = obj.get('class_id')
                                confidence = obj.get('confidence', 0.0)
                                
                                # í´ë˜ìŠ¤ ë§¤ì¹­ í™•ì¸
                                is_matching_class = any(search_class in class_name for search_class in search_classes)
                                
                                if is_matching_class and confidence > 0.3:  # ì‹ ë¢°ë„ 30% ì´ìƒ
                                    # ìƒ‰ìƒ ë§¤ì¹­ (ì„ íƒì )
                                    color_match = True
                                    if target_colors:
                                        # ìƒ‰ìƒ ë§¤ì¹­ ë¡œì§ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
                                        obj_description = f"{class_name} {obj.get('description', '')}".lower()
                                        color_match = any(color in obj_description for color in target_colors)
                                    
                                    if color_match:
                                        tracking_results.append({
                                            'frame_id': frame.image_id,
                                            'timestamp': frame.timestamp,
                                            'confidence': confidence,
                                            'bbox': obj.get('bbox', []),
                                            'description': f"{class_name} (ì‹ ë¢°ë„: {confidence:.1%})",
                                            'tracking_id': f"{class_name}_{frame.image_id}_{len(tracking_results)}",
                                            'match_reasons': [f"YOLO ê°ì²´ ê°ì§€: {class_name}"],
                                            'object_info': {
                                                'class': class_name,
                                                'class_id': class_id,
                                                'confidence': confidence,
                                                'bbox': obj.get('bbox', [])
                                            }
                                        })
                                        
                                        object_frames.append(frame.image_id)
                                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ í”„ë ˆì„ {frame.image_id} ê°ì²´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            
            logger.info(f"ğŸ¯ {target_object_type} ê°ì²´ê°€ ê°ì§€ëœ í”„ë ˆì„: {len(object_frames)}ê°œ")
            
            # ì‹ ë¢°ë„ìˆœ ì •ë ¬
            tracking_results.sort(key=lambda x: x['confidence'], reverse=True)
            
            return tracking_results
            
        except Exception as e:
            logger.error(f"âŒ YOLO ê°ì²´ ì¶”ì  ì˜¤ë¥˜: {e}")
            return []
    
    def _perform_object_tracking(self, video, target_analysis, time_range):
        """ì‹¤ì œ ê°ì²´ ì¶”ì  ìˆ˜í–‰ - YOLO ê°ì²´ ë° PersonDetection ë°ì´í„° í™œìš©"""
        tracking_results = []
        
        try:
            # ê°€ë°© ê²€ìƒ‰ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if target_analysis.get('object_type') == 'bag' or 'ê°€ë°©' in target_analysis.get('keywords', []):
                return self._perform_bag_tracking(video, target_analysis, time_range)
            
            # ì¼ë°˜ ê°ì²´ ê²€ìƒ‰ì¸ ê²½ìš° YOLO ê°ì²´ ì¶”ì 
            if target_analysis.get('object_type') and target_analysis.get('object_type') != 'person':
                return self._perform_yolo_object_tracking(video, target_analysis, time_range)
            
            # PersonDetection ë°ì´í„°ì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ì‚¬ëŒ ê²€ìƒ‰)
            from .models import PersonDetection
            
            # ê¸°ë³¸ ì¿¼ë¦¬
            persons_query = PersonDetection.objects.filter(frame__video=video)
            
            # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                persons_query = persons_query.filter(
                    frame__timestamp__gte=start_time, 
                    frame__timestamp__lte=end_time
                )
                logger.info(f"â° ì‹œê°„ í•„í„°ë§: {start_time}s ~ {end_time}s")
            
            persons = list(persons_query.select_related('frame'))
            logger.info(f"ğŸ“Š ë¶„ì„í•  PersonDetection ìˆ˜: {len(persons)}ê°œ")
            
            if not persons:
                logger.warning("âš ï¸ ë¶„ì„í•  PersonDetectionì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            for person in persons:
                try:
                    match_score, match_reasons = self._calculate_person_match_score(person, target_analysis)
                    
                    if match_score >= 0.4:  # 40% ì´ìƒ ë§¤ì¹­ (ì„ê³„ê°’ ë‚®ì¶¤)
                        tracking_results.append({
                            'frame_id': person.frame.image_id,
                            'timestamp': person.frame.timestamp,
                            'confidence': match_score,
                            'bbox': [person.bbox_x1, person.bbox_y1, person.bbox_x2, person.bbox_y2],
                            'description': self._generate_person_description(person),
                            'tracking_id': person.track_id or f"person_{person.id}",
                            'match_reasons': match_reasons,
                            'person_attributes': {
                                'gender': person.gender_estimation,
                                'age': person.age_group,
                                'upper_color': person.upper_body_color,
                                'lower_color': person.lower_body_color,
                                'posture': person.posture
                            }
                        })
                except Exception as person_error:
                    logger.warning(f"âš ï¸ PersonDetection {person.id} ì²˜ë¦¬ ì‹¤íŒ¨: {person_error}")
                    continue
            
            # ì‹ ë¢°ë„ìˆœ ì •ë ¬
            tracking_results.sort(key=lambda x: x['confidence'], reverse=True)
            
            return tracking_results
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ì  ìˆ˜í–‰ ì˜¤ë¥˜: {e}")
            return []
        
    def _perform_lenient_tracking(self, video, target_analysis, time_range):
        try:
            frames_query = Frame.objects.filter(video=video).order_by('timestamp')
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
                
            tracking_results = []
            for frame in frames_query:
                try:
                    detected_objects = self._get_detected_objects(frame)
                    for obj in detected_objects:
                        match_score = 0.0
                        match_reasons = []
                        
                        # ê°ì²´ íƒ€ì… (í•„ìˆ˜)
                        if target_analysis.get('object_type'):
                            if obj['class'] == target_analysis['object_type']:
                                match_score += 0.3
                                match_reasons.append(f"{obj['class']} ê°ì²´ íƒ€ì… ë§¤ì¹­")
                            else:
                                continue  # ê°ì²´ íƒ€ì…ì´ ë‹¤ë¥´ë©´ ê±´ë„ˆë›°ê¸°
                        
                        # ìƒ‰ìƒ (ê´€ëŒ€í•˜ì§€ë§Œ ì—¬ì „íˆ ì„ ë³„ì )
                        color_matched = False
                        if target_analysis.get('colors'):
                            for color in target_analysis['colors']:
                                obj_color_desc = obj['color_description'].lower()
                                if color == 'black':
                                    if 'black' in obj_color_desc:
                                        if 'mixed' not in obj_color_desc:
                                            match_score += 0.3  # ìˆœìˆ˜ black
                                        else:
                                            match_score += 0.1  # black-mixed
                                        match_reasons.append(f"{color} ìƒ‰ìƒ ë§¤ì¹­")
                                        color_matched = True
                                        break
                                else:
                                    if color in obj_color_desc or color in [str(c).lower() for c in obj['colors']]:
                                        match_score += 0.2
                                        match_reasons.append(f"{color} ìƒ‰ìƒ ë§¤ì¹­")
                                        color_matched = True
                                        break
                            
                            if not color_matched:
                                continue  # ìƒ‰ìƒì´ ì§€ì •ë˜ì—ˆëŠ”ë° ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ì œì™¸
                        
                        # í‚¤ì›Œë“œ ë§¤ì¹­
                        for keyword in target_analysis.get('keywords', []):
                            if keyword in obj['class'] and keyword not in ['ì‚¬ëŒ', 'ì˜·', 'ì…ì€']:
                                match_score += 0.1
                                match_reasons.append(f"í‚¤ì›Œë“œ '{keyword}' ë§¤ì¹­")
                        
                        # ê´€ëŒ€í•œ ê²€ìƒ‰ì—ì„œë„ ìµœì†Œ ì ìˆ˜ ìœ ì§€
                        if match_score >= 0.3:
                            tracking_results.append({
                                'frame_id': frame.image_id,
                                'timestamp': frame.timestamp,
                                'confidence': min(match_score, obj['confidence'] or 0.5),
                                'bbox': obj['bbox'],
                                'description': self._generate_match_description(obj, target_analysis),
                                'tracking_id': obj.get('track_id') or f"obj_{frame.image_id}",
                                'match_reasons': match_reasons
                            })
                except Exception:
                    continue
                    
            tracking_results.sort(key=lambda x: x['timestamp'])
            logger.info(f"ğŸ” ê´€ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼: {len(tracking_results)}ê°œ")
            return tracking_results
        except Exception as e:
            logger.error(f"âŒ ê´€ëŒ€í•œ ì¶”ì  ì˜¤ë¥˜: {e}")
            return []
    def _get_detected_objects(self, frame):
        """
        ë‹¤ì–‘í•œ ì €ì¥ ìŠ¤í‚¤ë§ˆë¥¼ í˜¸í™˜í•´ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
        ìš°ì„ ìˆœìœ„:
        1) frame.detected_objects
        2) frame.comprehensive_features['objects']
        3) frame.yolo_objects / frame.detections / frame.objects
        ë¬¸ìì—´(JSON)ë¡œ ì €ì¥ëœ ê²½ìš° íŒŒì‹± ì‹œë„.
        ê° ê°ì²´ëŠ” ìµœì†Œí•œ {'class','bbox','confidence'} í‚¤ë¥¼ ê°–ë„ë¡ ì •ê·œí™”.
        """
        import json

        candidates = []

        # 1) detected_objects
        if hasattr(frame, 'detected_objects') and frame.detected_objects:
            detected_obj = frame.detected_objects
            # persons ë°°ì—´ì´ ìˆìœ¼ë©´ ìš°ì„  ì¶”ì¶œ
            if isinstance(detected_obj, dict) and 'persons' in detected_obj:
                candidates.append(detected_obj['persons'])
            else:
                candidates.append(detected_obj)

        # 2) comprehensive_features.objects
        if hasattr(frame, 'comprehensive_features') and frame.comprehensive_features:
            objs = None
            if isinstance(frame.comprehensive_features, dict):
                objs = frame.comprehensive_features.get('objects') \
                or frame.comprehensive_features.get('detections')
            elif isinstance(frame.comprehensive_features, str):
                try:
                    cf = json.loads(frame.comprehensive_features)
                    objs = (cf or {}).get('objects') or (cf or {}).get('detections')
                except Exception:
                    pass
            if objs:
                candidates.append(objs)

        # 3) ê¸°íƒ€ í•„ë“œë“¤
        for attr in ('yolo_objects', 'detections', 'objects'):
            if hasattr(frame, attr) and getattr(frame, attr):
                candidates.append(getattr(frame, attr))

        # ì²« ë²ˆì§¸ ìœ íš¨ í›„ë³´ ì„ íƒ
        detected = None
        for c in candidates:
            try:
                if isinstance(c, str):
                    c = json.loads(c)
                if isinstance(c, dict):           # {'objects': [...]} í˜•íƒœ ì§€ì›
                    c = c.get('objects') or c.get('detections')
                if isinstance(c, list):
                    detected = c
                    break
            except Exception:
                continue

        if not isinstance(detected, list):
            return []

        # ì •ê·œí™”
        norm = []
        for o in detected:
            if not isinstance(o, dict):
                continue
            cls = (o.get('class') or o.get('label') or o.get('name') or '').lower()
            bbox = o.get('bbox') or o.get('box') or o.get('xyxy') or []
            conf = float(o.get('confidence') or o.get('score') or 0.0)
            colors = o.get('colors') or o.get('color') or []
            if isinstance(colors, str):
                colors = [colors]
            color_desc = (o.get('color_description') or o.get('dominant_color') or 'unknown')
            track_id = o.get('track_id') or o.get('id')

            norm.append({
                'class': cls,
                'bbox': bbox,
                'confidence': conf,
                'colors': colors,
                'color_description': str(color_desc).lower(),
                'track_id': track_id,
                # ì›ë³¸ë„ ê°™ì´ ë³´ê´€(ë””ë²„ê·¸/í™•ì¥ìš©)
                '_raw': o,
            })
        return norm

    def _find_matching_objects(self, frame, target_analysis):
        matches = []
        try:
            detected_objects = self._get_detected_objects(frame)
            if not detected_objects:
                return matches
                
            for obj in detected_objects:
                match_score = 0.0
                match_reasons = []
                
                # ê°ì²´ íƒ€ì… ë§¤ì¹­ (í•„ìˆ˜)
                if target_analysis.get('object_type') and obj['class'] == target_analysis['object_type']:
                    match_score += 0.4
                    match_reasons.append(f"{target_analysis['object_type']} ê°ì²´ ë§¤ì¹­")
                elif target_analysis.get('object_type') == 'bag' and obj['class'] == 'person':
                    # ê°€ë°©ì€ person ê°ì²´ì˜ accessoriesì—ì„œ í™•ì¸
                    if self._check_bag_in_accessories(obj):
                        match_score += 0.6
                        match_reasons.append("ê°€ë°© ì•¡ì„¸ì„œë¦¬ ë§¤ì¹­")
                    else:
                        continue
                elif target_analysis.get('object_type') == 'phone' and obj['class'] == 'person':
                    # ì „í™”ëŠ” person ê°ì²´ì˜ accessoriesì—ì„œ í™•ì¸
                    if self._check_phone_in_accessories(obj):
                        match_score += 0.6
                        match_reasons.append("ì „í™” ì•¡ì„¸ì„œë¦¬ ë§¤ì¹­")
                    else:
                        continue
                elif target_analysis.get('object_type') and obj['class'] != target_analysis['object_type']:
                    # ê°ì²´ íƒ€ì…ì´ ë‹¤ë¥´ë©´ ê±´ë„ˆë›°ê¸°
                    continue
                
                # ìƒ‰ìƒ ë§¤ì¹­ (_raw.attributes.clothing_colorì—ì„œ ì¶”ì¶œ)
                color_matched = False
                if target_analysis.get('colors'):
                    target_colors = target_analysis['colors']
                    
                    # _raw í•„ë“œì—ì„œ ìƒ‰ìƒ ì •ë³´ ì¶”ì¶œ
                    raw_data = obj.get('_raw', {})
                    attributes = raw_data.get('attributes', {})
                    clothing_color = attributes.get('clothing_color', {})
                    color_value = clothing_color.get('value', '').lower()
                    color_scores = clothing_color.get('all_scores', {})
                    
                    for target_color in target_colors:
                        target_color_lower = target_color.lower()
                        
                        # ì •í™•í•œ ìƒ‰ìƒ ë§¤ì¹­
                        if target_color_lower in color_value:
                            match_score += 0.5
                            match_reasons.append(f"ì •í™•í•œ {target_color} ìƒ‰ìƒ ë§¤ì¹­")
                            color_matched = True
                            break
                        
                        # all_scoresì—ì„œ ìƒ‰ìƒ ê²€ìƒ‰
                        for color_name, score in color_scores.items():
                            if target_color_lower in color_name.lower():
                                match_score += 0.3
                                match_reasons.append(f"ë¶€ë¶„ {target_color} ìƒ‰ìƒ ë§¤ì¹­ (ì‹ ë¢°ë„: {score:.2f})")
                                color_matched = True
                                break
                        
                        if color_matched:
                            break
                    
                    # ìƒ‰ìƒì´ ì§€ì •ë˜ì—ˆëŠ”ë° ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ì œì™¸
                    if not color_matched:
                        continue
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ (ë³´ì¡°)
                for keyword in target_analysis.get('keywords', []):
                    if keyword in obj['class'] and keyword not in ['ì‚¬ëŒ', 'ì˜·', 'ì…ì€']:
                        match_score += 0.1
                        match_reasons.append(f"í‚¤ì›Œë“œ '{keyword}' ë§¤ì¹­")
                
                # ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ ìƒí–¥ ì¡°ì •
                if match_score >= 0.6:  # 0.4ì—ì„œ 0.6ìœ¼ë¡œ ìƒí–¥ (ì •í™•ë„ í–¥ìƒ)
                    matches.append({
                        'confidence': min(match_score, obj['confidence'] or 0.5),
                        'bbox': obj['bbox'],
                        'description': self._generate_match_description(obj, target_analysis),
                        'match_reasons': match_reasons,
                        'tracking_id': obj.get('track_id') or f"obj_{frame.image_id}",
                    })
            return matches
        except Exception as e:
            logger.warning(f"âš ï¸ ê°ì²´ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return []

    
    def _generate_match_description(self, obj, target_analysis):
        """ë§¤ì¹­ ì„¤ëª… ìƒì„± - í–¥ìƒëœ ë²„ì „"""
        desc_parts = []
        
        # ìƒ‰ìƒ ì •ë³´
        color_desc = obj.get('color_description', '')
        if color_desc and color_desc != 'unknown':
            desc_parts.append(color_desc)
        
        # ê°ì²´ í´ë˜ìŠ¤
        obj_class = obj.get('class', 'ê°ì²´')
        desc_parts.append(obj_class)
        
        # ì„±ë³„ ì •ë³´ (ìˆëŠ” ê²½ìš°)
        if target_analysis.get('gender'):
            desc_parts.append(f"({target_analysis['gender']})")
        
        # ì˜ìƒ ì •ë³´ (ìˆëŠ” ê²½ìš°)
        if target_analysis.get('clothing'):
            clothing_desc = ', '.join(target_analysis['clothing'])
            desc_parts.append(f"[{clothing_desc}]")
        
        description = ' '.join(desc_parts) + ' ê°ì§€'
        
        return description
    
    def _parse_time_to_seconds(self, time_str):
        """ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆë¡œ ë³€í™˜ - í–¥ìƒëœ ë²„ì „"""
        try:
            if not time_str:
                return 0
            
            time_str = str(time_str).strip()
            
            if ':' in time_str:
                parts = time_str.split(':')
                minutes = int(parts[0])
                seconds = int(parts[1]) if len(parts) > 1 else 0
                return minutes * 60 + seconds
            else:
                # ìˆœìˆ˜ ìˆ«ìì¸ ê²½ìš°
                return int(float(time_str))
        except (ValueError, TypeError) as e:
            logger.warning(f"âš ï¸ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {time_str} -> {e}")
            return 0
    
    def _check_bag_in_accessories(self, obj):
        """ê°ì²´ì˜ accessoriesì—ì„œ ê°€ë°© í™•ì¸"""
        try:
            # _raw í•„ë“œì—ì„œ accessories ì •ë³´ ì¶”ì¶œ
            raw_data = obj.get('_raw', {})
            attributes = raw_data.get('attributes', {})
            accessories = attributes.get('accessories', {})
            
            if isinstance(accessories, dict):
                # value í•„ë“œ í™•ì¸
                accessory_value = accessories.get('value', '').lower()
                if any(bag_keyword in accessory_value for bag_keyword in ['bag', 'backpack', 'handbag']):
                    return True
                
                # all_scoresì—ì„œ ê°€ë°© ê´€ë ¨ ì ìˆ˜ í™•ì¸
                all_scores = accessories.get('all_scores', {})
                for score_key, score_value in all_scores.items():
                    if any(bag_keyword in score_key.lower() for bag_keyword in ['bag', 'backpack', 'handbag']):
                        if score_value > 0.1:  # ì‹ ë¢°ë„ 0.1 ì´ìƒ
                            return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ ê°€ë°© ì•¡ì„¸ì„œë¦¬ í™•ì¸ ì˜¤ë¥˜: {e}")
            return False
    
    def _check_phone_in_accessories(self, obj):
        """ê°ì²´ì˜ accessoriesì—ì„œ ì „í™” í™•ì¸"""
        try:
            # _raw í•„ë“œì—ì„œ accessories ì •ë³´ ì¶”ì¶œ
            raw_data = obj.get('_raw', {})
            attributes = raw_data.get('attributes', {})
            accessories = attributes.get('accessories', {})
            
            if isinstance(accessories, dict):
                # value í•„ë“œ í™•ì¸
                accessory_value = accessories.get('value', '').lower()
                if any(phone_keyword in accessory_value for phone_keyword in ['phone', 'cell_phone', 'mobile']):
                    return True
                
                # all_scoresì—ì„œ ì „í™” ê´€ë ¨ ì ìˆ˜ í™•ì¸
                all_scores = accessories.get('all_scores', {})
                for score_key, score_value in all_scores.items():
                    if any(phone_keyword in score_key.lower() for phone_keyword in ['phone', 'cell_phone', 'mobile']):
                        if score_value > 0.1:  # ì‹ ë¢°ë„ 0.1 ì´ìƒ
                            return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ ì „í™” ì•¡ì„¸ì„œë¦¬ í™•ì¸ ì˜¤ë¥˜: {e}")
            return False

@method_decorator(csrf_exempt, name='dispatch')
class TimeBasedAnalysisView(APIView):
    """ì‹œê°„ëŒ€ë³„ ë¶„ì„ - ìˆ˜ì •ëœ ë²„ì „"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            time_range = request.data.get('time_range', {})
            analysis_type = request.data.get('analysis_type', 'ì„±ë¹„ ë¶„í¬')
            
            logger.info(f"ğŸ“Š ì‹œê°„ëŒ€ë³„ ë¶„ì„ ìš”ì²­: ë¹„ë””ì˜¤={video_id}, ì‹œê°„ë²”ìœ„={time_range}, íƒ€ì…='{analysis_type}'")
            
            if not video_id or not time_range.get('start') or not time_range.get('end'):
                return Response({'error': 'ë¹„ë””ì˜¤ IDì™€ ì‹œê°„ ë²”ìœ„ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # ì‹œê°„ ë²”ìœ„ íŒŒì‹±
            start_time = self._parse_time_to_seconds(time_range['start'])
            end_time = self._parse_time_to_seconds(time_range['end'])
            
            logger.info(f"â° ë¶„ì„ ì‹œê°„: {start_time}ì´ˆ ~ {end_time}ì´ˆ")
            
            # í•´ë‹¹ ì‹œê°„ëŒ€ì˜ í”„ë ˆì„ë“¤ ë¶„ì„
            analysis_result = self._perform_time_based_analysis(
                video, start_time, end_time, analysis_type
            )
            
            logger.info(f"âœ… ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì™„ë£Œ")
            
            return Response({
                'video_id': video_id,
                'time_range': time_range,
                'analysis_type': analysis_type,
                'result': analysis_result,
                'search_type': 'time_analysis'
            })
            
        except Exception as e:
            logger.error(f"âŒ ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _perform_time_based_analysis(self, video, start_time, end_time, analysis_type):
        """ì‹œê°„ëŒ€ë³„ ë¶„ì„ ìˆ˜í–‰"""
        
        # í•´ë‹¹ ì‹œê°„ëŒ€ í”„ë ˆì„ë“¤ ê°€ì ¸ì˜¤ê¸°
        frames = Frame.objects.filter(
            video=video,
            timestamp__gte=start_time,
            timestamp__lte=end_time
        ).order_by('timestamp')
        
        frame_list = list(frames)
        logger.info(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ í”„ë ˆì„: {len(frame_list)}ê°œ")
        
        if 'ì„±ë¹„' in analysis_type or 'ì‚¬ëŒ' in analysis_type:
            return self._analyze_gender_distribution(frame_list, start_time, end_time)
        elif 'ì°¨ëŸ‰' in analysis_type or 'êµí†µ' in analysis_type:
            return self._analyze_vehicle_distribution(frame_list, start_time, end_time)
        else:
            return self._analyze_general_statistics(frame_list, start_time, end_time)
    
    def _analyze_gender_distribution(self, frames, start_time, end_time):
        """ì„±ë¹„ ë¶„ì„"""
        person_detections = []
        
        for frame in frames:
            if not hasattr(frame, 'detected_objects') or not frame.detected_objects:
                continue
                
            for obj in frame.detected_objects:
                if obj.get('class') == 'person':
                    person_detections.append({
                        'timestamp': frame.timestamp,
                        'confidence': obj.get('confidence', 0.5),
                        'bbox': obj.get('bbox', []),
                        'colors': obj.get('colors', []),
                        'color_description': obj.get('color_description', '')
                    })
        
        # ì„±ë³„ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ AI ëª¨ë¸ í•„ìš”)
        male_count = 0
        female_count = 0
        
        for detection in person_detections:
            # ìƒ‰ìƒ ê¸°ë°˜ ê°„ë‹¨í•œ ì„±ë³„ ì¶”ì •
            colors = detection['color_description'].lower()
            if 'blue' in colors or 'black' in colors or 'gray' in colors:
                male_count += 1
            elif 'pink' in colors or 'red' in colors:
                female_count += 1
            else:
                # 50:50ìœ¼ë¡œ ë¶„ë°°
                if len(person_detections) % 2 == 0:
                    male_count += 1
                else:
                    female_count += 1
        
        total_persons = male_count + female_count
        
        # ì˜ìƒ ìƒ‰ìƒ ë¶„í¬
        clothing_colors = {}
        for detection in person_detections:
            color = detection['color_description']
            if color and color != 'unknown':
                clothing_colors[color] = clothing_colors.get(color, 0) + 1
        
        # í”¼í¬ ì‹œê°„ëŒ€ ë¶„ì„
        time_distribution = {}
        for detection in person_detections:
            time_bucket = int(detection['timestamp'] // 30) * 30  # 30ì´ˆ ë‹¨ìœ„
            time_distribution[time_bucket] = time_distribution.get(time_bucket, 0) + 1
        
        peak_times = sorted(time_distribution.items(), key=lambda x: x[1], reverse=True)[:2]
        peak_time_strings = [f"{self._seconds_to_time_string(t[0])}-{self._seconds_to_time_string(t[0]+30)}" 
                           for t in peak_times]
        
        return {
            'total_persons': total_persons,
            'male_count': male_count,
            'female_count': female_count,
            'gender_ratio': {
                'male': round((male_count / total_persons * 100), 1) if total_persons > 0 else 0,
                'female': round((female_count / total_persons * 100), 1) if total_persons > 0 else 0
            },
            'clothing_colors': dict(sorted(clothing_colors.items(), key=lambda x: x[1], reverse=True)),
            'peak_times': peak_time_strings,
            'movement_patterns': 'left_to_right_dominant',  # ê°„ë‹¨í•œ ì˜ˆì‹œ
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _analyze_vehicle_distribution(self, frames, start_time, end_time):
        """ì°¨ëŸ‰ ë¶„í¬ ë¶„ì„"""
        vehicles = []
        
        for frame in frames:
            if not hasattr(frame, 'detected_objects') or not frame.detected_objects:
                continue
                
            for obj in frame.detected_objects:
                if obj.get('class') in ['car', 'truck', 'bus', 'motorcycle']:
                    vehicles.append({
                        'type': obj.get('class'),
                        'timestamp': frame.timestamp,
                        'confidence': obj.get('confidence', 0.5)
                    })
        
        vehicle_types = {}
        for v in vehicles:
            vehicle_types[v['type']] = vehicle_types.get(v['type'], 0) + 1
        
        duration_minutes = (end_time - start_time) / 60
        
        return {
            'total_vehicles': len(vehicles),
            'vehicle_types': vehicle_types,
            'average_per_minute': round(len(vehicles) / max(1, duration_minutes), 1),
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _analyze_general_statistics(self, frames, start_time, end_time):
        """ì¼ë°˜ í†µê³„ ë¶„ì„"""
        all_objects = []
        
        for frame in frames:
            if hasattr(frame, 'detected_objects') and frame.detected_objects:
                all_objects.extend(frame.detected_objects)
        
        object_counts = {}
        for obj in all_objects:
            obj_class = obj.get('class', 'unknown')
            object_counts[obj_class] = object_counts.get(obj_class, 0) + 1
        
        return {
            'total_objects': len(all_objects),
            'object_distribution': dict(sorted(object_counts.items(), key=lambda x: x[1], reverse=True)),
            'frames_analyzed': len(frames),
            'average_objects_per_frame': round(len(all_objects) / max(1, len(frames)), 1),
            'analysis_period': f"{self._seconds_to_time_string(start_time)} - {self._seconds_to_time_string(end_time)}"
        }
    
    def _parse_time_to_seconds(self, time_str):
        """ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆë¡œ ë³€í™˜"""
        try:
            if ':' in time_str:
                parts = time_str.split(':')
                minutes = int(parts[0])
                seconds = int(parts[1]) if len(parts) > 1 else 0
                return minutes * 60 + seconds
            else:
                return int(time_str)
        except:
            return 0
    
    def _seconds_to_time_string(self, seconds):
        """ì´ˆë¥¼ ì‹œê°„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes}:{secs:02d}"


@method_decorator(csrf_exempt, name='dispatch')
class CrossVideoSearchView(APIView):
    """ì˜ìƒ ê°„ ê²€ìƒ‰ - ìˆ˜ì •ëœ ë²„ì „"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            search_filters = request.data.get('filters', {})
            
            logger.info(f"ğŸ” í¬ë¡œìŠ¤ ë¹„ë””ì˜¤ ê²€ìƒ‰ ìš”ì²­: '{query}'")
            
            if not query:
                return Response({'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}, status=400)
            
            # ì¿¼ë¦¬ ë¶„ì„
            query_analysis = self._analyze_query(query)
            
            # ë¶„ì„ëœ ë¹„ë””ì˜¤ë“¤ ì¤‘ì—ì„œ ê²€ìƒ‰
            videos = Video.objects.filter(is_analyzed=True)
            matching_videos = []
            
            for video in videos:
                match_score = self._calculate_video_match_score(video, query_analysis, search_filters)
                if match_score > 0.3:  # ì„ê³„ê°’
                    matching_videos.append({
                        'video_id': video.id,
                        'video_name': video.original_name,
                        'match_score': match_score,
                        'match_reasons': self._get_match_reasons(video, query_analysis),
                        'metadata': self._get_video_metadata(video),
                        'thumbnail_url': f'/frame/{video.id}/100/',
                    })
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            matching_videos.sort(key=lambda x: x['match_score'], reverse=True)
            
            logger.info(f"âœ… í¬ë¡œìŠ¤ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì™„ë£Œ: {len(matching_videos)}ê°œ ê²°ê³¼")
            
            return Response({
                'query': query,
                'total_matches': len(matching_videos),
                'results': matching_videos[:20],  # ìƒìœ„ 20ê°œ
                'query_analysis': query_analysis,
                'search_type': 'cross_video'
            })
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡œìŠ¤ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _analyze_query(self, query):
        """ì¿¼ë¦¬ì—ì„œ ë‚ ì”¨, ì‹œê°„ëŒ€, ì¥ì†Œ ë“± ì¶”ì¶œ"""
        analysis = {
            'weather': None,
            'time_of_day': None,
            'location': None,
            'objects': [],
            'activities': []
        }
        
        query_lower = query.lower()
        
        # ë‚ ì”¨ í‚¤ì›Œë“œ
        weather_keywords = {
            'ë¹„': 'rainy', 'ë¹„ê°€': 'rainy', 'ìš°ì²œ': 'rainy',
            'ë§‘ì€': 'sunny', 'í™”ì°½í•œ': 'sunny', 'í–‡ë¹›': 'sunny',
            'íë¦°': 'cloudy', 'êµ¬ë¦„': 'cloudy'
        }
        
        # ì‹œê°„ëŒ€ í‚¤ì›Œë“œ
        time_keywords = {
            'ë°¤': 'night', 'ì•¼ê°„': 'night', 'ì €ë…': 'evening',
            'ë‚®': 'day', 'ì˜¤í›„': 'afternoon', 'ì•„ì¹¨': 'morning'
        }
        
        # ì¥ì†Œ í‚¤ì›Œë“œ
        location_keywords = {
            'ì‹¤ë‚´': 'indoor', 'ê±´ë¬¼': 'indoor', 'ë°©': 'indoor',
            'ì‹¤ì™¸': 'outdoor', 'ë„ë¡œ': 'outdoor', 'ê±°ë¦¬': 'outdoor'
        }
        
        for keyword, value in weather_keywords.items():
            if keyword in query_lower:
                analysis['weather'] = value
                break
        
        for keyword, value in time_keywords.items():
            if keyword in query_lower:
                analysis['time_of_day'] = value
                break
                
        for keyword, value in location_keywords.items():
            if keyword in query_lower:
                analysis['location'] = value
                break
        
        return analysis
    
    def _calculate_video_match_score(self, video, query_analysis, filters):
        """ë¹„ë””ì˜¤ì™€ ì¿¼ë¦¬ ê°„ì˜ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        reasons = []
        
        try:
            # í”„ë ˆì„ ë°ì´í„°ì—ì„œ ì§ì ‘ ë¶„ì„
            frames = Frame.objects.filter(video=video)[:10]
            
            if frames:
                # ë‚ ì”¨ ë§¤ì¹­
                if query_analysis.get('weather'):
                    weather_score = self._check_weather_match(frames, query_analysis['weather'])
                    if weather_score > 0:
                        score += weather_score
                        reasons.append(f"ë‚ ì”¨: {query_analysis['weather']}")
                
                # ì‹œê°„ëŒ€ ë§¤ì¹­
                if query_analysis.get('time_of_day'):
                    time_score = self._check_time_match(video, query_analysis['time_of_day'])
                    if time_score > 0:
                        score += time_score
                        reasons.append(f"ì‹œê°„ëŒ€: {query_analysis['time_of_day']}")
                
                # ì¥ì†Œ ë§¤ì¹­
                if query_analysis.get('location'):
                    location_score = self._check_location_match(frames, query_analysis['location'])
                    if location_score > 0:
                        score += location_score
                        reasons.append(f"ì¥ì†Œ: {query_analysis['location']}")
                
                # ê°ì²´ ë§¤ì¹­
                if query_analysis.get('objects'):
                    for obj in query_analysis['objects']:
                        object_score = self._check_object_match(frames, obj)
                        if object_score > 0:
                            score += object_score
                            reasons.append(f"ê°ì²´: {obj}")
                
                # í™œë™ ë§¤ì¹­
                if query_analysis.get('activities'):
                    for activity in query_analysis['activities']:
                        activity_score = self._check_activity_match(frames, activity)
                        if activity_score > 0:
                            score += activity_score
                            reasons.append(f"í™œë™: {activity}")
            
            # VideoAnalysisì—ì„œ ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° (ë³´ì¡°)
            elif hasattr(video, 'analysis'):
                analysis = video.analysis
                stats = analysis.analysis_statistics
                scene_types = stats.get('scene_types', [])
                
                # ë‚ ì”¨ ë§¤ì¹­
                if query_analysis.get('weather'):
                    weather_scenes = [s for s in scene_types if query_analysis['weather'] in s.lower()]
                    if weather_scenes:
                        score += 0.4
                        reasons.append(f"ë‚ ì”¨: {query_analysis['weather']}")
                
                # ì‹œê°„ëŒ€ ë§¤ì¹­
                if query_analysis.get('time_of_day'):
                    time_scenes = [s for s in scene_types if query_analysis['time_of_day'] in s.lower()]
                    if time_scenes:
                        score += 0.3
                        reasons.append(f"ì‹œê°„ëŒ€: {query_analysis['time_of_day']}")
                
                # ì¥ì†Œ ë§¤ì¹­
                if query_analysis.get('location'):
                    location_scenes = [s for s in scene_types if query_analysis['location'] in s.lower()]
                    if location_scenes:
                        score += 0.3
                        reasons.append(f"ì¥ì†Œ: {query_analysis['location']}")
            
            return min(score, 1.0)
            
        except Exception:
            return 0.0
    
    def _get_match_reasons(self, video, query_analysis):
        """ë§¤ì¹­ ì´ìœ  ìƒì„±"""
        reasons = []
        
        if query_analysis['weather']:
            reasons.append(f"{query_analysis['weather']} ë‚ ì”¨ ì¡°ê±´")
        if query_analysis['time_of_day']:
            reasons.append(f"{query_analysis['time_of_day']} ì‹œê°„ëŒ€")
        if query_analysis['location']:
            reasons.append(f"{query_analysis['location']} í™˜ê²½")
            
        return reasons
    
    def _get_video_metadata(self, video):
        """ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        metadata = {
            'duration': video.duration,
            'file_size': video.file_size,
            'uploaded_at': video.uploaded_at.isoformat(),
            'analysis_type': 'basic'
        }
        
        if hasattr(video, 'analysis'):
            stats = video.analysis.analysis_statistics
            metadata.update({
                'analysis_type': stats.get('analysis_type', 'basic'),
                'scene_types': stats.get('scene_types', []),
                'dominant_objects': stats.get('dominant_objects', [])
            })
        
        return metadata


class AdvancedSearchAutoView(APIView):
    """í†µí•© ê³ ê¸‰ ê²€ìƒ‰ - ìë™ íƒ€ì… ê°ì§€"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            query = request.data.get('query', '').strip()
            video_id = request.data.get('video_id')
            time_range = request.data.get('time_range', {})
            options = request.data.get('options', {})
            
            if not query:
                return Response({'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}, status=400)
            
            # ê²€ìƒ‰ íƒ€ì… ìë™ ê°ì§€
            search_type = self._detect_search_type(query, video_id, time_range, options)
            
            # í•´ë‹¹ ê²€ìƒ‰ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ View í˜¸ì¶œ
            if search_type == 'cross-video':
                view = CrossVideoSearchView()
                return view.post(request)
            elif search_type == 'object-tracking':
                view = IntraVideoTrackingView()
                return view.post(request)
            elif search_type == 'time-analysis':
                view = TimeBasedAnalysisView()
                return view.post(request)
            else:
                # ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ fallback
                view = EnhancedVideoChatView()
                return view.post(request)
                
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _detect_search_type(self, query, video_id, time_range, options):
        """ê²€ìƒ‰ íƒ€ì… ìë™ ê°ì§€ ë¡œì§"""
        query_lower = query.lower()
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„ í‚¤ì›Œë“œ
        time_analysis_keywords = [
            'ì„±ë¹„', 'ë¶„í¬', 'í†µê³„', 'ì‹œê°„ëŒ€', 'êµ¬ê°„', 'ì‚¬ì´', 
            'ëª‡ëª…', 'ì–¼ë§ˆë‚˜', 'í‰ê· ', 'ë¹„ìœ¨', 'íŒ¨í„´', 'ë¶„ì„'
        ]
        
        # ê°ì²´ ì¶”ì  í‚¤ì›Œë“œ
        tracking_keywords = [
            'ì¶”ì ', 'ë”°ë¼ê°€', 'ì´ë™', 'ê²½ë¡œ', 'ì§€ë‚˜ê°„', 
            'ìƒì˜', 'ëª¨ì', 'ìƒ‰ê¹”', 'ì˜·', 'ì‚¬ëŒ', 'ì°¨ëŸ‰'
        ]
        
        # ì˜ìƒ ê°„ ê²€ìƒ‰ í‚¤ì›Œë“œ
        cross_video_keywords = [
            'ì´¬ì˜ëœ', 'ì˜ìƒ', 'ë¹„ë””ì˜¤', 'ì°¾ì•„', 'ë¹„ê°€', 'ë°¤', 
            'ë‚®', 'ì‹¤ë‚´', 'ì‹¤ì™¸', 'ì¥ì†Œ', 'ë‚ ì”¨'
        ]
        
        # ì‹œê°„ ë²”ìœ„ê°€ ìˆê³  ë¶„ì„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì‹œê°„ëŒ€ë³„ ë¶„ì„
        if (time_range.get('start') and time_range.get('end')) or \
           any(keyword in query_lower for keyword in time_analysis_keywords):
            return 'time-analysis'
        
        # íŠ¹ì • ë¹„ë””ì˜¤ IDê°€ ìˆê³  ì¶”ì  í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°ì²´ ì¶”ì 
        if video_id and any(keyword in query_lower for keyword in tracking_keywords):
            return 'object-tracking'
        
        # í¬ë¡œìŠ¤ ë¹„ë””ì˜¤ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì˜ìƒ ê°„ ê²€ìƒ‰
        if any(keyword in query_lower for keyword in cross_video_keywords):
            return 'cross-video'
        
        # ê¸°ë³¸ê°’: ë¹„ë””ì˜¤ IDê°€ ìˆìœ¼ë©´ ì¶”ì , ì—†ìœ¼ë©´ í¬ë¡œìŠ¤ ë¹„ë””ì˜¤
        return 'object-tracking' if video_id else 'cross-video'


# views.pyì— ì¶”ê°€í•  ëˆ„ë½ëœ Viewë“¤


# âœ… LLMStatsView ì¶”ê°€
class LLMStatsView(APIView):
    """LLM ì„±ëŠ¥ í†µê³„ ë·°"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            # ê°„ë‹¨í•œ í†µê³„ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìˆ˜ì§‘)
            stats = {
                'total_requests': 0,
                'model_usage': {
                    'gpt-4v': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'claude-3.5': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'gemini-pro': {'count': 0, 'avg_time': 0, 'success_rate': 0},
                    'groq-llama': {'count': 0, 'avg_time': 0, 'success_rate': 0}
                },
                'average_response_time': 0,
                'overall_success_rate': 0,
                'last_updated': datetime.now().isoformat()
            }
            
            return Response(stats)
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)


# âœ… cleanup_storage í•¨ìˆ˜ ì¶”ê°€
@csrf_exempt
@require_http_methods(["POST"])
def cleanup_storage(request):
    """ì €ì¥ ê³µê°„ ì •ë¦¬"""
    try:
        print("ğŸ§¹ ì €ì¥ ê³µê°„ ì •ë¦¬ ìš”ì²­")
        
        from django.conf import settings
        
        cleaned_files = []
        total_size_freed = 0
        
        # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬ (ì˜ˆì‹œ)
        temp_dirs = [
            os.path.join(settings.MEDIA_ROOT, 'temp'),
            os.path.join(settings.MEDIA_ROOT, 'analysis_temp'),
            '/tmp/video_analysis'
        ]
        
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                try:
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ë‚´ìš© ì •ë¦¬
                    for filename in os.listdir(temp_dir):
                        file_path = os.path.join(temp_dir, filename)
                        if os.path.isfile(file_path):
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            cleaned_files.append(filename)
                            total_size_freed += file_size
                except Exception as e:
                    print(f"âš ï¸ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {temp_dir} - {e}")
        
        # ì˜¤ë˜ëœ ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ ì •ë¦¬ (ì„ íƒì‚¬í•­)
        analysis_results_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
        if os.path.exists(analysis_results_dir):
            import time
            current_time = time.time()
            for filename in os.listdir(analysis_results_dir):
                file_path = os.path.join(analysis_results_dir, filename)
                if os.path.isfile(file_path):
                    # 7ì¼ ì´ìƒ ëœ íŒŒì¼ë“¤ ì‚­ì œ
                    if current_time - os.path.getmtime(file_path) > 7 * 24 * 3600:
                        try:
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            cleaned_files.append(filename)
                            total_size_freed += file_size
                        except Exception as e:
                            print(f"âš ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {filename} - {e}")
        
        result = {
            'success': True,
            'message': f'ì €ì¥ ê³µê°„ ì •ë¦¬ ì™„ë£Œ: {len(cleaned_files)}ê°œ íŒŒì¼ ì‚­ì œ',
            'details': {
                'files_cleaned': len(cleaned_files),
                'size_freed_bytes': total_size_freed,
                'size_freed_mb': round(total_size_freed / (1024 * 1024), 2),
                'cleaned_files': cleaned_files[:10],  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                'timestamp': datetime.now().isoformat()
            }
        }
        
        print(f"âœ… ì €ì¥ ê³µê°„ ì •ë¦¬ ì™„ë£Œ: {result}")
        return JsonResponse(result)
        
    except Exception as e:
        print(f"âŒ ì €ì¥ ê³µê°„ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return JsonResponse({
            'success': False,
            'error': str(e),
            'message': 'ì €ì¥ ê³µê°„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
        }, status=500)


# âœ… ëˆ„ë½ëœ ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° ë·°ë“¤

@csrf_exempt  
@require_http_methods(["GET"])
def check_video_exists(request, video_id):
    """ë¹„ë””ì˜¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        Video.objects.get(id=video_id)
        return JsonResponse({
            'exists': True,
            'video_id': video_id
        })
    except Video.DoesNotExist:
        return JsonResponse({
            'exists': False,
            'video_id': video_id
        })


# âœ… FrameWithBboxView - ë°”ìš´ë”© ë°•ìŠ¤ê°€ ìˆëŠ” í”„ë ˆì„ ë·°
class FrameWithBboxView(APIView):
    permission_classes = [AllowAny]
    
    def get(self, request, video_id, frame_number):
        try:
            print(f"ğŸ–¼ï¸ ë°”ìš´ë”© ë°•ìŠ¤ í”„ë ˆì„ ìš”ì²­: ë¹„ë””ì˜¤={video_id}, í”„ë ˆì„={frame_number}")
            
            video = Video.objects.get(id=video_id)
            frame = Frame.objects.get(video=video, image_id=frame_number)
            
            # ë””ë²„ê¹…: detected_objects í™•ì¸
            print(f"ğŸ” Frame {frame_number} detected_objects: {frame.detected_objects}")
            
            if not frame.detected_objects:
                print("âš ï¸ detected_objectsê°€ ì—†ìŠµë‹ˆë‹¤")
                # ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
                return self._get_original_frame(video, frame_number)
            
            # detected_objects íŒŒì‹±
            detected_objects = frame.detected_objects
            if isinstance(detected_objects, str):
                import json
                detected_objects = json.loads(detected_objects)
            
            if not isinstance(detected_objects, list):
                detected_objects = detected_objects.get('objects', []) if isinstance(detected_objects, dict) else []
            
            print(f"ğŸ“¦ íŒŒì‹±ëœ ê°ì²´ ìˆ˜: {len(detected_objects)}")
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            image_data = self._draw_bboxes_on_frame(video, frame_number, detected_objects)
            
            return HttpResponse(image_data, content_type='image/jpeg')
            
        except Video.DoesNotExist:
            return HttpResponse(status=404)
        except Frame.DoesNotExist:
            print(f"âš ï¸ Frame {frame_number} not found")
            return HttpResponse(status=404)
        except Exception as e:
            print(f"âŒ ë°•ìŠ¤ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
            import traceback
            print(traceback.format_exc())
            return HttpResponse(status=500)
    def _draw_bboxes_on_frame(self, video, frame_number, detected_objects):
        """í”„ë ˆì„ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            import cv2
            import io
            import numpy as np
            import os
            
            # ğŸ”§ ìˆ˜ì •: file.path ëŒ€ì‹  file_path í•„ë“œ ì‚¬ìš©
            video_path = video.file_path
            
            # íŒŒì¼ ê²½ë¡œê°€ ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not os.path.isabs(video_path):
                from django.conf import settings
                # MEDIA_ROOTë‚˜ ì ì ˆí•œ base pathì™€ ê²°í•©
                video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            
            print(f"ğŸ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ: {video_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(video_path):
                print(f"âš ï¸ ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {video_path}")
                return self._create_dummy_image_with_boxes(frame_number, detected_objects)
            
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                print(f"âš ï¸ ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
                cap.release()
                return self._create_dummy_image_with_boxes(frame_number, detected_objects)
            
            # í”„ë ˆì„ ë²ˆí˜¸ë¡œ ì´ë™ (0-based index)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                print(f"âš ï¸ í”„ë ˆì„ {frame_number} ì½ê¸° ì‹¤íŒ¨, ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±")
                return self._create_dummy_image_with_boxes(frame_number, detected_objects)
            
            # OpenCV ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(frame_rgb)
            
            # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
            img_width, img_height = image.size
            print(f"ğŸ–¼ï¸ ì‹¤ì œ ì´ë¯¸ì§€ í¬ê¸°: {img_width}x{img_height}")
            
            draw = ImageDraw.Draw(image)
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'cyan', 'magenta']
            
            for i, obj in enumerate(detected_objects):
                bbox = obj.get('bbox', [])
                obj_class = obj.get('class', 'object')
                confidence = obj.get('confidence', 0)
                track_id = obj.get('track_id', '')
                color_description = obj.get('color_description', '')
                
                if len(bbox) == 4:
                    # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                    x1_norm, y1_norm, x2_norm, y2_norm = bbox
                    
                    x1 = int(x1_norm * img_width)
                    y1 = int(y1_norm * img_height)
                    x2 = int(x2_norm * img_width)
                    y2 = int(y2_norm * img_height)
                    
                    # ì¢Œí‘œ ìœ íš¨ì„± ê²€ì‚¬
                    x1 = max(0, min(x1, img_width))
                    y1 = max(0, min(y1, img_height))
                    x2 = max(0, min(x2, img_width))
                    y2 = max(0, min(y2, img_height))
                    
                    color = colors[i % len(colors)]
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
                    
                    # ë ˆì´ë¸” ê·¸ë¦¬ê¸°
                    label_parts = [obj_class]
                    if track_id:
                        label_parts.append(f"ID:{track_id}")
                    if color_description:
                        label_parts.append(color_description)
                    label_parts.append(f"{confidence:.2f}")
                    
                    label = " | ".join(label_parts)
                    
                    # ë ˆì´ë¸” ë°°ê²½ ì¶”ê°€ (ê°€ë…ì„± í–¥ìƒ)
                    label_bbox = draw.textbbox((x1, y1-20), label)
                    draw.rectangle(label_bbox, fill=color, outline=color)
                    draw.text((x1, y1-20), label, fill='white')
            
            # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG', quality=90)
            print(f"âœ… ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ (ê°ì²´ ìˆ˜: {len(detected_objects)})")
            return buffer.getvalue()
            
        except Exception as e:
            print(f"âŒ í”„ë ˆì„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            print(traceback.format_exc())
            
            # í´ë°±: ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
            return self._create_dummy_image_with_boxes(frame_number, detected_objects)

    def _create_dummy_image_with_boxes(self, frame_number, detected_objects):
        """ë”ë¯¸ ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ í‘œì‹œ"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            import io
            
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            image = Image.new('RGB', (640, 480), color='lightgray')
            draw = ImageDraw.Draw(image)
            
            # ì œëª© ê·¸ë¦¬ê¸°
            draw.text((10, 10), f"Frame {frame_number} - Video File Not Found", fill='black')
            
            # ê°ì§€ëœ ê°ì²´ ì •ë³´ í‘œì‹œ
            y_offset = 40
            for i, obj in enumerate(detected_objects):
                obj_class = obj.get('class', 'object')
                confidence = obj.get('confidence', 0)
                track_id = obj.get('track_id', '')
                color_desc = obj.get('color_description', '')
                
                info_text = f"{i+1}. {obj_class}"
                if track_id:
                    info_text += f" (ID:{track_id})"
                if color_desc:
                    info_text += f" - {color_desc}"
                info_text += f" ({confidence:.2f})"
                
                draw.text((10, y_offset), info_text, fill='black')
                y_offset += 20
                
                if y_offset > 450:  # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ì—ì„œ í‘œì‹œ
                    break
            
            # ë°”ì´íŠ¸ë¡œ ë³€í™˜
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG', quality=90)
            return buffer.getvalue()
            
        except Exception as e:
            print(f"âŒ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±ë„ ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ê°„ë‹¨í•œ ì˜¤ë¥˜ ì´ë¯¸ì§€
            try:
                image = Image.new('RGB', (320, 240), color='red')
                draw = ImageDraw.Draw(image)
                draw.text((10, 10), "Error", fill='white')
                buffer = io.BytesIO()
                image.save(buffer, format='JPEG', quality=50)
                return buffer.getvalue()
            except:
                raise Exception("ì´ë¯¸ì§€ ìƒì„± ì™„ì „ ì‹¤íŒ¨")

    def _get_original_frame(self, video, frame_number):
        """ì›ë³¸ í”„ë ˆì„ ë°˜í™˜"""
        try:
            import cv2
            import io
            from PIL import Image
            import os
            
            # ğŸ”§ ìˆ˜ì •: file.path ëŒ€ì‹  file_path í•„ë“œ ì‚¬ìš©
            video_path = video.file_path
            
            # íŒŒì¼ ê²½ë¡œê°€ ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not os.path.isabs(video_path):
                from django.conf import settings
                video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            
            if not os.path.exists(video_path):
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
                image = Image.new('RGB', (640, 480), color='lightgray')
                draw = ImageDraw.Draw(image)
                draw.text((10, 10), f"Frame {frame_number} - No Detections", fill='black')
                
                buffer = io.BytesIO()
                image.save(buffer, format='JPEG', quality=90)
                return HttpResponse(buffer.getvalue(), content_type='image/jpeg')
            
            cap = cv2.VideoCapture(video_path)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)
            ret, frame = cap.read()
            cap.release()
            
            if ret:
                # OpenCV ì´ë¯¸ì§€ë¥¼ JPEGë¡œ ì¸ì½”ë”©
                _, buffer = cv2.imencode('.jpg', frame)
                return HttpResponse(buffer.tobytes(), content_type='image/jpeg')
            else:
                # í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨ì‹œ ë”ë¯¸ ì´ë¯¸ì§€
                image = Image.new('RGB', (640, 480), color='lightgray')
                buffer = io.BytesIO()
                image.save(buffer, format='JPEG', quality=90)
                return HttpResponse(buffer.getvalue(), content_type='image/jpeg')
                
        except Exception as e:
            print(f"âŒ ì›ë³¸ í”„ë ˆì„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨
            image = Image.new('RGB', (320, 240), color='red')
            buffer = io.BytesIO()
            image.save(buffer, format='JPEG', quality=50)
            return HttpResponse(buffer.getvalue(), content_type='image/jpeg')
# âœ… EnhancedFrameView - ê³ ê¸‰ í”„ë ˆì„ ë·°  
class EnhancedFrameView(FrameView):
    """ê¸°ì¡´ FrameViewë¥¼ í™•ì¥í•œ ê³ ê¸‰ í”„ë ˆì„ View"""
    
    def get(self, request, video_id, frame_number):
        try:
            # ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œ ì˜µì…˜ í™•ì¸
            show_bbox = request.GET.get('bbox', '').lower() in ['true', '1', 'yes']
            
            if show_bbox:
                # ë°”ìš´ë”© ë°•ìŠ¤ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ ë°˜í™˜
                bbox_view = FrameWithBboxView()
                return bbox_view.get(request, video_id, frame_number)
            else:
                # ê¸°ë³¸ í”„ë ˆì„ ë°˜í™˜
                return super().get(request, video_id, frame_number)
                
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ í”„ë ˆì„ ë·° ì˜¤ë¥˜: {e}")
            return super().get(request, video_id, frame_number)


from django.db.models import Sum, Count, Avg
from django.utils import timezone
from datetime import timedelta

class CostManagementView(APIView):
    """ë¹„ìš© ê´€ë¦¬ ë° ë¶„ì„ ë·°"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            # ì „ì²´ ë¹„ìš© í†µê³„
            total_videos = Video.objects.count()
            analyzed_videos = Video.objects.filter(image_analysis_completed=True).count()
            
            # ì´ ë¹„ìš© ê³„ì‚°
            total_cost = 0
            total_chats = 0
            
            for video in Video.objects.all():
                cost_info = video.get_analysis_cost_summary()
                total_cost += cost_info['estimated_cost']
                total_chats += video.total_chat_count
            
            # ìµœê·¼ 7ì¼ ë¹„ìš© ì¶”ì´
            end_date = timezone.now().date()
            start_date = end_date - timedelta(days=7)
            
            daily_costs = []
            for i in range(7):
                date = start_date + timedelta(days=i)
                daily_analysis = CostAnalysis.get_daily_summary(date)
                daily_costs.append({
                    'date': date.isoformat(),
                    'cost': daily_analysis.estimated_total_cost if daily_analysis else 0.0,
                    'api_calls': daily_analysis.total_api_calls if daily_analysis else 0
                })
            
            # ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰ í†µê³„
            model_stats = {}
            for video in Video.objects.filter(image_analysis_completed=True):
                models_used = video.api_cost_tracking.get('models_used', [])
                for model in models_used:
                    model_stats[model] = model_stats.get(model, 0) + 1
            
            # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
            avg_cost_per_video = total_cost / max(analyzed_videos, 1)
            avg_cost_per_chat = total_cost / max(total_chats, 1)
            
            # ì ˆì•½ ì¶”ì •
            without_optimization_cost = analyzed_videos * 0.10  # ë§¤ë²ˆ ì´ë¯¸ì§€ ë¶„ì„í–ˆë‹¤ë©´
            current_cost = total_cost
            savings = max(0, without_optimization_cost - current_cost)
            savings_percentage = (savings / without_optimization_cost * 100) if without_optimization_cost > 0 else 0
            
            return Response({
                'summary': {
                    'total_videos': total_videos,
                    'analyzed_videos': analyzed_videos,
                    'total_cost_usd': round(total_cost, 4),
                    'total_chats': total_chats,
                    'avg_cost_per_video': round(avg_cost_per_video, 4),
                    'avg_cost_per_chat': round(avg_cost_per_chat, 4)
                },
                'optimization_impact': {
                    'estimated_savings_usd': round(savings, 4),
                    'savings_percentage': round(savings_percentage, 1),
                    'optimization_strategy': 'first_chat_only_image_analysis'
                },
                'daily_trend': daily_costs,
                'model_usage': model_stats,
                'recommendations': self._get_cost_recommendations(total_cost, analyzed_videos, model_stats)
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)
    
    def _get_cost_recommendations(self, total_cost, analyzed_videos, model_stats):
        """ë¹„ìš© ìµœì í™” ì¶”ì²œì‚¬í•­"""
        recommendations = []
        
        if total_cost > 5.0:  # $5 ì´ìƒì¸ ê²½ìš°
            recommendations.append({
                'type': 'cost_alert',
                'message': 'ì´ ë¹„ìš©ì´ $5ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.',
                'priority': 'high'
            })
        
        if 'gpt-4' in str(model_stats):
            recommendations.append({
                'type': 'model_optimization',
                'message': 'GPT-4 ëŒ€ì‹  GPT-4o-mini ì‚¬ìš©ì„ ê³ ë ¤í•´ë³´ì„¸ìš”. ë¹„ìš©ì„ 90% ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                'priority': 'medium'
            })
        
        if analyzed_videos > 50:
            recommendations.append({
                'type': 'usage_optimization',
                'message': 'ë§ì€ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. RAG ì‹œìŠ¤í…œì„ í™œìš©í•´ ë‹µë³€ í’ˆì§ˆì„ ë†’ì´ì„¸ìš”.',
                'priority': 'low'
            })
        
        return recommendations


class VideoAnalysisStatusView(APIView):
    """ë¹„ë””ì˜¤ë³„ ë¶„ì„ ìƒíƒœ ë° ë¹„ìš© ì •ë³´"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            videos_data = []
            
            # ë¹„ë””ì˜¤ ë¶„ì„ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            from .services.video_analysis_service import get_video_analysis_service
            video_service = get_video_analysis_service()
            
            for video in Video.objects.all().order_by('-uploaded_at'):
                cost_summary = video.get_analysis_cost_summary()
                
                # ìƒˆë¡œìš´ ë¶„ì„ ìƒíƒœ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                analysis_status = video_service.get_analysis_status(video.id)
                
                videos_data.append({
                    'id': video.id,
                    'name': video.original_name,
                    'duration': video.duration,
                    'uploaded_at': video.uploaded_at.isoformat(),
                    'image_analysis_completed': video.image_analysis_completed,
                    'image_analysis_date': video.image_analysis_date.isoformat() if video.image_analysis_date else None,
                    'total_chats': video.total_chat_count,
                    'cost_summary': cost_summary,
                    'has_json_analysis': bool(video.chat_analysis_json_path and os.path.exists(video.chat_analysis_json_path)),
                    'json_path': video.chat_analysis_json_path,
                    'analysis_status': video.analysis_status,
                    # ìƒˆë¡œìš´ ë¶„ì„ ì •ë³´ ì¶”ê°€
                    'enhanced_analysis': analysis_status.get('enhanced_analysis', False),
                    'success_rate': analysis_status.get('success_rate', 0.0),
                    'processing_time': analysis_status.get('processing_time', 0),
                    'analysis_type': analysis_status.get('analysis_type', 'unknown'),
                    'unique_objects': analysis_status.get('unique_objects', 0)
                })
            
            return Response({
                'videos': videos_data,
                'total_count': len(videos_data),
                'summary': {
                    'with_image_analysis': sum(1 for v in videos_data if v['image_analysis_completed']),
                    'total_cost': sum(v['cost_summary']['estimated_cost'] for v in videos_data),
                    'total_chats': sum(v['total_chats'] for v in videos_data)
                }
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)


class ResetVideoAnalysisView(APIView):
    """ë¹„ë””ì˜¤ ë¶„ì„ ìƒíƒœ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸/ê´€ë¦¬ìš©)"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            reset_costs = request.data.get('reset_costs', False)
            
            if not video_id:
                return Response({'error': 'video_idê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # ë¶„ì„ ìƒíƒœ ë¦¬ì…‹
            video.image_analysis_completed = False
            video.image_analysis_date = None
            
            # JSON íŒŒì¼ ì‚­ì œ
            if video.chat_analysis_json_path and os.path.exists(video.chat_analysis_json_path):
                try:
                    os.remove(video.chat_analysis_json_path)
                    print(f"âœ… JSON íŒŒì¼ ì‚­ì œ: {video.chat_analysis_json_path}")
                except Exception as e:
                    print(f"âš ï¸ JSON íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            video.chat_analysis_json_path = ''
            
            # ë¹„ìš© ì •ë³´ ë¦¬ì…‹ (ì˜µì…˜)
            if reset_costs:
                video.total_chat_count = 0
                video.api_cost_tracking = {}
            
            video.save()
            
            # VideoAnalysisì—ì„œ ì´ë¯¸ì§€ ë¶„ì„ ì •ë³´ ì œê±°
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                stats = analysis.analysis_statistics
                stats.pop('image_analysis_completed', None)
                stats.pop('image_analysis_date', None)
                stats.pop('json_file_path', None)
                analysis.analysis_statistics = stats
                analysis.save()
            
            return Response({
                'success': True,
                'message': f'ë¹„ë””ì˜¤ "{video.original_name}"ì˜ ë¶„ì„ ìƒíƒœê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'video_id': video_id,
                'reset_costs': reset_costs
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=500)


class CostOptimizationTipsView(APIView):
    """ë¹„ìš© ìµœì í™” íŒ ë° ê°€ì´ë“œ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        return Response({
            'optimization_strategies': {
                'current_implementation': {
                    'name': 'ì²« ì±„íŒ… ì „ìš© ì´ë¯¸ì§€ ë¶„ì„',
                    'description': 'ì²« ë²ˆì§¸ ì±„íŒ…ì—ì„œë§Œ ì´ë¯¸ì§€ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , ì´í›„ ì±„íŒ…ì€ ì €ì¥ëœ ê²°ê³¼ í™œìš©',
                    'cost_reduction': '80-90%',
                    'status': 'active'
                },
                'additional_optimizations': [
                    {
                        'name': 'ëª¨ë¸ ì„ íƒ ìµœì í™”',
                        'description': 'GPT-4 ëŒ€ì‹  GPT-4o-mini ì‚¬ìš© (ì„±ëŠ¥ 90% ìœ ì§€, ë¹„ìš© 90% ì ˆì•½)',
                        'impact': 'high'
                    },
                    {
                        'name': 'ì´ë¯¸ì§€ í’ˆì§ˆ ì¡°ì •',
                        'description': 'ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ 800pxë¡œ ì œí•œí•˜ê³  JPEG í’ˆì§ˆì„ 70%ë¡œ ì„¤ì •',
                        'impact': 'medium'
                    },
                    {
                        'name': 'RAG ì‹œìŠ¤í…œ í™œìš©',
                        'description': 'ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„° DBì— ì €ì¥í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ',
                        'impact': 'medium'
                    },
                    {
                        'name': 'ë°°ì¹˜ ì²˜ë¦¬',
                        'description': 'ì—¬ëŸ¬ ë¹„ë””ì˜¤ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ API í˜¸ì¶œ ìµœì í™”',
                        'impact': 'low'
                    }
                ]
            },
            'cost_estimation': {
                'without_optimization': {
                    'per_chat': '$0.05-0.15',
                    'description': 'ë§¤ ì±„íŒ…ë§ˆë‹¤ ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰'
                },
                'with_optimization': {
                    'first_chat': '$0.05-0.15', 
                    'subsequent_chats': '$0.001-0.005',
                    'description': 'ì²« ì±„íŒ…ë§Œ ì´ë¯¸ì§€ ë¶„ì„, ì´í›„ëŠ” í…ìŠ¤íŠ¸ë§Œ'
                }
            },
            'monitoring_tips': [
                'ì¼ì¼ API ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§',
                'ëª¨ë¸ë³„ ë¹„ìš© íš¨ìœ¨ì„± ì¶”ì ',
                'ì‚¬ìš©ìë³„ ì±„íŒ… íŒ¨í„´ ë¶„ì„',
                'ì›”ë³„ ë¹„ìš© ì˜ˆì‚° ì„¤ì •'
            ]
        })

from django.core.management.base import BaseCommand
from django.utils import timezone
from datetime import timedelta
from chat.models import Video, CostAnalysis

class Command(BaseCommand):
    help = 'ë¹„ìš© ë¶„ì„ ë°ì´í„° ìƒì„± ë° ì—…ë°ì´íŠ¸'
    
    def add_arguments(self, parser):
        parser.add_argument('--days', type=int, default=7, help='ë¶„ì„í•  ì¼ìˆ˜')
        parser.add_argument('--update-monthly', action='store_true', help='ì›”ë³„ ì§‘ê³„ ì—…ë°ì´íŠ¸')
    
    def handle(self, *args, **options):
        days = options['days']
        end_date = timezone.now().date()
        start_date = end_date - timedelta(days=days)
        
        self.stdout.write(f"ğŸ“Š {days}ì¼ê°„ ë¹„ìš© ë¶„ì„ ì¤‘...")
        
        for i in range(days):
            date = start_date + timedelta(days=i)
            
            # í•´ë‹¹ ë‚ ì§œì˜ ë¹„ë””ì˜¤ë“¤ ë¶„ì„
            videos_on_date = Video.objects.filter(
                image_analysis_date__date=date
            )
            
            if videos_on_date.exists():
                total_cost = sum(
                    video.get_analysis_cost_summary()['estimated_cost'] 
                    for video in videos_on_date
                )
                
                total_calls = sum(
                    video.api_cost_tracking.get('total_api_calls', 0)
                    for video in videos_on_date
                )
                
                image_calls = sum(
                    video.api_cost_tracking.get('image_analysis_calls', 0)
                    for video in videos_on_date
                )
                
                # CostAnalysis ë ˆì½”ë“œ ìƒì„±/ì—…ë°ì´íŠ¸
                cost_analysis, created = CostAnalysis.objects.get_or_create(
                    date=date,
                    period_type='daily',
                    defaults={
                        'total_api_calls': total_calls,
                        'image_analysis_calls': image_calls,
                        'text_only_calls': total_calls - image_calls,
                        'estimated_total_cost': total_cost
                    }
                )
                
                if not created:
                    cost_analysis.total_api_calls = total_calls
                    cost_analysis.image_analysis_calls = image_calls
                    cost_analysis.text_only_calls = total_calls - image_calls
                    cost_analysis.estimated_total_cost = total_cost
                    cost_analysis.save()
                
                status = "ìƒì„±ë¨" if created else "ì—…ë°ì´íŠ¸ë¨"
                self.stdout.write(f"  {date}: ${total_cost:.4f} ({total_calls}íšŒ í˜¸ì¶œ) - {status}")
        
        self.stdout.write(self.style.SUCCESS("âœ… ë¹„ìš© ë¶„ì„ ì™„ë£Œ"))


# chat/views.py - ë¹„ìš© ì ˆì•½í˜• ë¹„ë””ì˜¤ ì±„íŒ… ì‹œìŠ¤í…œ

import threading
import time
import json
import cv2
import os
import base64
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient
from .video_analyzer import get_video_analyzer
from .multi_llm_service import get_multi_llm_analyzer
from .db_builder import get_video_rag_system

class CostEffectiveVideoChatView(APIView):
    """ë¹„ìš© ì ˆì•½í˜• ë¹„ë””ì˜¤ ì±„íŒ… - ì²« ì±„íŒ…ì—ë§Œ ì´ë¯¸ì§€ ë¶„ì„, ì´í›„ JSON ê¸°ë°˜ ë‹µë³€"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = get_video_analyzer()
        self.llm_client = LLMClient()
        self.multi_llm_analyzer = get_multi_llm_analyzer()
        self.rag_system = get_video_rag_system()
    
    def post(self, request):
        try:
            user_query = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            analysis_mode = request.data.get('analysis_mode', 'single')
            use_multi_llm = request.data.get('use_multi_llm', analysis_mode != 'single')
            
            print(f"ğŸ¤– ì±„íŒ… ìš”ì²­: '{user_query}' (ë¹„ë””ì˜¤: {video_id}, ëª¨ë“œ: {analysis_mode})")
            
            if not user_query:
                return Response({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}, status=400)
            
            if not video_id:
                return Response({'error': 'ë¹„ë””ì˜¤ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # ğŸ”¥ í•µì‹¬: ì´ë¯¸ì§€ ë¶„ì„ ì´ë ¥ í™•ì¸
            image_analysis_status = self._check_image_analysis_status(video)
            
            if image_analysis_status['needs_analysis']:
                print("ğŸ–¼ï¸ ì²« ë²ˆì§¸ ì±„íŒ… - ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰")
                response = self._handle_first_chat_with_analysis(
                    user_query, video, analysis_mode, use_multi_llm
                )
            else:
                print("ğŸ“„ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œë¨ - JSON ê¸°ë°˜ ë‹µë³€")
                response = self._handle_subsequent_chat_with_json(
                    user_query, video, image_analysis_status['json_path']
                )
            
            # ì±„íŒ… ì´ë ¥ ì €ì¥
            self._save_chat_history(video, user_query, response.get('response', ''))
            
            return Response(response)
            
        except Exception as e:
            print(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return Response({
                'error': f'ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'response': 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                'response_type': 'error'
            }, status=500)
    
    def _check_image_analysis_status(self, video):
        """ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰ ì—¬ë¶€ í™•ì¸"""
        try:
            # 1. Video ëª¨ë¸ì˜ image_analysis_completed í•„ë“œ í™•ì¸
            if video.image_analysis_completed:
                json_path = video.chat_analysis_json_path
                if json_path and os.path.exists(json_path):
                    return {
                        'needs_analysis': False,
                        'json_path': json_path,
                        'analysis_date': video.image_analysis_date
                    }
            
            # 2. VideoAnalysis ëª¨ë¸ì—ì„œ ì´ë¯¸ì§€ ë¶„ì„ ì—¬ë¶€ í™•ì¸
            if hasattr(video, 'analysis'):
                analysis = video.analysis
                stats = analysis.analysis_statistics
                
                # ì´ë¯¸ì§€ ë¶„ì„ì´ ìˆ˜í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if stats.get('image_analysis_completed', False):
                    json_path = stats.get('json_file_path')
                    if json_path and os.path.exists(json_path):
                        return {
                            'needs_analysis': False,
                            'json_path': json_path,
                            'analysis_date': analysis.created_at
                        }
            
            # 3. JSON íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¡œ í™•ì¸
            analysis_results_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            pattern = f"chat_analysis_{video.id}_*.json"
            
            if os.path.exists(analysis_results_dir):
                import glob
                existing_files = glob.glob(os.path.join(analysis_results_dir, pattern))
                
                if existing_files:
                    # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
                    latest_file = max(existing_files, key=os.path.getmtime)
                    return {
                        'needs_analysis': False,
                        'json_path': latest_file,
                        'analysis_date': datetime.fromtimestamp(os.path.getmtime(latest_file))
                    }
            
            # 4. ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
            return {
                'needs_analysis': True,
                'json_path': None,
                'analysis_date': None
            }
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'needs_analysis': True, 'json_path': None, 'analysis_date': None}
# chat/views.py - ë¹„ìš© ì ˆì•½í˜• ì±„íŒ… ë·° ì¶”ê°€

import threading
import time
import json
import cv2
import os
import base64
from datetime import datetime
from django.conf import settings
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from .models import Video, VideoAnalysis, Scene, Frame, SearchHistory
from .llm_client import LLMClient
from .video_analyzer import get_video_analyzer
from .multi_llm_service import get_multi_llm_analyzer

class CostEfficientChatView(APIView):
    """ë¹„ìš© ì ˆì•½í˜• ë¹„ë””ì˜¤ ì±„íŒ… - ì²« ì±„íŒ…ì—ë§Œ ì´ë¯¸ì§€ ë¶„ì„, ì´í›„ JSON ê¸°ë°˜ ë‹µë³€"""
    permission_classes = [AllowAny]
    
    def __init__(self):
        super().__init__()
        self.video_analyzer = get_video_analyzer()
        self.llm_client = LLMClient()
        self.multi_llm_analyzer = get_multi_llm_analyzer()
        
    def post(self, request):
        try:
            user_query = request.data.get('message', '').strip()
            video_id = request.data.get('video_id')
            analysis_mode = request.data.get('analysis_mode', 'single')
            use_multi_llm = request.data.get('use_multi_llm', analysis_mode != 'single')
            
            print(f"ğŸ’° ë¹„ìš©ì ˆì•½ ì±„íŒ… ìš”ì²­: '{user_query}' (ë¹„ë””ì˜¤: {video_id}, ëª¨ë“œ: {analysis_mode})")
            
            if not user_query:
                return Response({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}, status=400)
            
            if not video_id:
                return Response({'error': 'ë¹„ë””ì˜¤ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # ì±„íŒ… ì¹´ìš´íŠ¸ ì¦ê°€
            video.increment_chat_count()
            
            # ğŸ”¥ í•µì‹¬: ì´ë¯¸ì§€ ë¶„ì„ ì´ë ¥ í™•ì¸
            if not video.image_analysis_completed:
                print("ğŸ–¼ï¸ ì²« ë²ˆì§¸ ì±„íŒ… - ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰")
                response = self._handle_first_chat_with_analysis(
                    user_query, video, analysis_mode, use_multi_llm
                )
            else:
                print("ğŸ“„ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œë¨ - JSON ê¸°ë°˜ ë‹µë³€")
                response = self._handle_subsequent_chat_with_json(
                    user_query, video
                )
            
            # ì±„íŒ… ì´ë ¥ ì €ì¥
            self._save_chat_history(video, user_query, response.get('response', ''))
            
            return Response(response)
            
        except Exception as e:
            print(f"âŒ ë¹„ìš©ì ˆì•½ ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return Response({
                'error': f'ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                'response': 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                'response_type': 'error'
            }, status=500)
    
    def _handle_first_chat_with_analysis(self, user_query, video, analysis_mode, use_multi_llm):
        """ì²« ë²ˆì§¸ ì±„íŒ… - ì´ë¯¸ì§€ ë¶„ì„ í¬í•¨"""
        try:
            print("ğŸ–¼ï¸ ì²« ë²ˆì§¸ ì±„íŒ…ì„ ìœ„í•œ ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            
            # í‚¤í”„ë ˆì„ ì¶”ì¶œ (ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ 2-3ê°œë§Œ)
            frame_images = self._extract_key_frames_for_llm(video, max_frames=2)
            
            if not frame_images:
                print("âš ï¸ í‚¤í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨ - ê¸°ì¡´ ë°ì´í„°ë¡œ ë‹µë³€")
                return self._handle_fallback_response(user_query, video)
            
            # ë¹„ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            video_context = self._prepare_video_context(video)
            
            # ë©€í‹° LLM ë¶„ì„ ìˆ˜í–‰
            if use_multi_llm and analysis_mode in ['multi', 'comparison']:
                multi_responses = self.multi_llm_analyzer.analyze_video_multi_llm(
                    frame_images, user_query, video_context
                )
                comparison_result = self.multi_llm_analyzer.compare_responses(multi_responses)
                
                # ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
                analysis_result = {
                    'video_id': video.id,
                    'query': user_query,
                    'analysis_type': 'multi_llm_image_analysis',
                    'frame_count': len(frame_images),
                    'llm_responses': {
                        model: {
                            'response': resp.response_text,
                            'confidence': resp.confidence_score,
                            'processing_time': resp.processing_time,
                            'success': resp.success
                        }
                        for model, resp in multi_responses.items()
                    },
                    'comparison_analysis': comparison_result['comparison'],
                    'timestamp': datetime.now().isoformat(),
                    'video_context': video_context
                }
                
                # JSON íŒŒì¼ ì €ì¥
                json_path = self._save_analysis_result(video, analysis_result)
                
                # ë¹„ë””ì˜¤ì— ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ í‘œì‹œ
                video.mark_image_analysis_completed(json_path)
                
                # ë¹„ìš© ì¶”ì 
                estimated_cost = self._calculate_analysis_cost(analysis_result)
                video.update_cost_tracking('image_analysis', estimated_cost, 'multi_llm')
                
                if analysis_mode == 'comparison':
                    return {
                        'response_type': 'first_chat_multi_llm_comparison',
                        'query': user_query,
                        'video_info': {'id': video.id, 'name': video.original_name},
                        'llm_responses': analysis_result['llm_responses'],
                        'comparison_analysis': analysis_result['comparison_analysis'],
                        'recommendation': comparison_result['comparison']['recommendation'],
                        'cost_info': {
                            'estimated_cost': estimated_cost,
                            'optimization_enabled': True,
                            'future_chats_cost': 'text_only (~$0.001)'
                        }
                    }
                else:
                    best_model = comparison_result['comparison']['best_response']
                    best_response = multi_responses.get(best_model)
                    
                    return {
                        'response_type': 'first_chat_multi_llm_optimized',
                        'response': best_response.response_text if best_response and best_response.success else "ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        'query': user_query,
                        'video_info': {'id': video.id, 'name': video.original_name},
                        'selected_model': best_model,
                        'confidence': best_response.confidence_score if best_response else 0,
                        'models_used': list(multi_responses.keys()),
                        'cost_info': {
                            'estimated_cost': estimated_cost,
                            'optimization_enabled': True
                        }
                    }
            else:
                # ë‹¨ì¼ LLM ë¶„ì„
                single_response = self._analyze_with_single_llm(frame_images[0], user_query, video_context)
                
                analysis_result = {
                    'video_id': video.id,
                    'query': user_query,
                    'analysis_type': 'single_llm_image_analysis', 
                    'frame_count': len(frame_images),
                    'response': single_response,
                    'timestamp': datetime.now().isoformat(),
                    'video_context': video_context
                }
                
                # JSON íŒŒì¼ ì €ì¥
                json_path = self._save_analysis_result(video, analysis_result)
                
                # ë¹„ë””ì˜¤ì— ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ í‘œì‹œ
                video.mark_image_analysis_completed(json_path)
                
                # ë¹„ìš© ì¶”ì 
                estimated_cost = self._calculate_analysis_cost(analysis_result)
                video.update_cost_tracking('image_analysis', estimated_cost, 'single_llm')
                
                return {
                    'response_type': 'first_chat_single_llm',
                    'response': single_response,
                    'query': user_query,
                    'video_info': {'id': video.id, 'name': video.original_name},
                    'cost_info': {
                        'estimated_cost': estimated_cost,
                        'optimization_enabled': True
                    }
                }
                
        except Exception as e:
            print(f"âŒ ì²« ë²ˆì§¸ ì±„íŒ… ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._handle_fallback_response(user_query, video)
    
    def _handle_subsequent_chat_with_json(self, user_query, video):
        """ì´í›„ ì±„íŒ… - JSON íŒŒì¼ ê¸°ë°˜ ë‹µë³€"""
        try:
            print(f"ğŸ“„ JSON ê¸°ë°˜ ë‹µë³€ ìƒì„±: {video.chat_analysis_json_path}")
            
            # JSON íŒŒì¼ì—ì„œ ì´ì „ ë¶„ì„ ê²°ê³¼ ë¡œë“œ
            if not video.chat_analysis_json_path or not os.path.exists(video.chat_analysis_json_path):
                print("âš ï¸ JSON íŒŒì¼ì´ ì—†ìŒ - ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ì²˜ë¦¬")
                response_text = self.llm_client.generate_smart_response(
                    user_query=user_query,
                    search_results=None,
                    video_info=f"ë¹„ë””ì˜¤: {video.original_name}",
                    use_multi_llm=False
                )
                
                # í…ìŠ¤íŠ¸ ì „ìš© ë¹„ìš© ì¶”ì 
                estimated_cost = 0.002  # í…ìŠ¤íŠ¸ ì „ìš© ë¹„ìš© (ë§¤ìš° ì €ë ´)
                video.update_cost_tracking('text_only', estimated_cost)
                
                return {
                    'response_type': 'text_only_fallback',
                    'response': response_text,
                    'query': user_query,
                    'video_info': {'id': video.id, 'name': video.original_name},
                    'cost_info': {
                        'estimated_cost': estimated_cost,
                        'note': 'JSON íŒŒì¼ ì—†ìŒ - í…ìŠ¤íŠ¸ ì „ìš© ì²˜ë¦¬'
                    }
                }
            
            # JSON íŒŒì¼ ë¡œë“œ
            with open(video.chat_analysis_json_path, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
            
            # ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ í™œìš©
            if analysis_data.get('analysis_type') == 'multi_llm_image_analysis':
                # ì´ì „ ë©€í‹° LLM ê²°ê³¼ í™œìš©
                response_text = self._generate_contextual_response_from_json(
                    user_query, analysis_data, video
                )
            else:
                # ë‹¨ì¼ LLM ê²°ê³¼ í™œìš©
                response_text = self._generate_simple_response_from_json(
                    user_query, analysis_data, video
                )
            
            # í…ìŠ¤íŠ¸ ì „ìš© ë¹„ìš© ì¶”ì 
            estimated_cost = 0.001  # ë§¤ìš° ì €ë ´í•œ í…ìŠ¤íŠ¸ ì „ìš© ë¹„ìš©
            video.update_cost_tracking('text_only', estimated_cost)
            
            return {
                'response_type': 'json_based_optimized',
                'response': response_text,
                'query': user_query,
                'video_info': {'id': video.id, 'name': video.original_name},
                'cost_info': {
                    'estimated_cost': estimated_cost,
                    'data_source': 'saved_analysis',
                    'optimization_savings': '~95% cost reduction'
                }
            }
            
        except Exception as e:
            print(f"âŒ JSON ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._handle_fallback_response(user_query, video)
        


from django.http import JsonResponse, HttpResponse
from django.views import View
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
import json
import os

class AnalysisStatusView(APIView):
    """ë¶„ì„ ìƒíƒœ ìƒì„¸ ì¡°íšŒ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            # ìµœì‹  ë¶„ì„ ì •ë³´ ì¡°íšŒ
            latest_analysis = VideoAnalysis.objects.filter(video=video).order_by('-id').first()
            
            response_data = {
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_status': video.analysis_status,
                'is_analyzed': video.is_analyzed,
                'analysis_progress': 100 if video.analysis_status == 'completed' else 
                                   (50 if video.analysis_status == 'processing' else 0),
                'video_info': {
                    'duration': getattr(video, 'duration', 0),
                    'total_frames': getattr(video, 'total_frames', 0),
                    'fps': getattr(video, 'fps', 0),
                    'width': getattr(video, 'width', 0),
                    'height': getattr(video, 'height', 0)
                }
            }
            
            if latest_analysis:
                response_data['latest_analysis'] = {
                    'id': latest_analysis.id,
                    'enhanced_analysis': latest_analysis.enhanced_analysis,
                    'success_rate': latest_analysis.success_rate,
                    'processing_time_seconds': latest_analysis.processing_time_seconds,
                    'frames_analyzed': latest_analysis.analysis_statistics.get('total_frames_analyzed', 0),
                    'dominant_objects': latest_analysis.analysis_statistics.get('dominant_objects', []),
                    'ai_features_used': latest_analysis.analysis_statistics.get('ai_features_used', {}),
                    'json_file_path': latest_analysis.analysis_statistics.get('json_file_path', ''),
                    'created_at': latest_analysis.created_at.isoformat() if hasattr(latest_analysis, 'created_at') else None
                }
            
            # í”„ë ˆì„ ë° ì”¬ ê°œìˆ˜
            frame_count = Frame.objects.filter(video=video).count()
            scene_count = Scene.objects.filter(video=video).count()
            
            response_data['analysis_counts'] = {
                'frames': frame_count,
                'scenes': scene_count
            }
            
            return Response(response_data)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ë¶„ì„ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class AnalysisResultsView(APIView):
    """ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì¡°íšŒ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if not video.is_analyzed:
                return Response({
                    'error': 'ì•„ì§ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                    'analysis_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
            latest_analysis = VideoAnalysis.objects.filter(video=video).order_by('-id').first()
            frames = Frame.objects.filter(video=video).order_by('timestamp')[:20]  # ìµœëŒ€ 20ê°œ
            scenes = Scene.objects.filter(video=video).order_by('scene_id')
            
            response_data = {
                'video_id': video.id,
                'video_name': video.original_name,
                'analysis_summary': {},
                'sample_frames': [],
                'scenes': [],
                'json_file_available': False,
                'json_file_path': None
            }
            
            if latest_analysis:
                response_data['analysis_summary'] = {
                    'success_rate': latest_analysis.success_rate,
                    'processing_time': latest_analysis.processing_time_seconds,
                    'total_frames_analyzed': latest_analysis.analysis_statistics.get('total_frames_analyzed', 0),
                    'dominant_objects': latest_analysis.analysis_statistics.get('dominant_objects', []),
                    'scene_types': latest_analysis.analysis_statistics.get('scene_types', []),
                    'text_extracted': latest_analysis.analysis_statistics.get('text_extracted', False),
                    'ai_features_used': latest_analysis.analysis_statistics.get('ai_features_used', {}),
                    'analysis_quality_metrics': latest_analysis.analysis_statistics.get('analysis_quality_metrics', {}),
                    'caption_statistics': {
                        'frames_with_caption': latest_analysis.caption_statistics.get('frames_with_caption', 0),
                        'enhanced_captions': latest_analysis.caption_statistics.get('enhanced_captions', 0),
                        'average_confidence': latest_analysis.caption_statistics.get('average_confidence', 0)
                    }
                }
                
                # JSON íŒŒì¼ ê²½ë¡œ í™•ì¸
                json_file_path = latest_analysis.analysis_statistics.get('json_file_path', '')
                if json_file_path and os.path.exists(json_file_path):
                    response_data['json_file_available'] = True
                    response_data['json_file_path'] = json_file_path
            
            # ìƒ˜í”Œ í”„ë ˆì„ë“¤
            for frame in frames:
                response_data['sample_frames'].append({
                    'frame_id': frame.image_id,
                    'timestamp': frame.timestamp,
                    'caption': frame.final_caption or frame.enhanced_caption or frame.caption,
                    'object_count': len(frame.detected_objects),
                    'detected_objects': [obj.get('class', '') for obj in frame.detected_objects[:5]]  # ìµœëŒ€ 5ê°œë§Œ
                })
            
            # ì”¬ ì •ë³´
            for scene in scenes:
                response_data['scenes'].append({
                    'scene_id': scene.scene_id,
                    'start_time': scene.start_time,
                    'end_time': scene.end_time,
                    'duration': scene.duration,
                    'frame_count': scene.frame_count,
                    'dominant_objects': scene.dominant_objects[:3]  # ìµœëŒ€ 3ê°œë§Œ
                })
            
            return Response(response_data)
            
        except Video.DoesNotExist:
            return Response({
                'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class AnalyzerSystemStatusView(APIView):
    """AI ë¶„ì„ ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ ì¡°íšŒ"""
    permission_classes = [AllowAny]
    
    def get(self, request):
        try:
            if not VIDEO_ANALYZER_AVAILABLE:
                return Response({
                    'system_status': 'unavailable',
                    'error': 'video_analyzer ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
                    'available_features': {},
                    'recommendation': 'video_analyzer.py íŒŒì¼ê³¼ ì˜ì¡´ì„±ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”'
                })
            
            # ë¶„ì„ê¸° ìƒíƒœ ì¡°íšŒ
            analyzer_status = get_analyzer_status()
            
            # RAG ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ
            try:
                rag_system = get_video_rag_system()
                rag_info = rag_system.get_database_info() if rag_system else None
                rag_available = rag_system is not None
            except:
                rag_info = None
                rag_available = False
            
            # ì‹œìŠ¤í…œ í†µê³„
            total_videos = Video.objects.count()
            analyzed_videos = Video.objects.filter(is_analyzed=True).count()
            processing_videos = Video.objects.filter(analysis_status='processing').count()
            
            response_data = {
                'system_status': 'operational' if analyzer_status.get('status') == 'initialized' else 'limited',
                'analyzer': analyzer_status,
                'rag_system': {
                    'available': rag_available,
                    'info': rag_info
                },
                'statistics': {
                    'total_videos': total_videos,
                    'analyzed_videos': analyzed_videos,
                    'processing_videos': processing_videos,
                    'analysis_rate': (analyzed_videos / max(total_videos, 1)) * 100
                },
                'capabilities': {
                    'yolo_object_detection': analyzer_status.get('features', {}).get('yolo', False),
                    'clip_scene_analysis': analyzer_status.get('features', {}).get('clip', False),
                    'ocr_text_extraction': analyzer_status.get('features', {}).get('ocr', False),
                    'vqa_question_answering': analyzer_status.get('features', {}).get('vqa', False),
                    'scene_graph_generation': analyzer_status.get('features', {}).get('scene_graph', False),
                    'rag_search_system': rag_available
                },
                'device': analyzer_status.get('device', 'unknown'),
                'last_checked': datetime.now().isoformat()
            }
            
            return Response(response_data)
            
        except Exception as e:
            return Response({
                'system_status': 'error',
                'error': f'ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}',
                'last_checked': datetime.now().isoformat()
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class DownloadAnalysisResultView(APIView):
    """ë¶„ì„ ê²°ê³¼ JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    permission_classes = [AllowAny]
    
    def get(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            latest_analysis = VideoAnalysis.objects.filter(video=video).order_by('-id').first()
            
            if not latest_analysis:
                return Response({
                    'error': 'ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            json_file_path = latest_analysis.analysis_statistics.get('json_file_path', '')
            
            if not json_file_path or not os.path.exists(json_file_path):
                return Response({
                    'error': 'JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_404_NOT_FOUND)
            
            # JSON íŒŒì¼ ì½ê¸°
            with open(json_file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # HTTP ì‘ë‹µìœ¼ë¡œ JSON ë°˜í™˜
            response = HttpResponse(
                json.dumps(json_data, ensure_ascii=False, indent=2),
                content_type='application/json; charset=utf-8'
            )
            response['Content-Disposition'] = f'attachment; filename="analysis_{video.id}_{video.original_name}.json"'
            
            return response
            
        except Video.DoesNotExist:
            return Response({
                'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'JSON ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# ========================================
# ë¶„ì„ ì·¨ì†Œ ë° ê´€ë¦¬ ê¸°ëŠ¥
# ========================================

class CancelAnalysisView(APIView):
    """ë¶„ì„ ì·¨ì†Œ"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if video.analysis_status != 'processing':
                return Response({
                    'error': 'ì§„í–‰ ì¤‘ì¸ ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤.',
                    'current_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ìƒíƒœë¥¼ cancelledë¡œ ë³€ê²½
            video.analysis_status = 'cancelled'
            video.save()
            
            return Response({
                'success': True,
                'message': 'ë¶„ì„ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'video_id': video.id,
                'new_status': 'cancelled'
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ë¶„ì„ ì·¨ì†Œ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class RestartAnalysisView(APIView):
    """ë¶„ì„ ì¬ì‹œì‘"""
    permission_classes = [AllowAny]
    
    def post(self, request, video_id):
        try:
            video = Video.objects.get(id=video_id)
            
            if video.analysis_status == 'processing':
                return Response({
                    'error': 'ì´ë¯¸ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.',
                    'current_status': video.analysis_status
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            cleanup = request.data.get('cleanup_previous', False)
            if cleanup:
                VideoAnalysis.objects.filter(video=video).delete()
                Frame.objects.filter(video=video).delete()
                Scene.objects.filter(video=video).delete()
            
            # ë¶„ì„ ìƒíƒœ ì´ˆê¸°í™”
            video.analysis_status = 'pending'
            video.is_analyzed = False
            video.save()
            
            return Response({
                'success': True,
                'message': 'ë¶„ì„ì´ ì¬ì‹œì‘ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ìš”ì²­í•´ì£¼ì„¸ìš”.',
                'video_id': video.id,
                'new_status': 'pending',
                'cleanup_performed': cleanup
            })
            
        except Video.DoesNotExist:
            return Response({
                'error': 'í•´ë‹¹ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({
                'error': f'ë¶„ì„ ì¬ì‹œì‘ ì‹¤íŒ¨: {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# chat/views.py - EnhancedVideoChatView ê°œì„  ë²„ì „

import json
import os
import re
import time
from collections import Counter
from datetime import datetime
from typing import List, Dict, Any, Optional
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from rest_framework.permissions import AllowAny
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.conf import settings

from .models import Video, VideoAnalysis, SearchHistory
from .llm_client import LLMClient
from .multi_llm_service import get_multi_llm_analyzer
from .db_builder import get_video_rag_system

class SmartJSONParser:
    """ê³ ì„±ëŠ¥ JSON íŒŒì‹± ë° person ê°ì²´ ì¶”ì¶œ ìµœì í™”"""
    
    @staticmethod
    def extract_person_info_optimized(json_data: Dict) -> Dict[str, Any]:
        """ìµœì í™”ëœ person ì •ë³´ ì¶”ì¶œ"""
        person_analysis = {
            'total_person_detections': 0,
            'unique_persons_estimated': 0,
            'frames_with_people': [],
            'person_tracking_data': {},
            'confidence_scores': [],
            'gender_analysis': {'male': 0, 'female': 0, 'unknown': 0},
            'temporal_consistency': 0.0
        }
        
        frame_results = json_data.get('frame_results', [])
        if not frame_results:
            return person_analysis
        
        # í”„ë ˆì„ë³„ person ì¶”ì 
        for frame_data in frame_results:
            frame_id = frame_data.get('image_id', 0)
            timestamp = frame_data.get('timestamp', 0)
            objects = frame_data.get('objects', [])
            
            frame_persons = []
            for obj in objects:
                if obj.get('class') == 'person':
                    person_data = {
                        'bbox': obj.get('bbox', []),
                        'confidence': obj.get('confidence', 0),
                        'track_id': obj.get('track_id'),
                        'frame_id': frame_id,
                        'timestamp': timestamp
                    }
                    frame_persons.append(person_data)
                    person_analysis['confidence_scores'].append(obj.get('confidence', 0))
            
            if frame_persons:
                person_analysis['frames_with_people'].append({
                    'frame_id': frame_id,
                    'timestamp': timestamp,
                    'person_count': len(frame_persons),
                    'persons': frame_persons
                })
                person_analysis['total_person_detections'] += len(frame_persons)
        
        # track-based countingìœ¼ë¡œ ê³ ìœ  person ìˆ˜ ê³„ì‚°
        person_analysis['unique_persons_estimated'] = SmartJSONParser._estimate_unique_persons(
            person_analysis['frames_with_people']
        )
        
        # ì„±ë³„ ë¶„ì„ (ìº¡ì…˜ ë° VQA ê²°ê³¼ í™œìš©)
        person_analysis['gender_analysis'] = SmartJSONParser._analyze_gender_from_captions(frame_results)
        
        # ì‹œê°„ì  ì¼ê´€ì„± ê³„ì‚°
        person_analysis['temporal_consistency'] = SmartJSONParser._calculate_temporal_consistency(
            person_analysis['frames_with_people']
        )
        
        return person_analysis
    
    @staticmethod
    def _estimate_unique_persons(frames_with_people: List) -> int:
        """ë‹¤ì¤‘ í”„ë ˆì„ ì¶”ì ìœ¼ë¡œ ê³ ìœ  person ìˆ˜ ì¶”ì •"""
        if not frames_with_people:
            return 0
        
        # track_idê°€ ìˆëŠ” ê²½ìš°
        track_ids = set()
        for frame_data in frames_with_people:
            for person in frame_data['persons']:
                track_id = person.get('track_id')
                if track_id is not None:
                    track_ids.add(track_id)
        
        if track_ids:
            return len(track_ids)
        
        # track_idê°€ ì—†ëŠ” ê²½ìš° íœ´ë¦¬ìŠ¤í‹± ë°©ë²• ì‚¬ìš©
        person_counts = [frame_data['person_count'] for frame_data in frames_with_people]
        if not person_counts:
            return 0
        
        # ê°€ì¥ ë†’ì€ confidenceë¥¼ ê°€ì§„ í”„ë ˆì„ë“¤ì˜ í‰ê· ê°’ ì‚¬ìš©
        high_confidence_frames = []
        for frame_data in frames_with_people:
            avg_confidence = sum(p['confidence'] for p in frame_data['persons']) / len(frame_data['persons'])
            if avg_confidence > 0.7:  # ë†’ì€ ì‹ ë¢°ë„ í”„ë ˆì„ë§Œ
                high_confidence_frames.append(frame_data['person_count'])
        
        if high_confidence_frames:
            return max(high_confidence_frames)  # ìµœëŒ€ê°’ ì‚¬ìš©
        else:
            return max(person_counts) if person_counts else 0
    
    @staticmethod
    def _analyze_gender_from_captions(frame_results: List) -> Dict[str, int]:
        """ìº¡ì…˜ì—ì„œ ì„±ë³„ ì •ë³´ ë¶„ì„"""
        gender_analysis = {'male': 0, 'female': 0, 'unknown': 0}
        
        male_keywords = ['ë‚¨ì', 'ë‚¨ì„±', 'man', 'male', 'ì•„ì €ì”¨', 'ì²­ë…„', 'ì•„ë¹ ', 'ì•„ë“¤']
        female_keywords = ['ì—¬ì', 'ì—¬ì„±', 'woman', 'female', 'ì•„ì¤Œë§ˆ', 'ì†Œë…€', 'ì—„ë§ˆ', 'ë”¸']
        
        for frame_data in frame_results:
            # ìº¡ì…˜ì—ì„œ ì„±ë³„ í‚¤ì›Œë“œ ì°¾ê¸°
            captions = [
                frame_data.get('final_caption', ''),
                frame_data.get('enhanced_caption', ''),
                frame_data.get('caption', '')
            ]
            
            frame_text = ' '.join(captions).lower()
            
            male_mentions = sum(1 for keyword in male_keywords if keyword in frame_text)
            female_mentions = sum(1 for keyword in female_keywords if keyword in frame_text)
            
            gender_analysis['male'] += male_mentions
            gender_analysis['female'] += female_mentions
            
            # VQA ê²°ê³¼ë„ í™•ì¸
            scene_analysis = frame_data.get('scene_analysis', {})
            vqa_results = scene_analysis.get('vqa_results', {})
            
            for question, answer in vqa_results.items():
                if 'people' in question.lower() or 'ì‚¬ëŒ' in question:
                    answer_lower = answer.lower()
                    if any(keyword in answer_lower for keyword in male_keywords):
                        gender_analysis['male'] += 1
                    if any(keyword in answer_lower for keyword in female_keywords):
                        gender_analysis['female'] += 1
        
        return gender_analysis
    
    @staticmethod
    def _calculate_temporal_consistency(frames_with_people: List) -> float:
        """ì‹œê°„ì  ì¼ê´€ì„± ê³„ì‚°"""
        if len(frames_with_people) < 2:
            return 1.0
        
        person_counts = [frame_data['person_count'] for frame_data in frames_with_people]
        
        # í‘œì¤€í¸ì°¨ ê¸°ë°˜ ì¼ê´€ì„± ê³„ì‚°
        import numpy as np
        if len(set(person_counts)) == 1:
            return 1.0  # ì™„ì „ ì¼ê´€ì„±
        
        mean_count = np.mean(person_counts)
        std_count = np.std(person_counts)
        
        # ì¼ê´€ì„± ì ìˆ˜ (0-1)
        consistency = max(0, 1 - (std_count / max(mean_count, 1)))
        return consistency

class KoreanQuestionClassifier:
    """í•œêµ­ì–´ ì§ˆë¬¸ ë¶„ë¥˜ ì‹œìŠ¤í…œ"""
    
    QUESTION_PATTERNS = {
        'person_count': {
            'keywords': ['ëª‡ ëª…', 'ëª‡ëª…', 'ì‚¬ëŒ', 'ì¸ë¬¼', 'ì¸ì›', 'ì„±ë¹„', 'ë‚¨ë…€', 'ì‚¬ëŒ ìˆ˜'],
            'patterns': [r'ëª‡\s*ëª…', r'ì‚¬ëŒ.*ëª‡', r'ì¸ì›.*ëª‡', r'ì„±ë¹„', r'ë‚¨.*ì—¬'],
            'weight': 1.0
        },
        'object_search': {
            'keywords': ['ì°¾', 'ì–´ë””', 'ì–¸ì œ', 'ë¬´ì—‡', 'ë­ê°€', 'ìˆì–´', 'ë³´ì—¬', 'ë‚˜ì™€'],
            'patterns': [r'.*ì°¾.*', r'ì–´ë””.*ìˆ', r'ì–¸ì œ.*ë‚˜', r'ë¬´ì—‡.*ë³´'],
            'weight': 0.9
        },
        'scene_summary': {
            'keywords': ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì „ì²´', 'ì¤„ê±°ë¦¬', 'ì„¤ëª…'],
            'patterns': [r'ìš”ì•½.*í•´', r'ì •ë¦¬.*í•´', r'ë‚´ìš©.*ë­', r'ì „ì²´.*ì–´ë–¤'],
            'weight': 0.8
        },
        'action_analysis': {
            'keywords': ['í–‰ë™', 'ë™ì‘', 'í™œë™', 'í•˜ê³ ìˆ', 'ì›€ì§ì„', 'ë­˜ í•´'],
            'patterns': [r'.*í•˜ê³ \s*ìˆ', r'ë¬´ì—‡.*í•˜ëŠ”', r'ì–´ë–¤.*í–‰ë™'],
            'weight': 0.8
        },
        'time_location': {
            'keywords': ['ì‹œê°„', 'ì¥ì†Œ', 'ìœ„ì¹˜', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ë°°ê²½'],
            'patterns': [r'ì–¸ì œ.*', r'ì–´ë””ì„œ.*', r'.*ì‹œê°„', r'.*ì¥ì†Œ'],
            'weight': 0.7
        }
    }
    
    @classmethod
    def classify_question(cls, question: str) -> Dict[str, float]:
        """ì§ˆë¬¸ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ì ìˆ˜ ë°˜í™˜"""
        question_lower = question.lower().strip()
        scores = {}
        
        for category, config in cls.QUESTION_PATTERNS.items():
            score = 0.0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_matches = sum(1 for keyword in config['keywords'] if keyword in question_lower)
            keyword_score = (keyword_matches / len(config['keywords'])) * 0.6
            
            # íŒ¨í„´ ë§¤ì¹­
            pattern_matches = sum(1 for pattern in config['patterns'] if re.search(pattern, question_lower))
            pattern_score = (pattern_matches / len(config['patterns'])) * 0.4
            
            total_score = (keyword_score + pattern_score) * config['weight']
            scores[category] = total_score
        
        return scores
    
    @classmethod
    def get_primary_category(cls, question: str) -> str:
        """ì£¼ìš” ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        scores = cls.classify_question(question)
        if not scores:
            return 'general'
        
        max_category = max(scores.keys(), key=lambda k: scores[k])
        if scores[max_category] > 0.3:  # ì„ê³„ê°’
            return max_category
        else:
            return 'general'

class ContextAwareResponseGenerator:
    """ë§¥ë½ ì¸ì‹ ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.session_memory = {}  # ì„¸ì…˜ë³„ ëŒ€í™” ì´ë ¥
        self.response_templates = {
            'person_count': {
                'single': "í™”ë©´ì— {count}ëª…ì´ ë³´ì…ë‹ˆë‹¤.",
                'multiple': "ë¹„ë””ì˜¤ ì „ì²´ì—ì„œ ì´ {count}ëª…ì´ ë“±ì¥í•©ë‹ˆë‹¤.",
                'uncertain': "ì •í™•íˆ ì„¸ê¸° ì–´ë ¤ìš´ë°, ëŒ€ëµ {estimate}ëª… ì •ë„ ë³´ì´ë„¤ìš”.",
                'with_gender': "ì´ {total}ëª…ì´ ë“±ì¥í•˜ë©°, ë‚¨ì„± {male}ëª…, ì—¬ì„± {female}ëª…ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤.",
                'temporal': "{frames}ê°œ ì¥ë©´ì—ì„œ ì‚¬ëŒì´ ë“±ì¥í•˜ë©°, í‰ê·  {avg_count}ëª…ì”© ë³´ì…ë‹ˆë‹¤."
            },
            'object_search': {
                'found': "'{object}'ë¥¼ {count}ê°œ ì¥ë©´ì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                'not_found': "'{object}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'location': "'{object}'ëŠ” {timestamp}ì´ˆ ì§€ì ì—ì„œ í™•ì¸ë©ë‹ˆë‹¤."
            },
            'scene_summary': {
                'basic': "ì´ ì˜ìƒì€ {scenes}ê°œì˜ ì£¼ìš” ì¥ë©´ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                'detailed': "ì˜ìƒì˜ ì£¼ìš” ë‚´ìš©: {content}"
            }
        }
    
    def generate_contextual_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """ë§¥ë½ì„ ê³ ë ¤í•œ ì‘ë‹µ ìƒì„±"""
        category = KoreanQuestionClassifier.get_primary_category(question)
        
        if category == 'person_count':
            return self._generate_person_count_response(question, analysis_data, video_info)
        elif category == 'object_search':
            return self._generate_object_search_response(question, analysis_data, video_info)
        elif category == 'scene_summary':
            return self._generate_scene_summary_response(question, analysis_data, video_info)
        else:
            return self._generate_general_response(question, analysis_data, video_info)
    
    def _generate_person_count_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """ì‚¬ëŒ ìˆ˜ ê´€ë ¨ ì‘ë‹µ ìƒì„±"""
        person_info = analysis_data.get('person_analysis', {})
        
        unique_count = person_info.get('unique_persons_estimated', 0)
        total_detections = person_info.get('total_person_detections', 0)
        frames_with_people = person_info.get('frames_with_people', [])
        gender_analysis = person_info.get('gender_analysis', {})
        temporal_consistency = person_info.get('temporal_consistency', 0)
        
        templates = self.response_templates['person_count']
        
        # ì„±ë¹„ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        if any(keyword in question for keyword in ['ì„±ë¹„', 'ë‚¨ë…€', 'ì„±ë³„']):
            if gender_analysis['male'] > 0 or gender_analysis['female'] > 0:
                return templates['with_gender'].format(
                    total=unique_count,
                    male=gender_analysis['male'],
                    female=gender_analysis['female']
                )
        
        # ì‹œê°„ì  ì¼ê´€ì„±ì´ ë‚®ìœ¼ë©´ ë¶ˆí™•ì‹¤ í‘œí˜„
        if temporal_consistency < 0.6:
            estimate = max(unique_count, int(total_detections / max(len(frames_with_people), 1)))
            return templates['uncertain'].format(estimate=estimate)
        
        # ì—¬ëŸ¬ í”„ë ˆì„ì— ê±¸ì³ ë“±ì¥í•˜ëŠ” ê²½ìš°
        if len(frames_with_people) > 1:
            avg_count = total_detections / len(frames_with_people)
            return templates['temporal'].format(
                frames=len(frames_with_people),
                avg_count=f"{avg_count:.1f}"
            )
        
        # ê¸°ë³¸ ì‘ë‹µ
        if unique_count == 1:
            return templates['single'].format(count=unique_count)
        else:
            return templates['multiple'].format(count=unique_count)
    
    def _generate_object_search_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """ê°ì²´ ê²€ìƒ‰ ì‘ë‹µ ìƒì„±"""
        # ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ ëŒ€ìƒ ì¶”ì¶œ
        search_terms = self._extract_search_terms(question)
        
        frame_results = analysis_data.get('frame_results', [])
        found_objects = []
        
        for frame_data in frame_results:
            frame_id = frame_data.get('image_id', 0)
            timestamp = frame_data.get('timestamp', 0)
            objects = frame_data.get('objects', [])
            
            for obj in objects:
                obj_class = obj.get('class', '').lower()
                if any(term.lower() in obj_class for term in search_terms):
                    found_objects.append({
                        'frame_id': frame_id,
                        'timestamp': timestamp,
                        'object': obj_class,
                        'confidence': obj.get('confidence', 0)
                    })
        
        templates = self.response_templates['object_search']
        
        if found_objects:
            search_term = search_terms[0] if search_terms else 'í•´ë‹¹ ê°ì²´'
            return templates['found'].format(
                object=search_term,
                count=len(found_objects)
            )
        else:
            search_term = search_terms[0] if search_terms else 'í•´ë‹¹ ê°ì²´'
            return templates['not_found'].format(object=search_term)
    
    def _generate_scene_summary_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """ì¥ë©´ ìš”ì•½ ì‘ë‹µ ìƒì„±"""
        frame_results = analysis_data.get('frame_results', [])
        
        # ì£¼ìš” ê°ì²´ ì¶”ì¶œ
        all_objects = []
        for frame_data in frame_results:
            objects = frame_data.get('objects', [])
            all_objects.extend([obj.get('class', '') for obj in objects])
        
        object_counter = Counter(all_objects)
        dominant_objects = [obj for obj, count in object_counter.most_common(5)]
        
        # ì£¼ìš” ì¥ë©´ ì‹ë³„
        unique_scenes = len(frame_results)
        
        summary = f"ì´ ì˜ìƒì€ {unique_scenes}ê°œì˜ ì¥ë©´ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
        
        if dominant_objects:
            summary += f"ì£¼ìš” ê°ì²´ë¡œëŠ” {', '.join(dominant_objects[:3])} ë“±ì´ ë“±ì¥í•©ë‹ˆë‹¤. "
        
        # ì‚¬ëŒì´ ë“±ì¥í•˜ëŠ” ê²½ìš°
        person_info = analysis_data.get('person_analysis', {})
        if person_info.get('unique_persons_estimated', 0) > 0:
            summary += f"ì´ {person_info['unique_persons_estimated']}ëª…ì˜ ì‚¬ëŒì´ ë“±ì¥í•©ë‹ˆë‹¤."
        
        return summary
    
    def _generate_general_response(self, question: str, analysis_data: Dict, video_info: Dict) -> str:
        """ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±"""
        frame_results = analysis_data.get('frame_results', [])
        
        return f"'{question}'ì— ëŒ€í•´ ë¶„ì„í•œ ê²°ê³¼, {len(frame_results)}ê°œ ì¥ë©´ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    def _extract_search_terms(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì–´ ì¶”ì¶œ"""
        # í•œêµ­ì–´ ê°ì²´ëª… ë§¤í•‘
        object_mapping = {
            'ìë™ì°¨': 'car', 'ì°¨': 'car', 'ìŠ¹ìš©ì°¨': 'car',
            'ì‚¬ëŒ': 'person', 'ì¸ë¬¼': 'person',
            'ê°•ì•„ì§€': 'dog', 'ê°œ': 'dog',
            'ê³ ì–‘ì´': 'cat', 'ëƒ¥ì´': 'cat',
            'ì˜ì': 'chair', 'ì±…ìƒ': 'table',
            'í•¸ë“œí°': 'cell_phone', 'í°': 'cell_phone',
            'ì»´í“¨í„°': 'laptop', 'ë…¸íŠ¸ë¶': 'laptop'
        }
        
        terms = []
        
        # ì§ì ‘ ë§¤í•‘ëœ ê°ì²´ ì°¾ê¸°
        for korean, english in object_mapping.items():
            if korean in question:
                terms.append(english)
        
        # ì˜ì–´ ê°ì²´ëª… ì§ì ‘ ì¶”ì¶œ
        import re
        english_objects = re.findall(r'[a-zA-Z]+', question)
        terms.extend(english_objects)
        
        return terms

# views.py (í•„ìš” import)
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.permissions import AllowAny
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt
from django.db.models import Q
import time, json, re

COLOR_MAP_KR2EN = {
    'ì´ˆë¡':'green','ë…¹ìƒ‰':'green','ë¹¨ê°•':'red','ë¹¨ê°„':'red','ì ìƒ‰':'red',
    'ì£¼í™©':'orange','ì˜¤ë Œì§€':'orange','ë…¸ë‘':'yellow','ë…¸ë€':'yellow','í™©ìƒ‰':'yellow',
    'íŒŒë‘':'blue','íŒŒë€':'blue','ì²­ìƒ‰':'blue','ë³´ë¼':'purple','ìì£¼':'purple',
    'ê²€ì •':'black','ê²€ì€':'black','í•˜ì–‘':'white','í°':'white','ë°±ìƒ‰':'white',
    'íšŒìƒ‰':'gray','ê·¸ë ˆì´':'gray','ê°ˆìƒ‰':'brown',
    'í•‘í¬':'pink','ë¶„í™':'pink','ê¸ˆìƒ‰':'gold','ì€ìƒ‰':'silver'
}

OBJ_MAP_KR2EN = {
    'ì‚¬ëŒ':'person','ë‚¨ì„±':'person','ì—¬ì„±':'person','ì¸ë¬¼':'person',
    'ê°€ë°©':'handbag','í•¸ë“œë°±':'handbag','ë°±íŒ©':'backpack',
    'ìë™ì°¨':'car','ì°¨':'car','ìì „ê±°':'bicycle','ì˜¤í† ë°”ì´':'motorcycle',
    'ê°œ':'dog','ê°•ì•„ì§€':'dog','ê³ ì–‘ì´':'cat','ì˜ì':'chair','ë…¸íŠ¸ë¶':'laptop','íœ´ëŒ€í°':'cell_phone','í•¸ë“œí°':'cell_phone','í°':'cell_phone',
    'í‹°ë¹„':'tv','tv':'tv'
}

SCENE_KEYWORDS = {
    'ë¹„':'rain','ë¹„ì˜¤ëŠ”':'rain','ìš°ì²œ':'rain',
    'ë°¤':'night','ì•¼ê°„':'night','ë‚®':'day','ì‹¤ë‚´':'indoor','ì‹¤ì™¸':'outdoor'
}

# views.py
import os, time, json, subprocess, tempfile
from datetime import datetime
from django.conf import settings
from django.http import FileResponse, Http404
from django.urls import reverse
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt
from rest_framework.views import APIView
from rest_framework.permissions import AllowAny
from rest_framework.response import Response

from .models import Video, Frame, Scene


@method_decorator(csrf_exempt, name='dispatch')
class EnhancedVideoChatView(APIView):
    """í–¥ìƒëœ ë¹„ë””ì˜¤ ì±„íŒ… - ìì—°ì–´ ì§ˆì˜ì— ëŒ€í•´ í…ìŠ¤íŠ¸ + ì¸ë„¤ì¼/í´ë¦½ì„ í•¨ê»˜ ë°˜í™˜"""
    permission_classes = [AllowAny]

    # ---------- ì´ˆê¸°í™” ----------
    def __init__(self):
        super().__init__()
        self.llm_client = None
        self.video_analyzer = None
    def _initialize_services(self):
        """ì„œë¹„ìŠ¤ ì•ˆì „ ì´ˆê¸°í™” - LLM í´ë¼ì´ì–¸íŠ¸ ê°œì„ """
        if self.llm_client is None:
            try:
                from .llm_client import get_llm_client
                self.llm_client = get_llm_client()
                if self.llm_client.is_available():
                    print("LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    print("LLM í´ë¼ì´ì–¸íŠ¸ ë¹„í™œì„±í™” - ê¸°ë³¸ ì„¤ëª… ìƒì„± ëª¨ë“œ")
            except Exception as e:
                print(f"LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # Mock í´ë¼ì´ì–¸íŠ¸ë¡œ í´ë°±
                from .llm_client import MockLLMClient
                self.llm_client = MockLLMClient()

        if self.video_analyzer is None:
            try:
                from .video_analyzer import get_video_analyzer
                self.video_analyzer = get_video_analyzer()
                print("ë¹„ë””ì˜¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"ë¹„ë””ì˜¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ---------- ê³µìš© ìœ í‹¸ ----------
    def _frame_urls(self, request, video_id, frame_number):
        """í”„ë ˆì„ ì •ê·œ ì´ë¯¸ì§€ & ë°•ìŠ¤ì´ë¯¸ì§€ URL"""
        base = request.build_absolute_uri
        return {
            'image': base(reverse('frame_normal', args=[video_id, frame_number])),
            'image_bbox': base(reverse('frame_with_bbox', args=[video_id, frame_number])),
        }

    def _clip_url(self, request, video_id, timestamp, duration=4):
        """í”„ë¦¬ë·° í´ë¦½ URL"""
        url = reverse('clip_preview', args=[video_id, int(timestamp)])
        return request.build_absolute_uri(f"{url}?duration={int(duration)}")

    def _format_time(self, seconds):
        try:
            m, s = int(seconds) // 60, int(seconds) % 60
            return f"{m}:{s:02d}"
        except:
            return "0:00"

    def _get_video_safe(self, video_id):
        try:
            if video_id:
                return Video.objects.get(id=video_id)
            return Video.objects.filter(is_analyzed=True).first()
        except:
            return None

    # ---------- NLU(ê°„ë‹¨ ìŠ¬ë¡¯ ì¶”ì¶œ) ----------
  # EnhancedVideoChatViewì— ì¶”ê°€í•  ë©”ì„œë“œë“¤

    def _nlu(self, text: str):
        """intent + slots ê°„ë‹¨ ì¶”ì¶œ (ì˜ìƒ ì„¤ëª… ì˜ë„ ì¶”ê°€)"""
        q = text.lower()
        intent = 'general'
        
        # ì˜ìƒ ì„¤ëª… í‚¤ì›Œë“œ ì¶”ê°€
        if any(k in q for k in ['ì„¤ëª…í•´ì¤˜', 'ì„¤ëª…í•´', 'ì–´ë–¤', 'ë¬´ìŠ¨', 'ë‚´ìš©', 'ì¥ë©´', 'ì˜ìƒì— ëŒ€í•´', 'ë­ê°€ ë‚˜ì™€', 'ì–´ë–»ê²Œ', 'ìƒí™©']):
            intent = 'video_description'
        elif any(k in q for k in ['ìš”ì•½', 'summary']): 
            intent = 'summary'
        elif any(k in q for k in ['í•˜ì´ë¼ì´íŠ¸', 'highlight']): 
            intent = 'highlight'
        elif any(k in q for k in ['ì •ë³´', 'info']): 
            intent = 'info'
        elif any(k in q for k in ['ì„±ë¹„', 'gender']): 
            intent = 'gender_distribution'
        elif any(k in q for k in ['ë¶„ìœ„ê¸°', 'ë¬´ë“œ', 'mood']): 
            intent = 'scene_mood'
        elif any(k in q for k in ['ë¹„ì˜¤ëŠ”', 'ë°¤', 'ë‚®', 'ì‹¤ë‚´', 'ì‹¤ì™¸']): 
            intent = 'cross_video'
        elif any(k in q for k in ['ì°¾ì•„ì¤˜', 'ì°¾ì•„ ì¤˜', 'ì°¾ì•„', 'ê²€ìƒ‰', 'ë‚˜ì™€', 'ë³´ì—¬ì¤˜', 'ì¶”ì ']): 
            intent = 'object_tracking'
        elif any(k in q for k in ['ìˆì–´?', 'ë‚˜ì™€?', 'ë“±ì¥í•´?']): 
            intent = 'object_presence'

        # ê¸°ì¡´ ìƒ‰ìƒ/ê°ì²´/ì‹œê°„ë²”ìœ„ ì²˜ë¦¬ (ë™ì¼)
        color_map = {
            'ë¹¨ê°•':'red','ë¹¨ê°„':'red','ì ìƒ‰':'red',
            'ì£¼í™©':'orange','ì˜¤ë Œì§€':'orange',
            'ë…¸ë‘':'yellow','ë…¸ë€':'yellow','í™©ìƒ‰':'yellow',
            'ì´ˆë¡':'green','ë…¹ìƒ‰':'green',
            'íŒŒë‘':'blue','íŒŒë€':'blue','ì²­ìƒ‰':'blue',
            'ë³´ë¼':'purple','ìì£¼':'purple',
            'ê²€ì •':'black','ê²€ì€':'black',
            'í•˜ì–‘':'white','í°':'white','ë°±ìƒ‰':'white',
            'íšŒìƒ‰':'gray','ê·¸ë ˆì´':'gray',
            'ê°ˆìƒ‰':'brown',
            'í•‘í¬':'pink','ë¶„í™':'pink',
        }
        colors = [v for k,v in color_map.items() if k in q]

        object_map = {
            'ì‚¬ëŒ':'person','ë‚¨ì„±':'person','ì—¬ì„±':'person','ì¸ë¬¼':'person',
            'ê°€ë°©':'handbag','í•¸ë“œë°±':'handbag',
            'tv':'tv','í‹°ë¹„':'tv','í…”ë ˆë¹„ì „':'tv',
            'ì˜ì':'chair',
            'ìì „ê±°':'bicycle',
            'ì°¨':'car','ìë™ì°¨':'car',
            'ê³ ì–‘ì´':'cat','ê°œ':'dog',
            'ë…¸íŠ¸ë¶':'laptop','íœ´ëŒ€í°':'cell_phone'
        }
        objects = []
        for k,v in object_map.items():
            if k in q:
                objects.append(v)
        objects = list(dict.fromkeys(objects))

        import re
        tmatch = re.search(r'(\d{1,2}:\d{2})\s*[-~]\s*(\d{1,2}:\d{2})', q)
        trange = None
        if tmatch:
            def to_sec(s):
                mm, ss = s.split(':')
                return int(mm) * 60 + int(ss)
            trange = {'start': to_sec(tmatch.group(1)), 'end': to_sec(tmatch.group(2))}

        return {'intent': intent, 'slots': {'colors': colors, 'objects': objects, 'time_range': trange}}

    def _handle_video_description(self, video: Video, raw_text: str, request=None):
        """LLMì„ í™œìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ìƒ ì„¤ëª… ìƒì„±"""
        try:
            # í”„ë ˆì„ë“¤ì˜ ìº¡ì…˜ ì •ë³´ ìˆ˜ì§‘
            frames = Frame.objects.filter(video=video).order_by('timestamp')
            
            if not frames.exists():
                return {'text': 'ì˜ìƒ ë¶„ì„ ë°ì´í„°ê°€ ì—†ì–´ì„œ ì„¤ëª…ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'items': []}
            
            # ëŒ€í‘œ ìº¡ì…˜ë“¤ ìˆ˜ì§‘ (ì „ì²´ ì˜ìƒì˜ 5-8ê°œ êµ¬ê°„)
            total_frames = frames.count()
            sample_count = min(8, max(5, total_frames // 6))  # 5-8ê°œ êµ¬ê°„
            sample_indices = [int(i * total_frames / sample_count) for i in range(sample_count)]
            
            key_scenes = []
            caption_data = []
            
            for idx in sample_indices:
                try:
                    frame = frames[idx] if idx < total_frames else frames.last()
                    
                    # ìµœê³  í’ˆì§ˆ ìº¡ì…˜ ì„ íƒ
                    best_caption = ""
                    if hasattr(frame, 'final_caption') and frame.final_caption:
                        best_caption = frame.final_caption
                    elif hasattr(frame, 'enhanced_caption') and frame.enhanced_caption:
                        best_caption = frame.enhanced_caption
                    elif hasattr(frame, 'caption') and frame.caption:
                        best_caption = frame.caption
                    elif hasattr(frame, 'blip_caption') and frame.blip_caption:
                        best_caption = frame.blip_caption
                    
                    if best_caption and len(best_caption.strip()) > 10:
                        scene_data = {
                            'timestamp': float(frame.timestamp),
                            'time_str': self._format_time(frame.timestamp),
                            'frame_id': frame.image_id,
                            'caption': best_caption.strip()
                        }
                        key_scenes.append(scene_data)
                        caption_data.append({
                            'time': scene_data['time_str'],
                            'caption': best_caption.strip()
                        })
                        
                except (IndexError, AttributeError):
                    continue
            
            if not caption_data:
                return {'text': 'ì˜ìƒ ìº¡ì…˜ ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'items': []}
            
            # LLMì„ ì‚¬ìš©í•´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª… ìƒì„±
            llm_description = self._generate_llm_description(video, caption_data, raw_text)
            
            # ëŒ€í‘œ ì¥ë©´ ì´ë¯¸ì§€ë“¤ (3-5ê°œ)
            representative_scenes = key_scenes[::max(1, len(key_scenes)//4)][:5]  # ìµœëŒ€ 5ê°œ ì„ íƒ
            items = []
            
            for scene in representative_scenes:
                if request:
                    media = self._frame_urls(request, video.id, scene['frame_id'])
                    clip = self._clip_url(request, video.id, scene['timestamp'])
                    items.append({
                        'time': scene['time_str'],
                        'seconds': int(scene['timestamp']),
                        'frame_id': scene['frame_id'],
                        'desc': scene['caption'][:120] + "..." if len(scene['caption']) > 120 else scene['caption'],
                        'full_caption': scene['caption'],
                        'source': 'AI ë¶„ì„',
                        'thumbUrl': media.get('image'),
                        'thumbBBoxUrl': media.get('image_bbox'),
                        'clipUrl': clip,
                    })
            
            return {'text': llm_description, 'items': items}
            
        except Exception as e:
            print(f"ì˜ìƒ ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
            return {'text': f'ì˜ìƒ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}', 'items': []}

    def _generate_llm_description(self, video: Video, caption_data, user_query):
        """LLMì„ ì‚¬ìš©í•´ì„œ ìº¡ì…˜ë“¤ì„ ë¶„ì„í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª… ìƒì„±"""
        try:
            if not self.llm_client:
                # LLMì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ëª… ìƒì„±
                return self._generate_fallback_description(video, caption_data)
            
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_description_prompt(video, caption_data, user_query)
            
            # LLM í˜¸ì¶œ
            llm_response = self.llm_client.generate_response(prompt)
            
            if llm_response and len(llm_response.strip()) > 50:
                return llm_response.strip()
            else:
                return self._generate_fallback_description(video, caption_data)
                
        except Exception as e:
            print(f"LLM ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
            return self._generate_fallback_description(video, caption_data)

    def _build_description_prompt(self, video: Video, caption_data, user_query):
        """LLMìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        prompt = f"""ì˜ìƒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ì˜ìƒ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ì˜ìƒ ì •ë³´:
    - íŒŒì¼ëª…: {video.original_name}
    - ê¸¸ì´: {round(video.duration, 1)}ì´ˆ
    - ì‚¬ìš©ì ì§ˆë¬¸: "{user_query}"

    ì‹œê°„ëŒ€ë³„ ë¶„ì„ ê²°ê³¼:
    """
        
        for data in caption_data:
            prompt += f"- {data['time']}: {data['caption']}\n"
        
        prompt += """
    ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

    1. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
    2. ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ ìš”ì•½í•˜ì—¬ ì •ë¦¬
    3. ì˜ìƒì˜ ì „ì²´ì ì¸ íë¦„ê³¼ ì£¼ìš” ë‚´ìš© ê°•ì¡°
    4. 2-3ê°œ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„± (ê° ë¬¸ë‹¨ì€ 2-4ë¬¸ì¥)
    5. ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ í”„ë ˆì„ ë²ˆí˜¸ ê°™ì€ ì •ë³´ëŠ” ì œì™¸
    6. ì˜ìƒì˜ ë¶„ìœ„ê¸°ë‚˜ ìƒí™©ì„ ìƒìƒí•˜ê²Œ ì „ë‹¬

    ì„¤ëª… í˜•ì‹:
    ì²« ë²ˆì§¸ ë¬¸ë‹¨: ì˜ìƒì˜ ì „ì²´ì ì¸ ë°°ê²½ê³¼ ìƒí™©
    ë‘ ë²ˆì§¸ ë¬¸ë‹¨: ì£¼ìš” ì¥ë©´ê³¼ í™œë™
    ì„¸ ë²ˆì§¸ ë¬¸ë‹¨: ì˜ìƒì˜ íŠ¹ì§•ì´ë‚˜ ì¸ìƒì ì¸ ë¶€ë¶„

    ì´ì œ ì˜ìƒ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""

        return prompt

    def _generate_fallback_description(self, video: Video, caption_data):
        """LLMì´ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê¸°ë³¸ ì„¤ëª… ìƒì„±"""
        
        description = f"'{video.original_name}' ì˜ìƒ ë¶„ì„\n\n"
        
        # ê¸°ë³¸ ì •ë³´
        description += f"ì´ ì˜ìƒì€ ì´ {round(video.duration, 1)}ì´ˆ ê¸¸ì´ì˜ ì˜ìƒì…ë‹ˆë‹¤.\n\n"
        
        # ì£¼ìš” ë‚´ìš© ìš”ì•½
        all_captions = " ".join([data['caption'] for data in caption_data]).lower()
        
        # ì¥ì†Œ ì¶”ì¶œ
        locations = []
        if 'ì‹¤ë‚´' in all_captions or 'indoor' in all_captions:
            locations.append('ì‹¤ë‚´')
        if 'ì‡¼í•‘ëª°' in all_captions:
            locations.append('ì‡¼í•‘ëª°')
        if 'ê±°ë¦¬' in all_captions:
            locations.append('ê±°ë¦¬')
        
        # ì‹œê°„ëŒ€ ì¶”ì¶œ
        time_info = []
        if 'ì˜¤í›„' in all_captions:
            time_info.append('ì˜¤í›„ ì‹œê°„')
        if 'ë°ì€' in all_captions:
            time_info.append('ë°ì€ í™˜ê²½')
        
        # í™œë™ ì¶”ì¶œ
        activities = []
        if 'ê±·' in all_captions:
            activities.append('ì‚¬ëŒë“¤ì´ ê±·ê³  ìˆëŠ”')
        if 'ì‡¼í•‘' in all_captions:
            activities.append('ì‡¼í•‘í•˜ëŠ”')
        
        # ì„¤ëª… êµ¬ì„±
        if locations:
            description += f"{', '.join(locations)}ì—ì„œ "
        if time_info:
            description += f"{', '.join(time_info)}ì— "
        if activities:
            description += f"{', '.join(activities)} ëª¨ìŠµì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.\n\n"
        
        # ì‹œê°„ëŒ€ë³„ ì£¼ìš” ë³€í™”
        if len(caption_data) >= 3:
            description += "ì˜ìƒ ì´ˆë°˜ì—ëŠ” "
            start_caption = caption_data[0]['caption']
            if 'ì‚¬ëŒ' in start_caption:
                description += "ì—¬ëŸ¬ ì‚¬ëŒë“¤ì´ ë“±ì¥í•˜ì—¬ "
            if 'ê±·' in start_caption:
                description += "ì´ë™í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì£¼ë©°, "
            
            description += "ì¤‘ë°˜ë¶€ì—ëŠ” "
            mid_caption = caption_data[len(caption_data)//2]['caption']
            if 'í™œë™' in mid_caption or 'ì‡¼í•‘' in mid_caption:
                description += "ë‹¤ì–‘í•œ í™œë™ë“¤ì´ ì´ì–´ì§‘ë‹ˆë‹¤. "
            
            description += "ì „ì²´ì ìœ¼ë¡œ ì¼ìƒì ì¸ ì¥ë©´ë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ëœ ì˜ìƒì…ë‹ˆë‹¤."
        
        return description

    def _generate_comprehensive_description(self, video: Video, key_scenes, detailed_captions):
        """ìˆ˜ì§‘ëœ ìº¡ì…˜ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ì˜ìƒ ì„¤ëª… ìƒì„±"""
        
        # 1. ê¸°ë³¸ ì •ë³´
        description = f"ğŸ“¹ '{video.original_name}' ì˜ìƒ ë¶„ì„ ê²°ê³¼\n\n"
        description += f"â±ï¸ ê¸¸ì´: {round(video.duration, 1)}ì´ˆ\n"
        description += f"ğŸ¬ ì´ {len(key_scenes)}ê°œ ì£¼ìš” ì¥ë©´ ë¶„ì„\n\n"
        
        # 2. ì „ì²´ì ì¸ íŠ¹ì§• ì¶”ì¶œ
        all_text = " ".join(detailed_captions).lower()
        
        # ì¥ì†Œ/í™˜ê²½ ì •ë³´
        locations = []
        if 'ì‹¤ë‚´' in all_text or 'indoor' in all_text:
            locations.append('ì‹¤ë‚´')
        if 'ì‹¤ì™¸' in all_text or 'outdoor' in all_text:
            locations.append('ì‹¤ì™¸')
        if 'ì‡¼í•‘ëª°' in all_text:
            locations.append('ì‡¼í•‘ëª°')
        if 'ê±°ë¦¬' in all_text or 'sidewalk' in all_text:
            locations.append('ê±°ë¦¬')
        if 'ê±´ë¬¼' in all_text or 'building' in all_text:
            locations.append('ê±´ë¬¼')
        
        # ì‹œê°„ëŒ€ ì •ë³´
        time_info = []
        if 'ì˜¤í›„' in all_text or 'afternoon' in all_text:
            time_info.append('ì˜¤í›„')
        if 'ì•„ì¹¨' in all_text or 'morning' in all_text:
            time_info.append('ì•„ì¹¨')
        if 'ë°¤' in all_text or 'night' in all_text:
            time_info.append('ë°¤')
        if 'ë°ì€' in all_text or 'bright' in all_text:
            time_info.append('ë°ì€ í™˜ê²½')
        
        # ì£¼ìš” ê°ì²´/í™œë™
        detected_objects = set()
        activities = set()
        
        for caption in detailed_captions:
            caption_lower = caption.lower()
            # ê°ì²´ ì¶”ì¶œ
            if 'ì‚¬ëŒ' in caption_lower or 'person' in caption_lower:
                detected_objects.add('ì‚¬ëŒ')
            if 'ê°€ë°©' in caption_lower or 'handbag' in caption_lower:
                detected_objects.add('ê°€ë°©')
            if 'tv' in caption_lower or 'í‹°ë¹„' in caption_lower:
                detected_objects.add('TV')
            if 'ì˜ì' in caption_lower or 'chair' in caption_lower:
                detected_objects.add('ì˜ì')
            
            # í™œë™ ì¶”ì¶œ
            if 'ê±·' in caption_lower or 'walking' in caption_lower:
                activities.add('ê±·ê¸°')
            if 'ì„œ' in caption_lower or 'standing' in caption_lower:
                activities.add('ì„œìˆê¸°')
            if 'ì‡¼í•‘' in caption_lower or 'shopping' in caption_lower:
                activities.add('ì‡¼í•‘')
            if 'ëŒ€í™”' in caption_lower or 'talking' in caption_lower:
                activities.add('ëŒ€í™”')
        
        # 3. ì¢…í•© ì„¤ëª…
        description += "ğŸï¸ **ì˜ìƒ ê°œìš”:**\n"
        
        if locations:
            description += f"- ì¥ì†Œ: {', '.join(locations)}\n"
        if time_info:
            description += f"- ì‹œê°„/í™˜ê²½: {', '.join(time_info)}\n"
        if detected_objects:
            description += f"- ì£¼ìš” ê°ì²´: {', '.join(list(detected_objects)[:5])}\n"
        if activities:
            description += f"- ì£¼ìš” í™œë™: {', '.join(list(activities)[:3])}\n"
        
        description += "\n"
        
        # 4. ì‹œê°„ëŒ€ë³„ ì£¼ìš” ì¥ë©´ (ì²˜ìŒ, ì¤‘ê°„, ë 3ê°œ êµ¬ê°„)
        if len(key_scenes) >= 3:
            description += "ğŸï¸ **ì£¼ìš” ì¥ë©´ ìš”ì•½:**\n\n"
            
            # ì‹œì‘ ì¥ë©´
            start_scene = key_scenes[0]
            description += f"**{start_scene['time_str']} (ì‹œì‘):** {start_scene['caption'][:150]}...\n\n"
            
            # ì¤‘ê°„ ì¥ë©´
            mid_scene = key_scenes[len(key_scenes)//2]
            description += f"**{mid_scene['time_str']} (ì¤‘ë°˜):** {mid_scene['caption'][:150]}...\n\n"
            
            # ë ì¥ë©´
            end_scene = key_scenes[-1]
            description += f"**{end_scene['time_str']} (ì¢…ë£Œ):** {end_scene['caption'][:150]}...\n\n"
        
        # 5. ì¶”ê°€ ì •ë³´
        description += "ğŸ’¡ **ë¶„ì„ ì •ë³´:**\n"
        description += f"- ë¶„ì„ ìƒíƒœ: {video.analysis_status}\n"
        description += f"- í”„ë ˆì„ ê¸°ë°˜ AI ë¶„ì„ì„ í†µí•´ ìƒì„±ëœ ì„¤ëª…ì…ë‹ˆë‹¤\n"
        description += f"- ì•„ë˜ ì´ë¯¸ì§€ë“¤ì„ í´ë¦­í•˜ë©´ í•´ë‹¹ ì‹œì ì˜ ìƒì„¸ ì¥ë©´ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
        
        return description
    # ---------- Frame JSON í†µì¼ ----------
    def _get_detected_objects(self, frame: Frame):
        """
        Frame.detected_objects(JSONField/CharField) â†’ list[dict] ë¡œ í†µì¼ ë°˜í™˜
        ê°ì²´ ì˜ˆì‹œ: {class:'person', bbox:[x1,y1,x2,y2], colors:['green'], color_description:'green-mixed', confidence:0.7, gender:'male', track_id:'t1'}
        """
        data = []
        raw = getattr(frame, 'detected_objects', None)
        if not raw:
            return data
        try:
            if isinstance(raw, str):
                data = json.loads(raw)
            elif isinstance(raw, (list, dict)):
                data = raw
        except Exception:
            return []
        if isinstance(data, dict):
            # {objects:[...]} í˜•íƒœë„ í—ˆìš©
            data = data.get('objects', [])
        # ì•ˆì „ í•„ë“œ ë³´ì •
        norm = []
        for o in data:
            norm.append({
                'class': (o.get('class') or o.get('label') or '').lower(),
                'bbox': o.get('bbox') or o.get('box') or [],
                'colors': o.get('colors') or [],
                'color_description': (o.get('color_description') or o.get('color') or 'unknown').lower(),
                'confidence': float(o.get('confidence', 0.5)),
                'gender': (o.get('gender') or '').lower(),
                'track_id': o.get('track_id') or o.get('id'),
            })
        return norm

    # ---------- POST ----------

    def post(self, request):
        try:
            self._initialize_services()
            user_query = (request.data.get('message') or '').strip()
            video_id = request.data.get('video_id')

            if not user_query:
                return Response({'response': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

            video = self._get_video_safe(video_id)
            if not video:
                return Response({'response': 'ë¶„ì„ëœ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œ/ë¶„ì„ í›„ ì´ìš©í•´ì£¼ì„¸ìš”.'})

            nlu = self._nlu(user_query)
            intent, slots = nlu['intent'], nlu['slots']

            # ì˜ìƒ ì„¤ëª… ì²˜ë¦¬ ì¶”ê°€
            if intent == 'video_description':
                out = self._handle_video_description(video, user_query, request=request)
            elif intent == 'object_tracking':
                out = self._handle_object_tracking(video, slots, user_query, request=request)
            elif intent == 'object_presence':
                out = self._handle_object_presence(video, user_query, slots, request=request)
            elif intent == 'gender_distribution':
                out = {'text': self._handle_gender_distribution(video, slots), 'items': []}
            elif intent == 'scene_mood':
                out = {'text': self._handle_scene_mood(video), 'items': []}
            elif intent == 'cross_video':
                out = {'text': self._handle_cross_video(user_query), 'items': []}
            elif intent == 'summary':
                out = self._handle_summary(video, request=request)
            elif intent == 'highlight':
                out = self._handle_highlight(video, request=request)
            elif intent == 'info':
                out = {'text': self._handle_info(video), 'items': []}
            else:
                out = {'text': f"'{user_query}' ì§ˆë¬¸ í™•ì¸! ìƒ‰ìƒ/ê°ì²´/ì‹œê°„ë²”ìœ„ë¥¼ í•¨ê»˜ ì£¼ì‹œë©´ ë” ì •í™•í•´ìš”. ì˜ˆ) 'ì´ˆë¡ ìƒì˜ ì‚¬ëŒ 0:05~0:10'", 'items': []}

            return Response({
                'response': out['text'],
                'video_id': video.id,
                'video_name': video.original_name,
                'query_type': intent,
                'timestamp': time.time(),
                'items': out.get('items', []),
            })

        except Exception as e:
            print(f"[EnhancedVideoChatView] ì˜¤ë¥˜: {e}")
            return Response({'response': f"ì§ˆë¬¸ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", 'fallback': True})
    # ---------- Intent Handlers ----------
    def _handle_object_tracking(self, video: Video, slots: dict, raw_text: str, request=None):
        """ìƒ‰/ê°ì²´/ì‹œê°„ ë²”ìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ ë§¤ì¹­ ì¥ë©´ + ì¸ë„¤ì¼/í´ë¦½ ë°˜í™˜"""
        colors = set(slots.get('colors') or [])
        objects = set(slots.get('objects') or ['person'])  # ê¸°ë³¸ ì‚¬ëŒ
        tr = slots.get('time_range')

        frames_qs = Frame.objects.filter(video=video).order_by('timestamp')
        if tr and tr.get('start') is not None and tr.get('end') is not None:
            frames_qs = frames_qs.filter(timestamp__gte=tr['start'], timestamp__lte=tr['end'])

        hits = []
        for f in frames_qs:
            dets = self._get_detected_objects(f)
            if not dets: continue
            for d in dets:
                score, reasons = 0.0, []
                # ê°ì²´ ë§¤ì¹­
                if objects:
                    if d['class'] in objects:
                        score += 0.5
                        reasons.append(f"{d['class']} ê°ì²´")
                    elif any(o in d['class'] for o in objects):
                        score += 0.3
                        reasons.append(f"{d['class']} ìœ ì‚¬ ê°ì²´")
                # ìƒ‰ìƒ ë§¤ì¹­
                if colors:
                    hit = False
                    cd = d['color_description']
                    if any(c in cd for c in colors):
                        hit = True
                    if not hit and d['colors']:
                        if any(c in (str(x).lower()) for x in d['colors'] for c in colors):
                            hit = True
                    if hit:
                        score += 0.3
                        reasons.append("ìƒ‰ìƒ ë§¤ì¹­")

                if score >= 0.5:
                    hits.append({
                        't': float(f.timestamp),
                        'time': self._format_time(f.timestamp),
                        'frame_id': f.image_id,
                        'desc': f"{d.get('color_description','')} {d.get('class','object')}".strip(),
                        'score': min(1.0, (score + d.get('confidence', 0.5) * 0.2)),
                        'reasons': reasons,
                        'track': d.get('track_id') or '',
                    })

        if not hits:
            return {'text': f"â€˜{raw_text}â€™ë¡œëŠ” ë§¤ì¹­ì´ ì—†ì—ˆì–´ìš”. ì‹œê°„ ë²”ìœ„ë¥¼ ë„“íˆê±°ë‚˜ ìƒ‰ìƒ ì—†ì´ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”.", 'items': []}

        # ì •ë ¬ + ì¤‘ë³µ ì œê±° + ìƒìœ„ 10ê°œ
        hits.sort(key=lambda x: (-x['score'], x['t']))
        uniq, seen = [], set()
        for h in hits:
            key = (int(h['t']), h['desc'])
            if key in seen: continue
            seen.add(key)
            media = self._frame_urls(request, video.id, h['frame_id']) if request else {}
            clip = self._clip_url(request, video.id, h['t']) if request else None
            uniq.append({
                'time': h['time'],
                'seconds': int(h['t']),
                'frame_id': h['frame_id'],
                'desc': h['desc'],
                'score': h['score'],
                'reasons': h['reasons'],
                'thumbUrl': media.get('image'),
                'thumbBBoxUrl': media.get('image_bbox'),
                'clipUrl': clip,
            })
            if len(uniq) >= 10: break

        text = "ğŸ” ìš”ì²­í•˜ì‹  ì¥ë©´ì„ ì°¾ì•˜ì–´ìš” (ìƒìœ„ {n}ê°œ):\n".format(n=len(uniq))
        text += "\n".join([f"- {it['time']} Â· {it['desc']} Â· ~{int(it['score']*100)}%" for it in uniq])
        return {'text': text, 'items': uniq}

    def _handle_object_presence(self, video: Video, raw_text: str, slots: dict, request=None):
        """íŠ¹ì • ê°ì²´/í‚¤ì›Œë“œ ë“±ì¥ ì—¬ë¶€ ê°„ë‹¨ í™•ì¸ + ì¸ë„¤ì¼"""
        objs = slots.get('objects') or []
        q = raw_text.lower()
        frames = Frame.objects.filter(video=video).order_by('timestamp')[:100]
        hits = []
        for f in frames:
            cap = (f.final_caption or f.enhanced_caption or f.caption or '').lower()
            dets = self._get_detected_objects(f)
            ok = False
            reason = ""
            if objs and any(o in (cap or '') for o in objs):
                ok, reason = True, "ìº¡ì…˜ ë§¤ì¹­"
            if not ok and dets:
                if objs and any(d['class'] in objs for d in dets):
                    ok, reason = True, "ê°ì²´ ë§¤ì¹­"
                elif any(k in cap for k in q.split()):
                    ok, reason = True, "í‚¤ì›Œë“œ ë§¤ì¹­"

            if ok:
                media = self._frame_urls(request, video.id, f.image_id)
                clip = self._clip_url(request, video.id, f.timestamp)
                hits.append({
                    'time': self._format_time(f.timestamp),
                    'seconds': int(f.timestamp),
                    'frame_id': f.image_id,
                    'desc': (f.final_caption or f.enhanced_caption or f.caption or '').strip()[:120],
                    'thumbUrl': media['image'],
                    'thumbBBoxUrl': media['image_bbox'],
                    'clipUrl': clip,
                })
            if len(hits) >= 10: break

        if not hits:
            return {'text': "í•´ë‹¹ í‚¤ì›Œë“œ/ê°ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.", 'items': []}
        text = "âœ… ì°¾ì•˜ìŠµë‹ˆë‹¤:\n" + "\n".join([f"- {h['time']} Â· {h['desc']}" for h in hits])
        return {'text': text, 'items': hits}

    def _handle_highlight(self, video: Video, request=None):
        """ìƒìœ„ 5ê°œ ì”¬ + ê° ì”¬ ëŒ€í‘œ ì¸ë„¤ì¼/í´ë¦½"""
        scenes = Scene.objects.filter(video=video).order_by('start_time')[:5]
        if not scenes:
            return {'text': "í•˜ì´ë¼ì´íŠ¸ê°€ ì•„ì§ ì—†ì–´ìš”. ë¶„ì„ì´ ëë‚¬ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.", 'items': []}

        items, lines = [], []
        for s in scenes:
            mid = (s.start_time + s.end_time) / 2.0
            f = Frame.objects.filter(video=video, timestamp__gte=mid).order_by('timestamp').first() or \
                Frame.objects.filter(video=video).order_by('-timestamp').first()
            media = self._frame_urls(request, video.id, f.image_id) if f else {}
            clip = self._clip_url(request, video.id, mid) if f else None
            objs = (s.dominant_objects or [])[:5]
            items.append({
                'range': [int(s.start_time), int(s.end_time)],
                'start': self._format_time(s.start_time),
                'end': self._format_time(s.end_time),
                'objects': objs,
                'thumbUrl': media.get('image'),
                'thumbBBoxUrl': media.get('image_bbox'),
                'clipUrl': clip,
            })
            lines.append(f"- {self._format_time(s.start_time)}â€“{self._format_time(s.end_time)} Â· {', '.join(objs) or 'ì¥ë©´'}")

        return {'text': "âœ¨ ì£¼ìš” ì¥ë©´:\n" + "\n".join(lines), 'items': items}

    def _handle_summary(self, video: Video, request=None):
        """ê°„ë‹¨ ìš”ì•½ + ëŒ€í‘œ ì¸ë„¤ì¼ ëª‡ ì¥"""
        summary = [
            f"â€˜{video.original_name}â€™ ìš”ì•½",
            f"- ê¸¸ì´: {round(video.duration,2)}ì´ˆ Â· ë¶„ì„ ìƒíƒœ: {video.analysis_status}",
        ]
        try:
            analysis = getattr(video, 'analysis', None)
            if analysis and analysis.analysis_statistics:
                stats = analysis.analysis_statistics
                dom = stats.get('dominant_objects', [])[:5]
                if dom:
                    summary.append(f"- ì£¼ìš” ê°ì²´: {', '.join(dom)}")
                scene_types = stats.get('scene_types', [])[:3]
                if scene_types:
                    summary.append(f"- ì¥ë©´ ìœ í˜•: {', '.join(scene_types)}")
        except:
            pass

        frames = Frame.objects.filter(video=video).order_by('timestamp')[:6]
        items = []
        for f in frames:
            media = self._frame_urls(request, video.id, f.image_id)
            clip = self._clip_url(request, video.id, f.timestamp)
            items.append({
                'time': self._format_time(f.timestamp),
                'seconds': int(f.timestamp),
                'frame_id': f.image_id,
                'desc': (f.final_caption or f.enhanced_caption or f.caption or '').strip()[:120],
                'thumbUrl': media['image'],
                'thumbBBoxUrl': media['image_bbox'],
                'clipUrl': clip,
            })

        return {'text': "\n".join(summary), 'items': items}

    def _handle_info(self, video: Video):
        sc = Scene.objects.filter(video=video).count()
        fc = Frame.objects.filter(video=video).count()
        return "\n".join([
            "ë¹„ë””ì˜¤ ì •ë³´",
            f"- íŒŒì¼ëª…: {video.original_name}",
            f"- ê¸¸ì´: {round(video.duration,2)}ì´ˆ",
            f"- ë¶„ì„ ìƒíƒœ: {video.analysis_status}",
            f"- ì”¬ ìˆ˜: {sc}ê°œ",
            f"- ë¶„ì„ í”„ë ˆì„: {fc}ê°œ",
        ])


    def _enhance_person_detection_with_gender(self, frame_data):
        """ì‚¬ëŒ ê°ì§€ ë°ì´í„°ì— ì„±ë³„ ì •ë³´ ë³´ê°• (ë¶„ì„ ì‹œì ì—ì„œ í˜¸ì¶œ)"""
        try:
            if not frame_data or not isinstance(frame_data, list):
                return frame_data
            
            enhanced_data = []
            for obj in frame_data:
                if not isinstance(obj, dict) or obj.get('class') != 'person':
                    enhanced_data.append(obj)
                    continue
                
                enhanced_obj = obj.copy()
                
                # ê¸°ì¡´ ì„±ë³„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¶”ì •
                if not enhanced_obj.get('gender'):
                    # ì—¬ê¸°ì„œ ì¶”ê°€ì ì¸ ì„±ë³„ ë¶„ì„ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŒ
                    # ì˜ˆ: ì˜ë³µ, ì²´í˜•, ë¨¸ë¦¬ì¹´ë½ ë“± ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
                    
                    # ì„ì‹œ: ëœë¤í•˜ê²Œ ì„±ë³„ í• ë‹¹ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„ì„ í•„ìš”)
                    import random
                    if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ì„±ë³„ ì¶”ì •
                        enhanced_obj['gender'] = random.choice(['male', 'female'])
                        enhanced_obj['gender_confidence'] = 0.6  # ë‚®ì€ ì‹ ë¢°ë„
                    else:
                        enhanced_obj['gender'] = 'unknown'
                        enhanced_obj['gender_confidence'] = 0.0
                
                enhanced_data.append(enhanced_obj)
            
            return enhanced_data
        except Exception as e:
            logger.warning(f"ì„±ë³„ ì •ë³´ ë³´ê°• ì‹¤íŒ¨: {e}")
            return frame_data

    def _get_detected_objects(self, frame: Frame):
        """
        Frame ê°ì²´ ì¶”ì¶œ ì‹œ ì„±ë³„ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
        """
        import json

        candidates = []

        # 1) detected_objects
        if hasattr(frame, 'detected_objects') and frame.detected_objects:
            candidates.append(frame.detected_objects)

        # 2) comprehensive_features.objects  
        if hasattr(frame, 'comprehensive_features') and frame.comprehensive_features:
            objs = None
            if isinstance(frame.comprehensive_features, dict):
                objs = frame.comprehensive_features.get('objects') \
                or frame.comprehensive_features.get('detections')
            elif isinstance(frame.comprehensive_features, str):
                try:
                    cf = json.loads(frame.comprehensive_features)
                    objs = (cf or {}).get('objects') or (cf or {}).get('detections')
                except Exception:
                    pass
            if objs:
                candidates.append(objs)

        # 3) ê¸°íƒ€ í•„ë“œë“¤
        for attr in ('yolo_objects', 'detections', 'objects'):
            if hasattr(frame, attr) and getattr(frame, attr):
                candidates.append(getattr(frame, attr))

        # ì²« ë²ˆì§¸ ìœ íš¨ í›„ë³´ ì„ íƒ
        detected = None
        for c in candidates:
            try:
                if isinstance(c, str):
                    c = json.loads(c)
                if isinstance(c, dict):
                    c = c.get('objects') or c.get('detections')
                if isinstance(c, list):
                    detected = c
                    break
            except Exception:
                continue

        if not isinstance(detected, list):
            return []

        # ì •ê·œí™” - ì„±ë³„ ì •ë³´ í¬í•¨
        norm = []
        for o in detected:
            if not isinstance(o, dict):
                continue
            
            cls = (o.get('class') or o.get('label') or o.get('name') or '').lower()
            bbox = o.get('bbox') or o.get('box') or o.get('xyxy') or []
            conf = float(o.get('confidence') or o.get('score') or 0.0)
            colors = o.get('colors') or o.get('color') or []
            if isinstance(colors, str):
                colors = [colors]
            color_desc = (o.get('color_description') or o.get('dominant_color') or 'unknown')
            track_id = o.get('track_id') or o.get('id')
            
            # ì„±ë³„ ì •ë³´ ì¶”ì¶œ ê°œì„ 
            gender = o.get('gender') or o.get('sex') or 'unknown'
            if isinstance(gender, bool):
                gender = 'male' if gender else 'female'
            gender = str(gender).lower()
            
            # ì„±ë³„ ì‹ ë¢°ë„
            gender_conf = float(o.get('gender_confidence') or o.get('gender_score') or 0.0)

            norm.append({
                'class': cls,
                'bbox': bbox,
                'confidence': conf,
                'colors': colors,
                'color_description': str(color_desc).lower(),
                'track_id': track_id,
                'gender': gender,
                'gender_confidence': gender_conf,
                '_raw': o,  # ì›ë³¸ ë°ì´í„°ë„ ë³´ê´€
            })
        return norm
    def _handle_scene_mood(self, video: Video):
        """ì”¬ íƒ€ì… ê¸°ë°˜ ê°„ë‹¨ ë¬´ë“œ ì„¤ëª…"""
        try:
            analysis = getattr(video, 'analysis', None)
            if analysis and analysis.analysis_statistics:
                types = (analysis.analysis_statistics.get('scene_types') or [])[:3]
                if types:
                    return f"ë¶„ìœ„ê¸°: {', '.join(types)}"
        except:
            pass
        return "ë¶„ìœ„ê¸° ì •ë³´ë¥¼ íŒŒì•…í•  ë‹¨ì„œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."

    def _handle_cross_video(self, raw_text: str):
        """ì—¬ëŸ¬ ì˜ìƒ ì¤‘ ì¡°ê±´ì— ë§ëŠ” í›„ë³´ ëª…ì‹œ (ì—¬ê¸°ì„  ì„¤ëª…ë§Œ)"""
        return "ì—¬ëŸ¬ ì˜ìƒ ê°„ ì¡°ê±´ ê²€ìƒ‰ì€ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. UIì—ì„œ ëª©ë¡/í•„í„°ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”."
    def _handle_gender_distribution(self, video: Video, slots: dict):
        """ì„±ë³„ ë¶„í¬ ë¶„ì„ - ê°œì„ ëœ ë²„ì „"""
        tr = slots.get('time_range')
        qs = Frame.objects.filter(video=video)
        if tr and tr.get('start') is not None and tr.get('end') is not None:
            qs = qs.filter(timestamp__gte=tr['start'], timestamp__lte=tr['end'])

        male = female = unknown = 0
        person_detections = []
        
        for f in qs:
            detected_objects = self._get_detected_objects(f)
            for d in detected_objects:
                if d['class'] != 'person': 
                    continue
                
                person_detections.append(d)
                
                # ì„±ë³„ ì •ë³´ ì¶”ì¶œ - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
                gender = None
                
                # 1. ì§ì ‘ì ì¸ gender í•„ë“œ
                if 'gender' in d and d['gender'] and d['gender'] != 'unknown':
                    gender = str(d['gender']).lower()
                
                # 2. ì›ë³¸ ë°ì´í„°ì—ì„œ ì„±ë³„ ì •ë³´ ì°¾ê¸°
                elif '_raw' in d and d['_raw']:
                    raw = d['_raw']
                    for key in ['gender', 'sex', 'male', 'female']:
                        if key in raw and raw[key]:
                            val = str(raw[key]).lower()
                            if val in ['male', 'man', 'm', 'true'] and key in ['male', 'gender']:
                                gender = 'male'
                                break
                            elif val in ['female', 'woman', 'f', 'true'] and key in ['female', 'gender']:
                                gender = 'female'  
                                break
                            elif val in ['male', 'female']:
                                gender = val
                                break
                
                # 3. ìƒ‰ìƒ/ì˜ë³µ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì¶”ì • (ë³´ì¡°ì )
                if not gender:
                    color_desc = d.get('color_description', '').lower()
                    colors = [str(c).lower() for c in d.get('colors', [])]
                    
                    # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± (ì •í™•ë„ ë‚®ìŒ, ì°¸ê³ ìš©)
                    if any('pink' in x for x in [color_desc] + colors):
                        gender = 'female_guess'
                    elif any('blue' in x for x in [color_desc] + colors):
                        gender = 'male_guess'
                
                # ì¹´ìš´íŒ…
                if gender in ['male', 'male_guess']:
                    male += 1
                elif gender in ['female', 'female_guess']:
                    female += 1
                else:
                    unknown += 1

        total = male + female + unknown
        
        if total == 0:
            return "ì˜ìƒì—ì„œ ì‚¬ëŒì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        # ê²°ê³¼ í¬ë§·íŒ…
        def pct(x): 
            return round(100.0 * x / total, 1) if total > 0 else 0
        
        result = f"ì„±ë¹„ ë¶„ì„ ê²°ê³¼ (ì´ {total}ëª… ê°ì§€):\n"
        result += f"ğŸ‘¨ ë‚¨ì„±: {male}ëª… ({pct(male)}%)\n"
        result += f"ğŸ‘© ì—¬ì„±: {female}ëª… ({pct(female)}%)\n"
        result += f"â“ ë¯¸ìƒ: {unknown}ëª… ({pct(unknown)}%)\n\n"
        
        # ì¶”ê°€ ì •ë³´
        if unknown > total * 0.8:  # 80% ì´ìƒì´ ë¯¸ìƒì¸ ê²½ìš°
            result += "ğŸ’¡ ì„±ë³„ ì¶”ì • ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒ ì´ìœ ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
            result += "- ì˜ìƒ í•´ìƒë„ë‚˜ ê°ë„ ë¬¸ì œ\n"
            result += "- ì‚¬ëŒì´ ë©€ë¦¬ ìˆê±°ë‚˜ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë³´ì„\n"
            result += "- AI ëª¨ë¸ì˜ ì„±ë³„ ë¶„ì„ ê¸°ëŠ¥ ì œí•œ\n\n"
        
        # ë””ë²„ê¹… ì •ë³´ (ê°œë°œ ì‹œì—ë§Œ í‘œì‹œ)
        result += f"ğŸ” ë””ë²„ê·¸ ì •ë³´:\n"
        result += f"- ì²˜ë¦¬ëœ í”„ë ˆì„ ìˆ˜: {qs.count()}ê°œ\n"
        result += f"- ê°ì§€ëœ person ê°ì²´: {len(person_detections)}ê°œ\n"
        
        if person_detections:
            sample_detection = person_detections[0]
            result += f"- ìƒ˜í”Œ ê°ì²´ ì •ë³´: {sample_detection.get('gender', 'N/A')} (ì‹ ë¢°ë„: {sample_detection.get('gender_confidence', 0)})\n"
        
        # ì‹œê°„ ë²”ìœ„ ì •ë³´
        if tr:
            result += f"ğŸ“… ë¶„ì„ êµ¬ê°„: {tr.get('start', 'ì‹œì‘')}~{tr.get('end', 'ë')}"
        else:
            result += f"ğŸ“… ë¶„ì„ êµ¬ê°„: ì „ì²´ ì˜ìƒ"
        
        return result
# views.py (ë™ì¼ íŒŒì¼ ë‚´)
class ClipPreviewView(APIView):
    """ffmpeg ë¡œ ì§§ì€ ë¯¸ë¦¬ë³´ê¸° í´ë¦½ ìƒì„±/ë°˜í™˜"""
    permission_classes = [AllowAny]

    def get(self, request, video_id, timestamp):
        duration = int(request.GET.get('duration', 4))
        try:
            video = Video.objects.get(id=video_id)
        except Video.DoesNotExist:
            raise Http404("video not found")

        src_path = getattr(getattr(video, 'file', None), 'path', None)
        if not src_path or not os.path.exists(src_path):
            raise Http404("file not found")

        tmp_dir = tempfile.mkdtemp()
        out_path = os.path.join(tmp_dir, f"clip_{video_id}_{timestamp}.mp4")

        cmd = [
            'ffmpeg','-y',
            '-ss', str(int(timestamp)),
            '-i', src_path,
            '-t', str(duration),
            '-c:v', 'libx264', '-preset', 'veryfast', '-crf', '28',
            '-an',
            out_path
        ]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except subprocess.CalledProcessError:
            raise Http404("ffmpeg error")

        resp = FileResponse(open(out_path, 'rb'), content_type='video/mp4')
        resp['Content-Disposition'] = f'inline; filename="clip_{video_id}_{timestamp}.mp4"'
        return resp


# ========== ì˜ìƒ ê°„ ê²€ìƒ‰ì„ ìœ„í•œ í—¬í¼ ë©”ì„œë“œë“¤ ==========

def _check_weather_match(frames, weather_condition):
    """ë‚ ì”¨ ì¡°ê±´ ë§¤ì¹­ í™•ì¸"""
    try:
        weather_keywords = {
            'rainy': ['ë¹„', 'rain', 'wet', 'umbrella', 'puddle', 'drizzle'],
            'sunny': ['sunny', 'sun', 'bright', 'sunlight', 'í–‡ë¹›', 'ë§‘ì€'],
            'cloudy': ['cloudy', 'cloud', 'overcast', 'íë¦¼', 'êµ¬ë¦„']
        }
        
        target_keywords = weather_keywords.get(weather_condition.lower(), [])
        if not target_keywords:
            return 0.0
        
        match_count = 0
        total_frames = len(frames)
        
        for frame in frames:
            caption = (frame.caption or '') + ' ' + (frame.enhanced_caption or '') + ' ' + (frame.final_caption or '')
            caption_lower = caption.lower()
            
            for keyword in target_keywords:
                if keyword in caption_lower:
                    match_count += 1
                    break
        
        if match_count > 0:
            return (match_count / total_frames) * 0.8
        
        return 0.0
        
    except Exception as e:
        logger.error(f"âŒ ë‚ ì”¨ ë§¤ì¹­ í™•ì¸ ì˜¤ë¥˜: {e}")
        return 0.0


def _check_time_match(video, time_condition):
    """ì‹œê°„ëŒ€ ì¡°ê±´ ë§¤ì¹­ í™•ì¸"""
    try:
        created_hour = video.created_at.hour
        
        time_mapping = {
            'morning': (6, 12),
            'afternoon': (12, 18),
            'evening': (18, 22),
            'night': (22, 6)
        }
        
        if time_condition.lower() in time_mapping:
            start_hour, end_hour = time_mapping[time_condition.lower()]
            
            if time_condition.lower() == 'night':
                if created_hour >= 22 or created_hour < 6:
                    return 0.7
            else:
                if start_hour <= created_hour < end_hour:
                    return 0.7
        
        return 0.0
        
    except Exception as e:
        logger.error(f"âŒ ì‹œê°„ëŒ€ ë§¤ì¹­ í™•ì¸ ì˜¤ë¥˜: {e}")
        return 0.0


def _check_location_match(frames, location_condition):
    """ì¥ì†Œ ì¡°ê±´ ë§¤ì¹­ í™•ì¸"""
    try:
        location_keywords = {
            'indoor': ['ì‹¤ë‚´', 'indoor', 'inside', 'room', 'building', 'office', 'home'],
            'outdoor': ['ì‹¤ì™¸', 'outdoor', 'outside', 'street', 'park', 'garden', 'outdoor'],
            'office': ['office', 'ì‚¬ë¬´ì‹¤', 'desk', 'computer', 'meeting'],
            'home': ['home', 'ì§‘', 'house', 'living room', 'bedroom']
        }
        
        target_keywords = location_keywords.get(location_condition.lower(), [])
        if not target_keywords:
            return 0.0
        
        match_count = 0
        total_frames = len(frames)
        
        for frame in frames:
            caption = (frame.caption or '') + ' ' + (frame.enhanced_caption or '') + ' ' + (frame.final_caption or '')
            caption_lower = caption.lower()
            
            for keyword in target_keywords:
                if keyword in caption_lower:
                    match_count += 1
                    break
        
        if match_count > 0:
            return (match_count / total_frames) * 0.8
        
        return 0.0
        
    except Exception as e:
        logger.error(f"âŒ ì¥ì†Œ ë§¤ì¹­ í™•ì¸ ì˜¤ë¥˜: {e}")
        return 0.0


def _check_object_match(frames, object_condition):
    """ê°ì²´ ì¡°ê±´ ë§¤ì¹­ í™•ì¸"""
    try:
        object_keywords = {
            'person': ['person', 'ì‚¬ëŒ', 'people', 'human'],
            'car': ['car', 'ìë™ì°¨', 'vehicle', 'automobile'],
            'bag': ['bag', 'ê°€ë°©', 'handbag', 'backpack'],
            'phone': ['phone', 'ì „í™”', 'smartphone', 'mobile']
        }
        
        target_keywords = object_keywords.get(object_condition.lower(), [])
        if not target_keywords:
            return 0.0
        
        match_count = 0
        total_frames = len(frames)
        
        for frame in frames:
            # detected_objectsì—ì„œ ê°ì²´ í™•ì¸
            detected_objects = _get_detected_objects_from_frame(frame)
            for obj in detected_objects:
                obj_class = obj.get('class', '').lower()
                if obj_class in target_keywords:
                    match_count += 1
                    break
        
        if match_count > 0:
            return (match_count / total_frames) * 0.8
        
        return 0.0
        
    except Exception as e:
        logger.error(f"âŒ ê°ì²´ ë§¤ì¹­ í™•ì¸ ì˜¤ë¥˜: {e}")
        return 0.0


def _check_activity_match(frames, activity_condition):
    """í™œë™ ì¡°ê±´ ë§¤ì¹­ í™•ì¸"""
    try:
        activity_keywords = {
            'walking': ['walking', 'ê±·ê¸°', 'walk', 'moving'],
            'sitting': ['sitting', 'ì•‰ê¸°', 'sit', 'seated'],
            'running': ['running', 'ë‹¬ë¦¬ê¸°', 'run', 'jogging'],
            'talking': ['talking', 'ëŒ€í™”', 'talk', 'speaking', 'conversation']
        }
        
        target_keywords = activity_keywords.get(activity_condition.lower(), [])
        if not target_keywords:
            return 0.0
        
        match_count = 0
        total_frames = len(frames)
        
        for frame in frames:
            caption = (frame.caption or '') + ' ' + (frame.enhanced_caption or '') + ' ' + (frame.final_caption or '')
            caption_lower = caption.lower()
            
            for keyword in target_keywords:
                if keyword in caption_lower:
                    match_count += 1
                    break
        
        if match_count > 0:
            return (match_count / total_frames) * 0.8
        
        return 0.0
        
    except Exception as e:
        logger.error(f"âŒ í™œë™ ë§¤ì¹­ í™•ì¸ ì˜¤ë¥˜: {e}")
        return 0.0


def _get_detected_objects_from_frame(frame):
    """í”„ë ˆì„ì—ì„œ ê°ì§€ëœ ê°ì²´ ì¶”ì¶œ"""
    try:
        detected_objects = frame.detected_objects
        if isinstance(detected_objects, dict) and 'persons' in detected_objects:
            return detected_objects['persons']
        return []
    except:
        return []


class GenderAnalysisView(APIView):
    """ì‹œê°„ëŒ€ë³„ ì„±ë¹„ ë¶„ì„"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            time_range = request.data.get('time_range', {})
            
            logger.info(f"ğŸ“Š ì„±ë¹„ ë¶„ì„ ìš”ì²­: ë¹„ë””ì˜¤={video_id}, ì‹œê°„ë²”ìœ„={time_range}")
            
            if not video_id:
                return Response({'error': 'ë¹„ë””ì˜¤ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # ì‹œê°„ëŒ€ë³„ ì„±ë¹„ ë¶„ì„
            analysis_result = self._analyze_gender_distribution(video, time_range)
            
            return Response({
                'video_id': video_id,
                'video_name': video.original_name,
                'time_range': time_range,
                'analysis_result': analysis_result,
                'analysis_type': 'gender_distribution'
            })
            
        except Exception as e:
            logger.error(f"âŒ ì„±ë¹„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _analyze_gender_distribution(self, video, time_range):
        """ì‹œê°„ëŒ€ë³„ ì„±ë¹„ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # í”„ë ˆì„ ì¿¼ë¦¬ ì„¤ì •
            frames_query = Frame.objects.filter(video=video).order_by('timestamp')
            
            # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
            if time_range.get('start') and time_range.get('end'):
                start_time = self._parse_time_to_seconds(time_range['start'])
                end_time = self._parse_time_to_seconds(time_range['end'])
                frames_query = frames_query.filter(timestamp__gte=start_time, timestamp__lte=end_time)
            
            frames = list(frames_query)
            
            if not frames:
                return {
                    'total_frames': 0,
                    'gender_distribution': {'male': 0, 'female': 0, 'unknown': 0},
                    'time_based_analysis': [],
                    'statistics': {
                        'male_percentage': 0,
                        'female_percentage': 0,
                        'unknown_percentage': 0
                    }
                }
            
            # ì„±ë³„ ë¶„í¬ ë¶„ì„
            gender_counts = {'male': 0, 'female': 0, 'unknown': 0}
            time_based_data = []
            
            for frame in frames:
                frame_gender_data = self._extract_gender_from_frame(frame)
                
                # ì „ì²´ ì„±ë³„ ì¹´ìš´íŠ¸
                for gender, count in frame_gender_data.items():
                    gender_counts[gender] += count
                
                # ì‹œê°„ëŒ€ë³„ ë°ì´í„°
                time_based_data.append({
                    'timestamp': frame.timestamp,
                    'frame_id': frame.image_id,
                    'gender_distribution': frame_gender_data,
                    'total_persons': sum(frame_gender_data.values())
                })
            
            # í†µê³„ ê³„ì‚°
            total_persons = sum(gender_counts.values())
            statistics = {
                'male_percentage': (gender_counts['male'] / total_persons * 100) if total_persons > 0 else 0,
                'female_percentage': (gender_counts['female'] / total_persons * 100) if total_persons > 0 else 0,
                'unknown_percentage': (gender_counts['unknown'] / total_persons * 100) if total_persons > 0 else 0
            }
            
            return {
                'total_frames': len(frames),
                'total_persons': total_persons,
                'gender_distribution': gender_counts,
                'time_based_analysis': time_based_data,
                'statistics': statistics,
                'time_range_used': {
                    'start': time_range.get('start', '0:00'),
                    'end': time_range.get('end', f'{video.duration:.1f}s')
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì„±ë¹„ ë¶„ì„ ìˆ˜í–‰ ì˜¤ë¥˜: {e}")
            return {
                'error': str(e),
                'total_frames': 0,
                'gender_distribution': {'male': 0, 'female': 0, 'unknown': 0},
                'time_based_analysis': [],
                'statistics': {'male_percentage': 0, 'female_percentage': 0, 'unknown_percentage': 0}
            }
    
    def _extract_gender_from_frame(self, frame):
        """í”„ë ˆì„ì—ì„œ ì„±ë³„ ì •ë³´ ì¶”ì¶œ"""
        try:
            gender_counts = {'male': 0, 'female': 0, 'unknown': 0}
            
            # detected_objectsì—ì„œ ì„±ë³„ ì •ë³´ ì¶”ì¶œ
            detected_objects = frame.detected_objects
            if isinstance(detected_objects, dict) and 'persons' in detected_objects:
                persons = detected_objects['persons']
                
                for person in persons:
                    gender = self._get_person_gender(person)
                    gender_counts[gender] += 1
            
            return gender_counts
            
        except Exception as e:
            logger.error(f"âŒ í”„ë ˆì„ ì„±ë³„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {'male': 0, 'female': 0, 'unknown': 0}
    
    def _get_person_gender(self, person):
        """ê°œë³„ ì‚¬ëŒì˜ ì„±ë³„ ì¶”ì¶œ"""
        try:
            # attributesì—ì„œ ì„±ë³„ ì •ë³´ ì¶”ì¶œ
            attributes = person.get('attributes', {})
            gender_info = attributes.get('gender', {})
            
            if isinstance(gender_info, dict):
                gender_value = gender_info.get('value', '').lower()
                
                if 'man' in gender_value or 'male' in gender_value:
                    return 'male'
                elif 'woman' in gender_value or 'female' in gender_value:
                    return 'female'
                else:
                    return 'unknown'
            else:
                return 'unknown'
                
        except Exception as e:
            logger.error(f"âŒ ì„±ë³„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return 'unknown'
    
    def _parse_time_to_seconds(self, time_str):
        """ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆë¡œ ë³€í™˜"""
        try:
            if ':' in time_str:
                parts = time_str.split(':')
                if len(parts) == 2:
                    minutes, seconds = map(float, parts)
                    return minutes * 60 + seconds
                elif len(parts) == 3:
                    hours, minutes, seconds = map(float, parts)
                    return hours * 3600 + minutes * 60 + seconds
            else:
                return float(time_str)
        except:
            return 0.0


class VideoSummaryView(APIView):
    """ì˜ìƒ ìš”ì•½ ê¸°ëŠ¥"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            summary_type = request.data.get('summary_type', 'comprehensive')  # comprehensive, brief, detailed
            
            logger.info(f"ğŸ“ ì˜ìƒ ìš”ì•½ ìš”ì²­: ë¹„ë””ì˜¤={video_id}, íƒ€ì…={summary_type}")
            
            if not video_id:
                return Response({'error': 'ë¹„ë””ì˜¤ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # ì˜ìƒ ìš”ì•½ ìƒì„±
            summary_result = self._generate_video_summary(video, summary_type)
            
            return Response({
                'video_id': video_id,
                'video_name': video.original_name,
                'summary_type': summary_type,
                'summary_result': summary_result,
                'analysis_type': 'video_summary'
            })
            
        except Exception as e:
            logger.error(f"âŒ ì˜ìƒ ìš”ì•½ ì˜¤ë¥˜: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _generate_video_summary(self, video, summary_type):
        """ì˜ìƒ ìš”ì•½ ìƒì„±"""
        try:
            # í”„ë ˆì„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            frames = Frame.objects.filter(video=video).order_by('timestamp')
            
            if not frames.exists():
                return {
                    'summary': 'ë¶„ì„ëœ í”„ë ˆì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.',
                    'key_events': [],
                    'statistics': {},
                    'duration': video.duration,
                    'frame_count': 0
                }
            
            # ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘
            statistics = self._collect_video_statistics(video, frames)
            
            # í‚¤ ì´ë²¤íŠ¸ ì¶”ì¶œ
            key_events = self._extract_key_events(frames)
            
            # ìš”ì•½ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
            if summary_type == 'brief':
                summary_text = self._generate_brief_summary(statistics, key_events)
            elif summary_type == 'detailed':
                summary_text = self._generate_detailed_summary(statistics, key_events, frames)
            else:  # comprehensive
                summary_text = self._generate_comprehensive_summary(statistics, key_events, frames)
            
            return {
                'summary': summary_text,
                'key_events': key_events,
                'statistics': statistics,
                'duration': video.duration,
                'frame_count': frames.count(),
                'summary_type': summary_type
            }
            
        except Exception as e:
            logger.error(f"âŒ ì˜ìƒ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                'summary': f'ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'key_events': [],
                'statistics': {},
                'duration': video.duration,
                'frame_count': 0
            }
    
    def _collect_video_statistics(self, video, frames):
        """ì˜ìƒ í†µê³„ ìˆ˜ì§‘"""
        try:
            stats = {
                'total_frames': frames.count(),
                'duration': video.duration,
                'person_count': 0,
                'gender_distribution': {'male': 0, 'female': 0, 'unknown': 0},
                'scene_types': [],
                'activities': [],
                'objects_detected': set(),
                'time_periods': []
            }
            
            for frame in frames:
                # ì‚¬ëŒ ìˆ˜ ê³„ì‚°
                detected_objects = frame.detected_objects
                if isinstance(detected_objects, dict) and 'persons' in detected_objects:
                    persons = detected_objects['persons']
                    stats['person_count'] += len(persons)
                    
                    # ì„±ë³„ ë¶„í¬
                    for person in persons:
                        gender = self._get_person_gender_from_attributes(person)
                        stats['gender_distribution'][gender] += 1
                
                # ì”¬ íƒ€ì… ì¶”ì¶œ
                if frame.scene_type:
                    if frame.scene_type not in stats['scene_types']:
                        stats['scene_types'].append(frame.scene_type)
                
                # í™œë™ ì¶”ì¶œ
                if frame.enhanced_caption:
                    activities = self._extract_activities_from_caption(frame.enhanced_caption)
                    stats['activities'].extend(activities)
                
                # ê°ì²´ ì¶”ì¶œ
                if detected_objects and isinstance(detected_objects, dict) and 'persons' in detected_objects:
                    for person in detected_objects['persons']:
                        obj_class = person.get('class', '')
                        if obj_class:
                            stats['objects_detected'].add(obj_class)
            
            # ì¤‘ë³µ ì œê±°
            stats['activities'] = list(set(stats['activities']))
            stats['objects_detected'] = list(stats['objects_detected'])
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ í†µê³„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def _extract_key_events(self, frames):
        """í‚¤ ì´ë²¤íŠ¸ ì¶”ì¶œ"""
        try:
            key_events = []
            
            # í”„ë ˆì„ì„ ì‹œê°„ìˆœìœ¼ë¡œ ê·¸ë£¹í™” (5ì´ˆ ê°„ê²©)
            time_groups = {}
            for frame in frames:
                time_group = int(frame.timestamp // 5) * 5
                if time_group not in time_groups:
                    time_groups[time_group] = []
                time_groups[time_group].append(frame)
            
            # ê° ì‹œê°„ ê·¸ë£¹ì—ì„œ ì¤‘ìš”í•œ ì´ë²¤íŠ¸ ì¶”ì¶œ
            for time_group, group_frames in time_groups.items():
                event = self._analyze_time_group(time_group, group_frames)
                if event:
                    key_events.append(event)
            
            return key_events
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ ì´ë²¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _analyze_time_group(self, time_group, frames):
        """ì‹œê°„ ê·¸ë£¹ ë¶„ì„"""
        try:
            if not frames:
                return None
            
            # ê°€ì¥ ë§ì€ ì‚¬ëŒì´ ìˆëŠ” í”„ë ˆì„ ì°¾ê¸°
            max_persons = 0
            representative_frame = frames[0]
            
            for frame in frames:
                detected_objects = frame.detected_objects
                if isinstance(detected_objects, dict) and 'persons' in detected_objects:
                    person_count = len(detected_objects['persons'])
                    if person_count > max_persons:
                        max_persons = person_count
                        representative_frame = frame
            
            # ì´ë²¤íŠ¸ ì •ë³´ ìƒì„±
            event = {
                'timestamp': time_group,
                'person_count': max_persons,
                'description': representative_frame.final_caption or representative_frame.caption or 'ì¥ë©´',
                'frame_id': representative_frame.image_id,
                'significance': 'high' if max_persons > 5 else 'medium' if max_persons > 2 else 'low'
            }
            
            return event
            
        except Exception as e:
            logger.error(f"âŒ ì‹œê°„ ê·¸ë£¹ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def _generate_brief_summary(self, statistics, key_events):
        """ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±"""
        try:
            duration_min = statistics.get('duration', 0) / 60
            person_count = statistics.get('person_count', 0)
            gender_dist = statistics.get('gender_distribution', {})
            
            summary_parts = [
                f"ì´ ì˜ìƒì€ {duration_min:.1f}ë¶„ ê¸¸ì´ì˜ ì˜ìƒì…ë‹ˆë‹¤.",
                f"ì´ {person_count}ëª…ì˜ ì‚¬ëŒì´ ë“±ì¥í•©ë‹ˆë‹¤."
            ]
            
            if person_count > 0:
                male_count = gender_dist.get('male', 0)
                female_count = gender_dist.get('female', 0)
                if male_count > 0 or female_count > 0:
                    summary_parts.append(f"ì„±ë³„ ë¶„í¬: ë‚¨ì„± {male_count}ëª…, ì—¬ì„± {female_count}ëª…")
            
            if key_events:
                summary_parts.append(f"ì£¼ìš” ì´ë²¤íŠ¸ {len(key_events)}ê°œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return " ".join(summary_parts)
            
        except Exception as e:
            logger.error(f"âŒ ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_detailed_summary(self, statistics, key_events, frames):
        """ìƒì„¸í•œ ìš”ì•½ ìƒì„±"""
        try:
            brief_summary = self._generate_brief_summary(statistics, key_events)
            
            detailed_parts = [brief_summary]
            
            # ì”¬ íƒ€ì… ì •ë³´
            scene_types = statistics.get('scene_types', [])
            if scene_types:
                detailed_parts.append(f"ì¥ì†Œ ìœ í˜•: {', '.join(scene_types)}")
            
            # í™œë™ ì •ë³´
            activities = statistics.get('activities', [])
            if activities:
                activity_counts = {}
                for activity in activities:
                    activity_counts[activity] = activity_counts.get(activity, 0) + 1
                top_activities = sorted(activity_counts.items(), key=lambda x: x[1], reverse=True)[:3]
                detailed_parts.append(f"ì£¼ìš” í™œë™: {', '.join([f'{act}({count}íšŒ)' for act, count in top_activities])}")
            
            # í‚¤ ì´ë²¤íŠ¸ ìƒì„¸ ì •ë³´
            if key_events:
                detailed_parts.append("ì£¼ìš” ì´ë²¤íŠ¸:")
                for i, event in enumerate(key_events[:5], 1):
                    detailed_parts.append(f"  {i}. {event['timestamp']}ì´ˆ: {event['description']} ({event['person_count']}ëª…)")
            
            return " ".join(detailed_parts)
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„¸í•œ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ìƒì„¸í•œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_comprehensive_summary(self, statistics, key_events, frames):
        """ì¢…í•©ì ì¸ ìš”ì•½ ìƒì„±"""
        try:
            detailed_summary = self._generate_detailed_summary(statistics, key_events, frames)
            
            comprehensive_parts = [detailed_summary]
            
            # ì‹œê°„ëŒ€ë³„ ë¶„ì„
            if frames.exists():
                first_frame = frames.first()
                last_frame = frames.last()
                comprehensive_parts.append(f"ë¶„ì„ ì‹œê°„ ë²”ìœ„: {first_frame.timestamp:.1f}ì´ˆ ~ {last_frame.timestamp:.1f}ì´ˆ")
            
            # ê°ì²´ ê°ì§€ ì •ë³´
            objects_detected = statistics.get('objects_detected', [])
            if objects_detected:
                comprehensive_parts.append(f"ê°ì§€ëœ ê°ì²´: {', '.join(objects_detected)}")
            
            return " ".join(comprehensive_parts)
            
        except Exception as e:
            logger.error(f"âŒ ì¢…í•©ì ì¸ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì¢…í•©ì ì¸ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _get_person_gender_from_attributes(self, person):
        """ì‚¬ëŒì˜ ì„±ë³„ ì •ë³´ ì¶”ì¶œ"""
        try:
            attributes = person.get('attributes', {})
            gender_info = attributes.get('gender', {})
            
            if isinstance(gender_info, dict):
                gender_value = gender_info.get('value', '').lower()
                if 'man' in gender_value or 'male' in gender_value:
                    return 'male'
                elif 'woman' in gender_value or 'female' in gender_value:
                    return 'female'
            
            return 'unknown'
        except:
            return 'unknown'
    
    def _extract_activities_from_caption(self, caption):
        """ìº¡ì…˜ì—ì„œ í™œë™ ì¶”ì¶œ"""
        try:
            activities = []
            activity_keywords = {
                'walking': ['walking', 'ê±·ê¸°', 'walk'],
                'sitting': ['sitting', 'ì•‰ê¸°', 'sit'],
                'running': ['running', 'ë‹¬ë¦¬ê¸°', 'run'],
                'talking': ['talking', 'ëŒ€í™”', 'talk'],
                'standing': ['standing', 'ì„œê¸°', 'stand']
            }
            
            caption_lower = caption.lower()
            for activity, keywords in activity_keywords.items():
                for keyword in keywords:
                    if keyword in caption_lower:
                        activities.append(activity)
                        break
            
            return activities
        except:
            return []


class VideoHighlightView(APIView):
    """ì˜ìƒ í•˜ì´ë¼ì´íŠ¸ ìë™ ì¶”ì¶œ"""
    permission_classes = [AllowAny]
    
    def post(self, request):
        try:
            video_id = request.data.get('video_id')
            highlight_criteria = request.data.get('criteria', {})
            
            logger.info(f"ğŸ¬ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ìš”ì²­: ë¹„ë””ì˜¤={video_id}, ê¸°ì¤€={highlight_criteria}")
            
            if not video_id:
                return Response({'error': 'ë¹„ë””ì˜¤ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)
            
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                return Response({'error': 'ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, status=404)
            
            # í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
            highlights = self._extract_highlights(video, highlight_criteria)
            
            return Response({
                'video_id': video_id,
                'video_name': video.original_name,
                'highlights': highlights,
                'total_highlights': len(highlights),
                'analysis_type': 'video_highlights'
            })
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return Response({'error': str(e)}, status=500)
    
    def _extract_highlights(self, video, criteria):
        """í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ"""
        try:
            frames = Frame.objects.filter(video=video).order_by('timestamp')
            
            if not frames.exists():
                return []
            
            # í•˜ì´ë¼ì´íŠ¸ ì ìˆ˜ ê³„ì‚°
            scored_frames = []
            for frame in frames:
                score = self._calculate_highlight_score(frame, criteria)
                if score > 0:
                    scored_frames.append({
                        'frame': frame,
                        'score': score,
                        'timestamp': frame.timestamp,
                        'frame_id': frame.image_id
                    })
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            scored_frames.sort(key=lambda x: x['score'], reverse=True)
            
            # í•˜ì´ë¼ì´íŠ¸ ìƒì„±
            highlights = self._create_highlights(scored_frames, criteria)
            
            return highlights
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ìˆ˜í–‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _calculate_highlight_score(self, frame, criteria):
        """í”„ë ˆì„ì˜ í•˜ì´ë¼ì´íŠ¸ ì ìˆ˜ ê³„ì‚°"""
        try:
            score = 0.0
            
            # ì‚¬ëŒ ìˆ˜ ê¸°ë°˜ ì ìˆ˜
            person_count = self._get_person_count(frame)
            if person_count > 0:
                # ì‚¬ëŒì´ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ìµœëŒ€ 3ì )
                score += min(person_count * 0.5, 3.0)
            
            # í™œë™ ê¸°ë°˜ ì ìˆ˜
            activity_score = self._get_activity_score(frame, criteria)
            score += activity_score
            
            # ê°ì²´ ê¸°ë°˜ ì ìˆ˜
            object_score = self._get_object_score(frame, criteria)
            score += object_score
            
            # ì”¬ ë³€í™” ê¸°ë°˜ ì ìˆ˜
            scene_score = self._get_scene_change_score(frame)
            score += scene_score
            
            # í’ˆì§ˆ ê¸°ë°˜ ì ìˆ˜
            quality_score = self._get_quality_score(frame)
            score += quality_score
            
            return score
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¼ì´íŠ¸ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _get_person_count(self, frame):
        """í”„ë ˆì„ì˜ ì‚¬ëŒ ìˆ˜ ë°˜í™˜"""
        try:
            detected_objects = frame.detected_objects
            if isinstance(detected_objects, dict) and 'persons' in detected_objects:
                return len(detected_objects['persons'])
            return 0
        except:
            return 0
    
    def _get_activity_score(self, frame, criteria):
        """í™œë™ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        try:
            score = 0.0
            caption = (frame.caption or '') + ' ' + (frame.enhanced_caption or '') + ' ' + (frame.final_caption or '')
            caption_lower = caption.lower()
            
            # í™œë™ í‚¤ì›Œë“œë³„ ì ìˆ˜
            activity_scores = {
                'running': 2.0,
                'jumping': 2.5,
                'dancing': 2.0,
                'fighting': 3.0,
                'talking': 1.0,
                'walking': 0.5,
                'sitting': 0.2,
                'standing': 0.3
            }
            
            for activity, activity_score in activity_scores.items():
                if activity in caption_lower:
                    score += activity_score
            
            # ì‚¬ìš©ì ì§€ì • í™œë™ ê¸°ì¤€
            if criteria.get('preferred_activities'):
                for activity in criteria['preferred_activities']:
                    if activity.lower() in caption_lower:
                        score += 1.5
            
            return min(score, 5.0)  # ìµœëŒ€ 5ì 
            
        except Exception as e:
            logger.error(f"âŒ í™œë™ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _get_object_score(self, frame, criteria):
        """ê°ì²´ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        try:
            score = 0.0
            detected_objects = frame.detected_objects
            
            if isinstance(detected_objects, dict) and 'persons' in detected_objects:
                persons = detected_objects['persons']
                
                for person in persons:
                    # ì•¡ì„¸ì„œë¦¬ ê¸°ë°˜ ì ìˆ˜
                    attributes = person.get('attributes', {})
                    accessories = attributes.get('accessories', {})
                    
                    if isinstance(accessories, dict):
                        accessory_value = accessories.get('value', '').lower()
                        if 'phone' in accessory_value:
                            score += 0.5
                        elif 'bag' in accessory_value or 'backpack' in accessory_value:
                            score += 0.3
                        elif 'watch' in accessory_value:
                            score += 0.2
                    
                    # ì˜ìƒ ìƒ‰ìƒ ê¸°ë°˜ ì ìˆ˜
                    clothing_color = attributes.get('clothing_color', {})
                    if isinstance(clothing_color, dict):
                        color_value = clothing_color.get('value', '').lower()
                        if 'bright' in color_value or 'colorful' in color_value:
                            score += 0.5
            
            # ì‚¬ìš©ì ì§€ì • ê°ì²´ ê¸°ì¤€
            if criteria.get('preferred_objects'):
                for obj in criteria['preferred_objects']:
                    if obj.lower() in str(detected_objects).lower():
                        score += 1.0
            
            return min(score, 3.0)  # ìµœëŒ€ 3ì 
            
        except Exception as e:
            logger.error(f"âŒ ê°ì²´ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _get_scene_change_score(self, frame):
        """ì”¬ ë³€í™” ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì”¬ íƒ€ì…ì´ ìˆìœ¼ë©´ ì ìˆ˜ ì¶”ê°€
            if frame.scene_type:
                return 1.0
            
            # ìº¡ì…˜ì— ì”¬ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì ìˆ˜ ì¶”ê°€
            caption = (frame.caption or '') + ' ' + (frame.enhanced_caption or '') + ' ' + (frame.final_caption or '')
            caption_lower = caption.lower()
            
            scene_keywords = ['scene', 'ì¥ë©´', 'change', 'ë³€í™”', 'new', 'ìƒˆë¡œìš´']
            for keyword in scene_keywords:
                if keyword in caption_lower:
                    return 0.5
            
            return 0.0
            
        except Exception as e:
            logger.error(f"âŒ ì”¬ ë³€í™” ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _get_quality_score(self, frame):
        """í’ˆì§ˆ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        try:
            score = 0.0
            
            # ì´ë¯¸ì§€ í’ˆì§ˆ ì •ë³´ê°€ ìˆìœ¼ë©´ í™œìš©
            if hasattr(frame, 'crop_quality') and frame.crop_quality:
                quality_data = frame.crop_quality
                if isinstance(quality_data, dict):
                    overall_quality = quality_data.get('overall', 0)
                    if overall_quality > 0.7:
                        score += 1.0
                    elif overall_quality > 0.5:
                        score += 0.5
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ ì ìˆ˜
            detected_objects = frame.detected_objects
            if isinstance(detected_objects, dict) and 'persons' in detected_objects:
                persons = detected_objects['persons']
                if persons:
                    avg_confidence = sum(p.get('confidence', 0) for p in persons) / len(persons)
                    if avg_confidence > 0.8:
                        score += 1.0
                    elif avg_confidence > 0.6:
                        score += 0.5
            
            return min(score, 2.0)  # ìµœëŒ€ 2ì 
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _create_highlights(self, scored_frames, criteria):
        """í•˜ì´ë¼ì´íŠ¸ ìƒì„±"""
        try:
            highlights = []
            min_score = criteria.get('min_score', 2.0)  # ìµœì†Œ ì ìˆ˜
            max_highlights = criteria.get('max_highlights', 10)  # ìµœëŒ€ í•˜ì´ë¼ì´íŠ¸ ìˆ˜
            
            # ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§
            filtered_frames = [f for f in scored_frames if f['score'] >= min_score]
            
            # ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤í•œ í•˜ì´ë¼ì´íŠ¸ ì„ íƒ
            selected_highlights = []
            last_timestamp = -10  # ìµœì†Œ 10ì´ˆ ê°„ê²©
            
            for frame_data in filtered_frames[:max_highlights * 2]:  # ì—¬ìœ ë¶„ì„ ë‘ê³  ì„ íƒ
                if frame_data['timestamp'] - last_timestamp >= 10:  # 10ì´ˆ ì´ìƒ ê°„ê²©
                    selected_highlights.append(frame_data)
                    last_timestamp = frame_data['timestamp']
                    
                    if len(selected_highlights) >= max_highlights:
                        break
            
            # í•˜ì´ë¼ì´íŠ¸ ì •ë³´ ìƒì„±
            for i, frame_data in enumerate(selected_highlights):
                frame = frame_data['frame']
                highlight = {
                    'id': i + 1,
                    'timestamp': frame_data['timestamp'],
                    'frame_id': frame_data['frame_id'],
                    'score': frame_data['score'],
                    'description': frame.final_caption or frame.caption or 'í•˜ì´ë¼ì´íŠ¸ ì¥ë©´',
                    'person_count': self._get_person_count(frame),
                    'thumbnail_url': f'/api/frame/{frame.video.id}/{frame.image_id}/',
                    'significance': self._get_significance_level(frame_data['score'])
                }
                highlights.append(highlight)
            
            return highlights
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¼ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return []
    
    def _get_significance_level(self, score):
        """ì¤‘ìš”ë„ ë ˆë²¨ ë°˜í™˜"""
        if score >= 5.0:
            return 'high'
        elif score >= 3.0:
            return 'medium'
        else:
            return 'low'


def _check_bag_in_accessories(obj):
    """ê°ì²´ì˜ accessoriesì—ì„œ ê°€ë°© í™•ì¸"""
    try:
        # _raw í•„ë“œì—ì„œ accessories ì •ë³´ ì¶”ì¶œ
        raw_data = obj.get('_raw', {})
        attributes = raw_data.get('attributes', {})
        accessories = attributes.get('accessories', {})
        
        if isinstance(accessories, dict):
            # value í•„ë“œ í™•ì¸
            accessory_value = accessories.get('value', '').lower()
            if any(bag_keyword in accessory_value for bag_keyword in ['bag', 'backpack', 'handbag']):
                return True
            
            # all_scoresì—ì„œ ê°€ë°© ê´€ë ¨ ì ìˆ˜ í™•ì¸
            all_scores = accessories.get('all_scores', {})
            for score_key, score_value in all_scores.items():
                if any(bag_keyword in score_key.lower() for bag_keyword in ['bag', 'backpack', 'handbag']):
                    if score_value > 0.1:  # ì‹ ë¢°ë„ 0.1 ì´ìƒ
                        return True
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ ê°€ë°© ì•¡ì„¸ì„œë¦¬ í™•ì¸ ì˜¤ë¥˜: {e}")
        return False
