# chat/video_analyzer.py - ê³ ë„í™”ëœ PAR ê¸°ë°˜ ì˜ìƒ ë¶„ì„ ì‹œìŠ¤í…œ

import os
import json
import numpy as np
import cv2
import time
import torch
from collections import Counter, defaultdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dotenv import load_dotenv

# í™˜ê²½ ì„¤ì •
load_dotenv()
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

# API ì„¤ì •
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# í•„ìˆ˜ ëª¨ë¸ë“¤ë§Œ import
YOLO_AVAILABLE = False
CLIP_AVAILABLE = False

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
    print("âœ… YOLO ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ YOLO ë¯¸ì„¤ì¹˜ - ê°ì²´ ê°ì§€ ê¸°ëŠ¥ ì œí•œ")

try:
    from transformers import CLIPProcessor, CLIPModel
    CLIP_AVAILABLE = True
    print("âœ… CLIP ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ CLIP ë¯¸ì„¤ì¹˜ - ì†ì„± ë¶„ì„ ê¸°ëŠ¥ ì œí•œ")

# API í´ë¼ì´ì–¸íŠ¸
try:
    from groq import Groq
    groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
except ImportError:
    groq_client = None

class AdvancedPedestrianAttributeRecognizer:
    """ê³ ë„í™”ëœ ë³´í–‰ì ì†ì„± ì¸ì‹ ëª¨ë“ˆ"""
    
    def __init__(self, device="cpu"):
        self.device = device
        self.clip_model = None
        self.clip_processor = None
        
        # í™•ì¥ëœ ì†ì„± ë¶„ë¥˜ í…œí”Œë¦¿
        self.attribute_templates = {
            'gender': ['a man', 'a woman', 'a person'],
            'age': ['a child', 'a teenager', 'a young adult', 'a middle-aged person', 'an elderly person'],
            'detailed_clothing': [
                'wearing a t-shirt', 'wearing a long sleeve shirt', 'wearing a polo shirt', 
                'wearing a tank top', 'wearing a sweater', 'wearing a hoodie',
                'wearing jeans', 'wearing dress pants', 'wearing shorts', 'wearing leggings',
                'wearing a mini skirt', 'wearing a long skirt', 'wearing a dress'
            ],
            'clothing_color': [
                'wearing red clothes', 'wearing blue clothes', 'wearing black clothes', 
                'wearing white clothes', 'wearing green clothes', 'wearing yellow clothes',
                'wearing pink clothes', 'wearing purple clothes', 'wearing orange clothes',
                'wearing gray clothes', 'wearing brown clothes'
            ],
            'accessories': [
                'wearing glasses', 'wearing sunglasses', 'wearing a hat', 'wearing a cap',
                'carrying a bag', 'carrying a backpack', 'carrying a handbag',
                'wearing a watch', 'carrying a phone', 'wearing earphones'
            ],
            'posture': [
                'standing upright', 'walking normally', 'walking fast', 'running',
                'sitting down', 'looking at phone', 'talking on phone', 'looking around'
            ],
            'facial_attributes': [
                'smiling', 'serious expression', 'wearing a mask', 'no mask',
                'looking forward', 'looking down', 'looking sideways'
            ],
            'hair_style': [
                'with short hair', 'with long hair', 'with curly hair', 'with straight hair',
                'bald', 'wearing a hat covering hair'
            ]
        }
        
        if CLIP_AVAILABLE:
            self._load_clip_model()
    
    def _load_clip_model(self):
        """CLIP ëª¨ë¸ ë¡œë”©"""
        try:
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16")
            print("âœ… ê³ ë„í™”ëœ PARìš© CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def extract_detailed_attributes(self, person_crop, bbox_info=None, context_info=None):
        """ê³ ë„í™”ëœ ë³´í–‰ì ì†ì„± ì¶”ì¶œ"""
        if not self.clip_model:
            return self._extract_basic_attributes(person_crop, bbox_info)
        
        try:
            from PIL import Image
            
            # OpenCV ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
            if len(person_crop.shape) == 3:
                person_crop_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)
            else:
                person_crop_rgb = person_crop
            
            image = Image.fromarray(person_crop_rgb)
            
            attributes = {}
            
            # ê° ì†ì„±ë³„ë¡œ CLIP ë¶„ì„ ìˆ˜í–‰ (ì„ê³„ê°’ ë‚®ì¶¤)
            for attr_name, templates in self.attribute_templates.items():
                inputs = self.clip_processor(
                    text=templates, 
                    images=image, 
                    return_tensors="pt", 
                    padding=True
                )
                
                outputs = self.clip_model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)
                
                best_idx = probs.argmax().item()
                confidence = probs[0][best_idx].item()
                
                # ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” ë§ì€ ì†ì„± ì¶”ì¶œ
                if confidence > 0.2:  # 0.3ì—ì„œ 0.2ë¡œ ë‚®ì¶¤
                    attributes[attr_name] = {
                        'value': templates[best_idx].replace('a ', '').replace('an ', ''),
                        'confidence': float(confidence),
                        'all_scores': {template: float(score) for template, score in zip(templates, probs[0])},
                        'top_3': self._get_top_n_results(templates, probs[0], 3)
                    }
                else:
                    # ì‹ ë¢°ë„ê°€ ë‚®ì•„ë„ ìµœìƒìœ„ ê²°ê³¼ëŠ” ì €ì¥
                    attributes[attr_name] = {
                        'value': templates[best_idx].replace('a ', '').replace('an ', ''),
                        'confidence': float(confidence),
                        'status': 'low_confidence'
                    }
            
            # ì–¼êµ´ ì˜ì—­ ë³„ë„ ë¶„ì„
            face_region = self._extract_face_region(person_crop)
            if face_region is not None:
                attributes['facial_details'] = self._analyze_facial_attributes(face_region)
            
            # í¬ì¦ˆ ë¶„ì„
            attributes['pose_analysis'] = self._analyze_pose(person_crop)
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
            if context_info:
                attributes['context'] = context_info
            
            return attributes
            
        except Exception as e:
            print(f"âš ï¸ ê³ ë„í™”ëœ ì†ì„± ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self._extract_basic_attributes(person_crop, bbox_info)
    
    def _get_top_n_results(self, templates, probs, n=3):
        """ìƒìœ„ Nê°œ ê²°ê³¼ ë°˜í™˜"""
        top_indices = probs.argsort(descending=True)[:n]
        return [(templates[i], float(probs[i])) for i in top_indices]
    
    def _extract_face_region(self, person_crop):
        """ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ"""
        try:
            # ìƒë‹¨ 1/4 ì˜ì—­ì„ ì–¼êµ´ë¡œ ê°€ì •
            h, w = person_crop.shape[:2]
            face_region = person_crop[:h//4, :]
            
            if face_region.size > 0:
                return face_region
        except Exception as e:
            print(f"âš ï¸ ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _analyze_facial_attributes(self, face_region):
        """ì–¼êµ´ ì†ì„± ë¶„ì„"""
        try:
            # ê°„ë‹¨í•œ ì–¼êµ´ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
            avg_brightness = np.mean(face_region)
            
            return {
                'brightness': float(avg_brightness),
                'estimated_lighting': 'bright' if avg_brightness > 120 else 'dark',
                'face_size_ratio': face_region.size / (face_region.shape[0] * face_region.shape[1])
            }
        except Exception as e:
            print(f"âš ï¸ ì–¼êµ´ ì†ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_pose(self, person_crop):
        """í¬ì¦ˆ ë¶„ì„"""
        try:
            h, w = person_crop.shape[:2]
            aspect_ratio = h / w
            
            # ê°„ë‹¨í•œ í¬ì¦ˆ ì¶”ì •
            pose_info = {
                'height_width_ratio': float(aspect_ratio),
                'estimated_pose': 'standing' if aspect_ratio > 2.0 else 'sitting_or_crouching',
                'body_orientation': self._estimate_body_orientation(person_crop)
            }
            
            return pose_info
        except Exception as e:
            print(f"âš ï¸ í¬ì¦ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _estimate_body_orientation(self, person_crop):
        """ì‹ ì²´ ë°©í–¥ ì¶”ì •"""
        # ê°„ë‹¨í•œ ë°©í–¥ ì¶”ì • ë¡œì§
        h, w = person_crop.shape[:2]
        left_half = person_crop[:, :w//2]
        right_half = person_crop[:, w//2:]
        
        left_intensity = np.mean(left_half)
        right_intensity = np.mean(right_half)
        
        if abs(left_intensity - right_intensity) < 10:
            return 'front_facing'
        elif left_intensity > right_intensity:
            return 'slightly_left'
        else:
            return 'slightly_right'
    
    def _extract_basic_attributes(self, person_crop, bbox_info=None):
        """ê¸°ë³¸ ì†ì„± ì¶”ì¶œ (CLIP ì—†ì„ ë•Œ)"""
        h, w = person_crop.shape[:2]
        
        # ìƒ‰ìƒ ê¸°ë°˜ ê°„ë‹¨í•œ ì†ì„± ì¶”ì¶œ
        hsv = cv2.cvtColor(person_crop, cv2.COLOR_BGR2HSV)
        
        # ìƒì˜ ì˜ì—­ (ìƒë‹¨ 1/3)
        upper_region = person_crop[:h//3, :]
        upper_color = self._get_dominant_color(upper_region)
        
        # í•˜ì˜ ì˜ì—­ (ì¤‘ê°„ 1/3)
        lower_region = person_crop[h//3:2*h//3, :]
        lower_color = self._get_dominant_color(lower_region)
        
        attributes = {
            'gender': {'value': 'unknown', 'confidence': 0.0},
            'age': {'value': 'unknown', 'confidence': 0.0},
            'detailed_clothing': {'value': f'{upper_color} shirt', 'confidence': 0.5},
            'clothing_color': {'value': upper_color, 'confidence': 0.6},
            'accessories': {'value': 'unknown', 'confidence': 0.0},
            'posture': {'value': 'standing', 'confidence': 0.4},
            'pose_analysis': {
                'height_width_ratio': float(h/w),
                'estimated_pose': 'standing' if h/w > 2.0 else 'sitting'
            }
        }
        
        return attributes
    
    def _get_dominant_color(self, image_region):
        """ì˜ì—­ì˜ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        try:
            # HSVë¡œ ë³€í™˜í•˜ì—¬ ìƒ‰ìƒ ë¶„ì„
            hsv = cv2.cvtColor(image_region, cv2.COLOR_BGR2HSV)
            h_mean = np.mean(hsv[:, :, 0])
            
            # ìƒ‰ìƒ ë²”ìœ„ë³„ ë¶„ë¥˜ (ë” ì„¸ë¶„í™”)
            if h_mean < 10 or h_mean > 170:
                return 'red'
            elif h_mean < 25:
                return 'orange'
            elif h_mean < 40:
                return 'yellow'
            elif h_mean < 80:
                return 'green'
            elif h_mean < 130:
                return 'blue'
            elif h_mean < 160:
                return 'purple'
            else:
                return 'pink'
        except:
            return 'unknown'

class EnhancedPersonTracker:
    """í–¥ìƒëœ ë³´í–‰ì ì¶”ì  ëª¨ë“ˆ"""
    
    def __init__(self):
        self.tracked_persons = {}
        self.next_id = 1
        self.max_distance = 80  # ê±°ë¦¬ ì„ê³„ê°’ ì¡°ì •
        self.track_history = {}  # ì¶”ì  ì´ë ¥ ì €ì¥
        
    def update_tracks_advanced(self, detections, frame_id, timestamp):
        """ê³ ë„í™”ëœ ì¶”ì  ID ì—…ë°ì´íŠ¸ (ëª¨ë“  ê°ì²´ í´ë˜ìŠ¤ ì§€ì›)"""
        current_detections = []
        
        for detection in detections:
            # ëª¨ë“  ê°ì²´ í´ë˜ìŠ¤ì— ëŒ€í•´ ì¶”ì  ìˆ˜í–‰
            object_class = detection.get('class', 'unknown')
            bbox = detection['bbox']
            center = self._get_bbox_center(bbox)
            
            # ê¸°ì¡´ íŠ¸ë™ê³¼ ë§¤ì¹­
            best_match_id = self._find_best_match(center, detection, frame_id)
            
            if best_match_id:
                # ê¸°ì¡´ íŠ¸ë™ ì—…ë°ì´íŠ¸
                self._update_existing_track(best_match_id, center, detection, frame_id, timestamp)
                track_id = best_match_id
            else:
                # ìƒˆë¡œìš´ íŠ¸ë™ ìƒì„±
                track_id = self._create_new_track(center, detection, frame_id, timestamp)
            
            detection['track_id'] = track_id
            detection['track_confidence'] = self._calculate_track_confidence(track_id)
            current_detections.append(detection)
        
        # ì˜¤ë˜ëœ íŠ¸ë™ ì •ë¦¬
        self._cleanup_old_tracks(frame_id)
        
        return current_detections
    
    def _find_best_match(self, center, detection, frame_id):
        """ê°€ì¥ ì í•©í•œ ê¸°ì¡´ íŠ¸ë™ ì°¾ê¸°"""
        best_match_id = None
        min_distance = float('inf')
        
        for track_id, track_info in self.tracked_persons.items():
            # ê±°ë¦¬ ê¸°ë°˜ ë§¤ì¹­
            last_center = track_info['last_center']
            distance = np.sqrt((center[0] - last_center[0])**2 + (center[1] - last_center[1])**2)
            
            # ì†ì„± ìœ ì‚¬ë„ ê³ ë ¤
            attribute_similarity = self._calculate_attribute_similarity(
                detection.get('attributes', {}), 
                track_info.get('attributes', {})
            )
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            composite_score = distance * (1 - attribute_similarity * 0.3)
            
            if composite_score < min_distance and distance < self.max_distance:
                min_distance = composite_score
                best_match_id = track_id
        
        return best_match_id
    
    def _calculate_attribute_similarity(self, attrs1, attrs2):
        """ì†ì„± ìœ ì‚¬ë„ ê³„ì‚°"""
        if not attrs1 or not attrs2:
            return 0.0
        
        similarity_score = 0.0
        comparison_count = 0
        
        key_attributes = ['clothing_color', 'detailed_clothing', 'accessories']
        
        for attr in key_attributes:
            if attr in attrs1 and attr in attrs2:
                val1 = attrs1[attr].get('value', '')
                val2 = attrs2[attr].get('value', '')
                
                if val1 and val2 and val1 == val2:
                    similarity_score += 1.0
                comparison_count += 1
        
        return similarity_score / max(comparison_count, 1)
    
    def _update_existing_track(self, track_id, center, detection, frame_id, timestamp):
        """ê¸°ì¡´ íŠ¸ë™ ì—…ë°ì´íŠ¸"""
        track_info = self.tracked_persons[track_id]
        track_info['last_center'] = center
        track_info['last_frame'] = frame_id
        track_info['last_timestamp'] = timestamp
        
        # ì†ì„± ì •ë³´ ì—…ë°ì´íŠ¸ (ë” ë†’ì€ ì‹ ë¢°ë„ë¡œ)
        new_attributes = detection.get('attributes', {})
        if new_attributes:
            if 'attributes' not in track_info:
                track_info['attributes'] = new_attributes
            else:
                track_info['attributes'] = self._merge_attributes(
                    track_info['attributes'], new_attributes
                )
        
        # ì´ë™ ì´ë ¥ ì €ì¥
        if track_id not in self.track_history:
            self.track_history[track_id] = []
        
        self.track_history[track_id].append({
            'frame_id': frame_id,
            'timestamp': timestamp,
            'center': center,
            'bbox': detection['bbox']
        })
    
    def _create_new_track(self, center, detection, frame_id, timestamp):
        """ìƒˆë¡œìš´ íŠ¸ë™ ìƒì„±"""
        track_id = self.next_id
        self.tracked_persons[track_id] = {
            'first_frame': frame_id,
            'last_frame': frame_id,
            'first_timestamp': timestamp,
            'last_timestamp': timestamp,
            'last_center': center,
            'attributes': detection.get('attributes', {}),
            'track_quality': 1.0
        }
        
        # ì´ë ¥ ì´ˆê¸°í™”
        self.track_history[track_id] = [{
            'frame_id': frame_id,
            'timestamp': timestamp,
            'center': center,
            'bbox': detection['bbox']
        }]
        
        self.next_id += 1
        return track_id
    
    def _merge_attributes(self, old_attrs, new_attrs):
        """ì†ì„± ì •ë³´ ë³‘í•© (ë” ë†’ì€ ì‹ ë¢°ë„ ìš°ì„ )"""
        merged = old_attrs.copy()
        
        for attr_name, new_attr_data in new_attrs.items():
            if attr_name not in merged:
                merged[attr_name] = new_attr_data
            else:
                old_confidence = merged[attr_name].get('confidence', 0)
                new_confidence = new_attr_data.get('confidence', 0)
                
                if new_confidence > old_confidence:
                    merged[attr_name] = new_attr_data
        
        return merged
    
    def _calculate_track_confidence(self, track_id):
        """ì¶”ì  ì‹ ë¢°ë„ ê³„ì‚°"""
        if track_id not in self.track_history:
            return 0.5
        
        history = self.track_history[track_id]
        
        # ì§€ì† ì‹œê°„ ê¸°ë°˜ ì‹ ë¢°ë„
        duration_frames = len(history)
        duration_confidence = min(1.0, duration_frames / 30)  # 30í”„ë ˆì„ê¹Œì§€ ì¦ê°€
        
        # ì´ë™ ì¼ê´€ì„± ê¸°ë°˜ ì‹ ë¢°ë„
        movement_consistency = self._calculate_movement_consistency(history)
        
        return (duration_confidence + movement_consistency) / 2
    
    def _calculate_movement_consistency(self, history):
        """ì´ë™ ì¼ê´€ì„± ê³„ì‚°"""
        if len(history) < 3:
            return 0.5
        
        velocities = []
        for i in range(1, len(history)):
            prev_center = history[i-1]['center']
            curr_center = history[i]['center']
            
            velocity = np.sqrt(
                (curr_center[0] - prev_center[0])**2 + 
                (curr_center[1] - prev_center[1])**2
            )
            velocities.append(velocity)
        
        if not velocities:
            return 0.5
        
        # ì†ë„ ë³€í™”ì˜ í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ
        velocity_std = np.std(velocities)
        consistency = max(0.0, 1.0 - velocity_std / 50.0)  # ì •ê·œí™”
        
        return consistency
    
    def _get_bbox_center(self, bbox):
        """ë°”ìš´ë”© ë°•ìŠ¤ì˜ ì¤‘ì‹¬ì  ê³„ì‚°"""
        x1, y1, x2, y2 = bbox
        return ((x1 + x2) / 2, (y1 + y2) / 2)
    
    def _cleanup_old_tracks(self, current_frame, max_age=45):  # 30ì—ì„œ 45ë¡œ ì¦ê°€
        """ì˜¤ë˜ëœ íŠ¸ë™ ì œê±°"""
        to_remove = []
        for track_id, track_info in self.tracked_persons.items():
            if current_frame - track_info['last_frame'] > max_age:
                to_remove.append(track_id)
        
        for track_id in to_remove:
            del self.tracked_persons[track_id]
            if track_id in self.track_history:
                del self.track_history[track_id]

class AdaptiveFrameSampler:
    """ì ì‘ì  í”„ë ˆì„ ìƒ˜í”Œë§"""
    
    def __init__(self):
        self.sampling_strategies = {
            'basic': {'interval_multiplier': 2.0, 'min_interval': 30},
            'enhanced': {'interval_multiplier': 1.5, 'min_interval': 20},
            'comprehensive': {'interval_multiplier': 1.0, 'min_interval': 15},
            'custom': {'interval_multiplier': 0.5, 'min_interval': 10}
        }
    
    def calculate_sampling_interval(self, fps, analysis_type, content_complexity=None):
        """ìƒ˜í”Œë§ ê°„ê²© ê³„ì‚°"""
        strategy = self.sampling_strategies.get(analysis_type, self.sampling_strategies['enhanced'])
        
        base_interval = max(strategy['min_interval'], int(fps * strategy['interval_multiplier']))
        
        # ì½˜í…ì¸  ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
        if content_complexity == 'high':
            base_interval = max(strategy['min_interval'] // 2, base_interval // 2)
        elif content_complexity == 'low':
            # ì§§ì€ ë¹„ë””ì˜¤ì˜ ê²½ìš° ë” ë§ì€ í”„ë ˆì„ ì²˜ë¦¬
            base_interval = min(int(fps * 0.5), base_interval)  # 0.5ì´ˆë§ˆë‹¤ ì²˜ë¦¬
        
        return base_interval
    
    def estimate_content_complexity(self, frame_sample):
        """ì½˜í…ì¸  ë³µì¡ë„ ì¶”ì •"""
        try:
            # ì—ì§€ ë°€ë„ë¡œ ë³µì¡ë„ ì¶”ì •
            gray = cv2.cvtColor(frame_sample, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            
            if edge_density > 0.1:
                return 'high'
            elif edge_density < 0.05:
                return 'low'
            else:
                return 'medium'
        except Exception:
            return 'medium'

class EnhancedVideoAnalyzer:
    """ê³ ë„í™”ëœ ë¹„ë””ì˜¤ ë¶„ì„ê¸°"""
    
    def __init__(self, model_path="yolov8n.pt"):
        self.model = None
        self.par_recognizer = AdvancedPedestrianAttributeRecognizer()
        self.person_tracker = EnhancedPersonTracker()
        self.frame_sampler = AdaptiveFrameSampler()
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
        self.specialized_models = {}
        
        print(f"ğŸš€ ê³ ë„í™”ëœ PAR ë¹„ë””ì˜¤ ë¶„ì„ê¸° ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        # YOLO ëª¨ë¸ ë¡œë”©
        if YOLO_AVAILABLE:
            try:
                self.model = YOLO(model_path)
                print(f"âœ… ê¸°ë³¸ YOLO ëª¨ë¸ ë¡œë“œ: {model_path}")
                
                # íŠ¹í™” ëª¨ë¸ë“¤ ë¡œë“œ ì‹œë„
                self._load_specialized_models()
            except Exception as e:
                print(f"âš ï¸ YOLO ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _load_specialized_models(self):
        """íŠ¹í™”ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        specialized_model_paths = {
            'person': 'yolov8n-person.pt',  # ì‚¬ëŒ íŠ¹í™” (ìˆë‹¤ë©´)
            'vehicle': 'yolov8n-vehicle.pt'  # ì°¨ëŸ‰ íŠ¹í™” (ìˆë‹¤ë©´)
        }
        
        for model_type, model_path in specialized_model_paths.items():
            try:
                if os.path.exists(model_path):
                    self.specialized_models[model_type] = YOLO(model_path)
                    print(f"âœ… {model_type} íŠ¹í™” ëª¨ë¸ ë¡œë“œ: {model_path}")
            except Exception as e:
                print(f"âš ï¸ {model_type} íŠ¹í™” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def detect_and_analyze_persons_advanced(self, frame, frame_id, timestamp, context_info=None):
        """ê³ ë„í™”ëœ ê°ì²´ ê²€ì¶œ ë° ì†ì„± ë¶„ì„ (ëª¨ë“  í´ë˜ìŠ¤ í¬í•¨)"""
        if not self.model:
            return []
        
        try:
            # ë‹¤ì¤‘ ì‹ ë¢°ë„ ë ˆë²¨ë¡œ ê²€ì¶œ
            confidence_levels = [0.25, 0.4, 0.6]
            all_detections = []
            
            print(f"      ğŸ¯ YOLO ê²€ì¶œ ì‹œì‘ (ì‹ ë¢°ë„ ë ˆë²¨: {confidence_levels})")
            
            for conf_level in confidence_levels:
                # ëª¨ë“  í´ë˜ìŠ¤ ê°ì§€ (person, bag, car, bicycle ë“±)
                results = self.model(frame, verbose=False, conf=conf_level)  # ëª¨ë“  í´ë˜ìŠ¤ ê°ì§€
                
                detections = []
                h, w = frame.shape[:2]
                
                for result in results:
                    if result.boxes is not None:
                        boxes = result.boxes.xyxy.cpu().numpy()
                        confidences = result.boxes.conf.cpu().numpy()
                        class_ids = result.boxes.cls.cpu().numpy()
                        
                        for box, conf, class_id in zip(boxes, confidences, class_ids):
                            # í´ë˜ìŠ¤ IDë¥¼ ì‹¤ì œ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
                            class_name = self.model.names[int(class_id)]
                            
                            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ê·œí™”
                            normalized_bbox = [
                                float(box[0]/w), float(box[1]/h),
                                float(box[2]/w), float(box[3]/h)
                            ]
                            
                            # ê°ì²´ ì˜ì—­ ì¶”ì¶œ
                            object_crop = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]
                            
                            if object_crop.size > 0:
                                # person í´ë˜ìŠ¤ì¸ ê²½ìš°ì—ë§Œ ìƒì„¸ ì†ì„± ë¶„ì„ ìˆ˜í–‰
                                if class_name == 'person':
                                    # ê³ ë„í™”ëœ ì†ì„± ë¶„ì„
                                    print(f"        ğŸ” ì‚¬ëŒ ì†ì„± ë¶„ì„ ì¤‘... (ì‹ ë¢°ë„: {conf:.2f})")
                                    attributes = self.par_recognizer.extract_detailed_attributes(
                                        object_crop, normalized_bbox, context_info
                                    )
                                    print(f"        âœ… ì‚¬ëŒ ì†ì„± ë¶„ì„ ì™„ë£Œ")
                                else:
                                    # personì´ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ ì†ì„±ë§Œ ì €ì¥
                                    attributes = {
                                        'object_type': {
                                            'value': class_name,
                                            'confidence': float(conf)
                                        }
                                    }
                                    print(f"        ğŸ” {class_name} ê°ì§€ (ì‹ ë¢°ë„: {conf:.2f})")
                                
                                detection = {
                                    'class': class_name,
                                    'bbox': normalized_bbox,
                                    'confidence': float(conf),
                                    'confidence_level': conf_level,
                                    'attributes': attributes,
                                    'frame_id': frame_id,
                                    'timestamp': timestamp,
                                    'crop_quality': self._assess_crop_quality(object_crop)
                                }
                                
                                detections.append(detection)
                
                # ì¤‘ë³µ ì œê±° í›„ ìµœê³  í’ˆì§ˆë§Œ ìœ ì§€
                detections = self._deduplicate_detections(detections)
                all_detections.extend(detections)
            
            # ìµœì¢… ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§
            final_detections = self._filter_best_detections(all_detections)
            
            # ê³ ë„í™”ëœ ì¶”ì  ID í• ë‹¹
            tracked_detections = self.person_tracker.update_tracks_advanced(
                final_detections, frame_id, timestamp
            )
            
            return tracked_detections
            
        except Exception as e:
            print(f"âš ï¸ ê³ ë„í™”ëœ ë³´í–‰ì ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []
    
    def _assess_crop_quality(self, person_crop):
        """ë³´í–‰ì crop í’ˆì§ˆ í‰ê°€"""
        try:
            h, w = person_crop.shape[:2]
            
            # í¬ê¸° ì ìˆ˜
            size_score = min(1.0, (h * w) / (100 * 200))  # 100x200 ê¸°ì¤€
            
            # ì„ ëª…ë„ ì ìˆ˜ (Laplacian variance)
            gray = cv2.cvtColor(person_crop, cv2.COLOR_BGR2GRAY)
            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_score = min(1.0, sharpness / 500)  # ì •ê·œí™”
            
            # ë°ê¸° ì ìˆ˜
            brightness = np.mean(person_crop)
            brightness_score = 1.0 - abs(brightness - 128) / 128  # 128 ê¸°ì¤€
            
            overall_quality = (size_score + sharpness_score + brightness_score) / 3
            
            return {
                'overall': float(overall_quality),
                'size': float(size_score),
                'sharpness': float(sharpness_score),
                'brightness': float(brightness_score)
            }
        except Exception:
            return {'overall': 0.5}
    
    def _deduplicate_detections(self, detections):
        """ë™ì¼ ì‹ ë¢°ë„ ë ˆë²¨ ë‚´ ì¤‘ë³µ ì œê±°"""
        if not detections:
            return []
        
        # IoU ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        filtered_detections = []
        
        for detection in detections:
            is_duplicate = False
            
            for existing in filtered_detections:
                iou = self._calculate_iou(detection['bbox'], existing['bbox'])
                
                # IoUê°€ ë†’ê³  ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì œê±°
                if iou > 0.5:
                    if detection['confidence'] <= existing['confidence']:
                        is_duplicate = True
                        break
                    else:
                        # ë” ë†’ì€ ì‹ ë¢°ë„ë©´ ê¸°ì¡´ ê²ƒ ì œê±°
                        filtered_detections.remove(existing)
            
            if not is_duplicate:
                filtered_detections.append(detection)
        
        return filtered_detections
    
    def _calculate_iou(self, box1, box2):
        """IoU ê³„ì‚°"""
        try:
            x1_max = max(box1[0], box2[0])
            y1_max = max(box1[1], box2[1])
            x2_min = min(box1[2], box2[2])
            y2_min = min(box1[3], box2[3])
            
            if x2_min <= x1_max or y2_min <= y1_max:
                return 0.0
            
            intersection = (x2_min - x1_max) * (y2_min - y1_max)
            
            area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
            area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
            union = area1 + area2 - intersection
            
            return intersection / union if union > 0 else 0.0
        except:
            return 0.0
    
    def _filter_best_detections(self, all_detections):
        """í’ˆì§ˆ ê¸°ë°˜ ìµœê³  ê²€ì¶œ ê²°ê³¼ í•„í„°ë§"""
        if not all_detections:
            return []
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        scored_detections = []
        
        for detection in all_detections:
            quality = detection.get('crop_quality', {}).get('overall', 0.5)
            confidence = detection['confidence']
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            composite_score = (confidence * 0.7) + (quality * 0.3)
            
            scored_detections.append({
                'detection': detection,
                'score': composite_score
            })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_detections.sort(key=lambda x: x['score'], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜ (ìµœëŒ€ 20ê°œ)
        return [item['detection'] for item in scored_detections[:20]]
    
    def analyze_video_comprehensive_advanced(self, video, analysis_type='enhanced', progress_callback=None):
        """ê³ ë„í™”ëœ ì¢…í•© ë¹„ë””ì˜¤ ë¶„ì„"""
        start_time = time.time()
        
        try:
            # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            video_path = self._find_video_path(video)
            if not video_path:
                raise Exception("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise Exception("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            # ì½˜í…ì¸  ë³µì¡ë„ ì¶”ì •ì„ ìœ„í•œ ìƒ˜í”Œ í”„ë ˆì„
            sample_frames = self._extract_sample_frames(cap, 5)
            content_complexity = self._estimate_overall_complexity(sample_frames)
            
            # ì ì‘ì  ìƒ˜í”Œë§ ê°„ê²© ê³„ì‚°
            sample_interval = self.frame_sampler.calculate_sampling_interval(
                fps, analysis_type, content_complexity
            )
            
            # ìµœëŒ€ ì²˜ë¦¬ í”„ë ˆì„ ìˆ˜ ì œí•œ (ë” ë§ì€ í”„ë ˆì„ ì²˜ë¦¬)
            max_frames_to_process = min(100, total_frames // max(1, sample_interval // 2) + 1)
            
            print(f"ğŸ“Š ê³ ë„í™”ëœ ë¶„ì„ ì‹œì‘")
            print(f"   - ë¶„ì„ íƒ€ì…: {analysis_type}")
            print(f"   - ì½˜í…ì¸  ë³µì¡ë„: {content_complexity}")
            print(f"   - ìƒ˜í”Œë§ ê°„ê²©: {sample_interval} í”„ë ˆì„")
            print(f"   - ì´ í”„ë ˆì„ ìˆ˜: {total_frames}")
            print(f"   - ì˜ˆìƒ ì²˜ë¦¬ í”„ë ˆì„: {max_frames_to_process}")
            print(f"   - ë¹„ë””ì˜¤ ê¸¸ì´: {duration:.1f}ì´ˆ")
            print(f"   - ì˜ˆìƒ ë¶„ì„ ì‹œê°„: {max_frames_to_process * 2:.1f}ì´ˆ (ì¶”ì •)")
            print("=" * 60)
            print()
            
            frame_results = []
            person_database = []
            scene_analysis = []
            processed_frames = 0
            frame_id = 0
            last_progress_time = time.time()
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì 
            quality_metrics = {
                'total_detections': 0,
                'high_quality_detections': 0,
                'tracking_continuity': 0,
                'attribute_confidence_avg': 0
            }
            
            while cap.isOpened() and processed_frames < max_frames_to_process:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_id += 1
                
                # ë” ë§ì€ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ìƒ˜í”Œë§ ê°„ê²©ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„
                effective_interval = max(1, sample_interval // 2)
                if frame_id % effective_interval != 0:
                    continue
                
                timestamp = frame_id / fps
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress_percent = (processed_frames / max_frames_to_process) * 100
                print(f"ğŸ”„ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘... ({processed_frames + 1}/{max_frames_to_process}) - {progress_percent:.1f}%")
                
                try:
                    # í”„ë ˆì„ë³„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
                    context_info = {
                        'timestamp': timestamp,
                        'frame_id': frame_id,
                        'lighting_condition': self._analyze_lighting(frame),
                        'scene_complexity': self.frame_sampler.estimate_content_complexity(frame)
                    }
                    
                    # ê³ ë„í™”ëœ ë³´í–‰ì ê²€ì¶œ ë° ì†ì„± ë¶„ì„
                    print(f"   ğŸ” ì‚¬ëŒ ê²€ì¶œ ì¤‘... (í”„ë ˆì„ {frame_id})")
                    detected_persons = self.detect_and_analyze_persons_advanced(
                        frame, frame_id, timestamp, context_info
                    )
                    print(f"   âœ… {len(detected_persons)}ëª… ê²€ì¶œ ì™„ë£Œ")
                    
                    # ì”¬ ë ˆë²¨ ë¶„ì„
                    print(f"   ğŸ¬ ì”¬ ë¶„ì„ ì¤‘...")
                    scene_info = self._analyze_scene_advanced(frame, detected_persons, context_info)
                    print(f"   âœ… ì”¬ ë¶„ì„ ì™„ë£Œ")
                    
                    # í”„ë ˆì„ ê²°ê³¼ ì €ì¥
                    frame_data = {
                        'image_id': frame_id,
                        'timestamp': timestamp,
                        'persons': detected_persons,
                        'person_count': len(detected_persons),
                        'scene_analysis': scene_info,
                        'context': context_info,
                        'quality_assessment': self._assess_frame_quality(frame, detected_persons)
                    }
                    
                    frame_results.append(frame_data)
                    person_database.extend(detected_persons)
                    scene_analysis.append(scene_info)
                    processed_frames += 1
                    
                    # ì§„í–‰ë¥  ë¡œê·¸ (2ì´ˆë§ˆë‹¤ ë˜ëŠ” 10%ë§ˆë‹¤)
                    current_time = time.time()
                    progress_percent = (frame_id / total_frames) * 100
                    
                    if (current_time - last_progress_time >= 2.0) or (processed_frames % max(1, (total_frames // sample_interval) // 10) == 0):
                        elapsed_time = current_time - start_time
                        estimated_total = elapsed_time * (total_frames / frame_id) if frame_id > 0 else 0
                        remaining_time = max(0, estimated_total - elapsed_time)
                        
                        print(f"ğŸ”„ ë¶„ì„ ì§„í–‰ë¥ : {progress_percent:.1f}% ({processed_frames}ê°œ í”„ë ˆì„ ì²˜ë¦¬)")
                        print(f"   - í˜„ì¬ í”„ë ˆì„: {frame_id}/{total_frames}")
                        print(f"   - ê°ì§€ëœ ì‚¬ëŒ: {len(detected_persons)}ëª…")
                        print(f"   - ê²½ê³¼ ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
                        print(f"   - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining_time:.1f}ì´ˆ")
                        print()
                        
                        last_progress_time = current_time
                    
                    # PersonDetection ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    print(f"   ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
                    self._save_person_detections_to_db(video, frame, detected_persons, frame_id, timestamp)
                    print(f"   âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
                    
                    # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    self._update_quality_metrics(quality_metrics, detected_persons)
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback and processed_frames % 5 == 0:
                        progress = (frame_id / total_frames) * 100
                        progress_callback(progress, f"ê³ ë„í™”ëœ ë¶„ì„: {len(person_database)}ëª… ê²€ì¶œ")
                
                except Exception as e:
                    print(f"âš ï¸ í”„ë ˆì„ {frame_id} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            cap.release()
            
            # ê³ ë„í™”ëœ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
            print("ğŸ“‹ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± ì¤‘...")
            analysis_summary = self._create_advanced_analysis_summary(
                person_database, scene_analysis, video, quality_metrics
            )
            print("âœ… ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì™„ë£Œ")
            
            processing_time = time.time() - start_time
            
            print("=" * 60)
            print(f"âœ… ê³ ë„í™”ëœ ë¶„ì„ ì™„ë£Œ!")
            print(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
            print(f"   - ì²˜ë¦¬ëœ í”„ë ˆì„: {processed_frames}ê°œ")
            print(f"   - ê²€ì¶œëœ ì¸ì›: {len(person_database)}ëª…")
            print(f"   - PersonDetection ì €ì¥: {len(person_database)}ê°œ")
            print(f"   - YOLO ê°ì²´ ê°ì§€: í™œì„±í™”")
            print(f"   - í’ˆì§ˆ ì ìˆ˜: {quality_metrics.get('overall_quality', 0):.2f}")
            print(f"   - ë¶„ì„ ì†ë„: {processed_frames/processing_time:.1f} í”„ë ˆì„/ì´ˆ")
            print(f"   - í‰ê·  í”„ë ˆì„ë‹¹ ì²˜ë¦¬ ì‹œê°„: {processing_time/processed_frames:.2f}ì´ˆ")
            print("=" * 60)
            print()
            
            return {
                'success': True,
                'video_summary': analysis_summary,
                'frame_results': frame_results,
                'person_database': person_database,
                'scene_analysis_summary': self._summarize_scene_analysis(scene_analysis),
                'quality_metrics': quality_metrics,
                'analysis_config': {
                    'method': 'Advanced_PAR_Analysis',
                    'analysis_type': analysis_type,
                    'content_complexity': content_complexity,
                    'sampling_interval': sample_interval,
                    'processing_time': processing_time,
                    'total_persons_detected': len(person_database),
                    'frames_analyzed': processed_frames
                },
                'total_frames_analyzed': processed_frames
            }
            
        except Exception as e:
            print(f"âŒ ê³ ë„í™”ëœ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _extract_sample_frames(self, cap, num_samples=5):
        """ìƒ˜í”Œ í”„ë ˆì„ ì¶”ì¶œ"""
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_indices = np.linspace(0, total_frames-1, num_samples, dtype=int)
        
        sample_frames = []
        for idx in sample_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                sample_frames.append(frame)
        
        # ì›ë˜ ìœ„ì¹˜ë¡œ ë˜ëŒë¦¬ê¸°
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        return sample_frames
    
    def _estimate_overall_complexity(self, sample_frames):
        """ì „ì²´ ì½˜í…ì¸  ë³µì¡ë„ ì¶”ì •"""
        if not sample_frames:
            return 'medium'
        
        complexities = []
        for frame in sample_frames:
            complexity = self.frame_sampler.estimate_content_complexity(frame)
            complexities.append(complexity)
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ë³µì¡ë„ ë°˜í™˜
        complexity_counts = Counter(complexities)
        return complexity_counts.most_common(1)[0][0]
    
    def _analyze_lighting(self, frame):
        """ì¡°ëª… ì¡°ê±´ ë¶„ì„"""
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            avg_brightness = np.mean(gray)
            
            if avg_brightness > 150:
                return 'bright'
            elif avg_brightness < 80:
                return 'dark'
            else:
                return 'normal'
        except:
            return 'normal'
    
    def _analyze_scene_advanced(self, frame, detected_persons, context_info):
        """ê³ ë„í™”ëœ ì”¬ ë¶„ì„"""
        try:
            scene_info = {
                'lighting': context_info.get('lighting_condition', 'normal'),
                'complexity': context_info.get('scene_complexity', 'medium'),
                'person_count': len(detected_persons),
                'person_density': self._calculate_person_density(frame, detected_persons),
                'dominant_colors': self._analyze_dominant_colors(frame),
                'activity_level': self._estimate_activity_level(detected_persons),
                'scene_type': self._classify_scene_type(frame, detected_persons)
            }
            
            return scene_info
        except Exception as e:
            print(f"âš ï¸ ì”¬ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _calculate_person_density(self, frame, detected_persons):
        """ì¸ì› ë°€ë„ ê³„ì‚°"""
        if not detected_persons:
            return 0.0
        
        h, w = frame.shape[:2]
        frame_area = h * w
        
        total_person_area = 0
        for person in detected_persons:
            bbox = person['bbox']
            person_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]) * w * h
            total_person_area += person_area
        
        return total_person_area / frame_area
    
    def _analyze_dominant_colors(self, frame):
        """ì£¼ìš” ìƒ‰ìƒ ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ë¥¼ ì‘ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
            small_frame = cv2.resize(frame, (150, 150))
            
            # K-meansë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            data = small_frame.reshape((-1, 3))
            data = np.float32(data)
            
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
            _, labels, centers = cv2.kmeans(data, 3, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
            
            # BGRì„ ìƒ‰ìƒëª…ìœ¼ë¡œ ë³€í™˜
            color_names = []
            for center in centers:
                color_name = self._bgr_to_color_name(center)
                color_names.append(color_name)
            
            return color_names
        except:
            return ['unknown']
    
    def _bgr_to_color_name(self, bgr):
        """BGR ê°’ì„ ìƒ‰ìƒëª…ìœ¼ë¡œ ë³€í™˜"""
        b, g, r = bgr
        
        if r > 150 and g < 100 and b < 100:
            return 'red'
        elif g > 150 and r < 100 and b < 100:
            return 'green'
        elif b > 150 and r < 100 and g < 100:
            return 'blue'
        elif r > 200 and g > 200 and b > 200:
            return 'white'
        elif r < 50 and g < 50 and b < 50:
            return 'black'
        elif r > 150 and g > 150 and b < 100:
            return 'yellow'
        else:
            return 'mixed'
    
    def _estimate_activity_level(self, detected_persons):
        """í™œë™ ìˆ˜ì¤€ ì¶”ì •"""
        if not detected_persons:
            return 'none'
        
        activity_scores = []
        for person in detected_persons:
            posture = person.get('attributes', {}).get('posture', {}).get('value', '')
            
            if 'running' in posture:
                activity_scores.append(3)
            elif 'walking' in posture:
                activity_scores.append(2)
            elif 'standing' in posture:
                activity_scores.append(1)
            else:
                activity_scores.append(0)
        
        if not activity_scores:
            return 'low'
        
        avg_activity = np.mean(activity_scores)
        
        if avg_activity > 2:
            return 'high'
        elif avg_activity > 1:
            return 'medium'
        else:
            return 'low'
    
    def _classify_scene_type(self, frame, detected_persons):
        """ì”¬ íƒ€ì… ë¶„ë¥˜"""
        person_count = len(detected_persons)
        
        if person_count == 0:
            return 'empty'
        elif person_count == 1:
            return 'individual'
        elif person_count <= 3:
            return 'small_group'
        elif person_count <= 10:
            return 'medium_group'
        else:
            return 'crowd'
    
    def _assess_frame_quality(self, frame, detected_persons):
        """í”„ë ˆì„ í’ˆì§ˆ í‰ê°€"""
        try:
            # ì „ì²´ì ì¸ í’ˆì§ˆ ì§€í‘œ
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # ì„ ëª…ë„
            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # ë°ê¸° ë¶„í¬
            brightness_std = np.std(gray)
            
            # ê²€ì¶œ í’ˆì§ˆ
            detection_quality = 0
            if detected_persons:
                quality_scores = [p.get('crop_quality', {}).get('overall', 0) for p in detected_persons]
                detection_quality = np.mean(quality_scores)
            
            overall_quality = (
                min(1.0, sharpness / 500) * 0.4 +
                min(1.0, brightness_std / 50) * 0.3 +
                detection_quality * 0.3
            )
            
            return {
                'overall': float(overall_quality),
                'sharpness': float(sharpness),
                'brightness_distribution': float(brightness_std),
                'detection_quality': float(detection_quality)
            }
        except:
            return {'overall': 0.5}
    
    def _update_quality_metrics(self, quality_metrics, detected_persons):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        quality_metrics['total_detections'] += len(detected_persons)
        
        high_quality_count = sum(1 for p in detected_persons 
                               if p.get('crop_quality', {}).get('overall', 0) > 0.7)
        quality_metrics['high_quality_detections'] += high_quality_count
        
        # ì†ì„± ì‹ ë¢°ë„ í‰ê·  ê³„ì‚°
        confidence_scores = []
        for person in detected_persons:
            attrs = person.get('attributes', {})
            for attr_name, attr_data in attrs.items():
                if isinstance(attr_data, dict) and 'confidence' in attr_data:
                    confidence_scores.append(attr_data['confidence'])
        
        if confidence_scores:
            current_avg = quality_metrics.get('attribute_confidence_avg', 0)
            new_avg = (current_avg + np.mean(confidence_scores)) / 2
            quality_metrics['attribute_confidence_avg'] = new_avg
    
    def _create_advanced_analysis_summary(self, person_database, scene_analysis, video, quality_metrics):
        """ê³ ë„í™”ëœ ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        if not person_database:
            return {'message': 'ê²€ì¶œëœ ë³´í–‰ìê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ê³ ìœ  ì¸ë¬¼ ì¶”ì¶œ
        unique_persons = {}
        for person in person_database:
            track_id = person.get('track_id')
            if track_id and track_id not in unique_persons:
                unique_persons[track_id] = person
        
        # ìƒì„¸ ì†ì„± í†µê³„
        detailed_stats = self._calculate_detailed_statistics(unique_persons.values())
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        temporal_analysis = self._analyze_temporal_patterns(person_database)
        
        # ì”¬ ë‹¤ì–‘ì„± ë¶„ì„
        scene_diversity = self._analyze_scene_diversity(scene_analysis)
        
        # í’ˆì§ˆ í‰ê°€
        overall_quality = self._calculate_overall_quality(quality_metrics)
        
        return {
            'total_detections': len(person_database),
            'unique_persons': len(unique_persons),
            'detailed_attribute_statistics': detailed_stats,
            'temporal_analysis': temporal_analysis,
            'scene_diversity': scene_diversity,
            'quality_assessment': overall_quality,
            'analysis_type': 'advanced_par_analysis',
            'key_insights': self._generate_key_insights(detailed_stats, temporal_analysis, scene_diversity)
        }
    
    def _calculate_detailed_statistics(self, unique_persons):
        """ìƒì„¸ ì†ì„± í†µê³„ ê³„ì‚°"""
        stats = defaultdict(lambda: defaultdict(int))
        
        for person in unique_persons:
            attributes = person.get('attributes', {})
            
            for attr_name, attr_data in attributes.items():
                if isinstance(attr_data, dict) and attr_data.get('confidence', 0) > 0.3:
                    value = attr_data['value']
                    stats[attr_name][value] += 1
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ìœ ì§€
        filtered_stats = {}
        for attr_name, values in stats.items():
            if values:
                # ê°€ì¥ ë¹ˆë²ˆí•œ ìƒìœ„ 5ê°œ
                top_values = dict(sorted(values.items(), key=lambda x: x[1], reverse=True)[:5])
                filtered_stats[attr_name] = top_values
        
        return filtered_stats
    
    def _analyze_temporal_patterns(self, person_database):
        """ì‹œê°„ì  íŒ¨í„´ ë¶„ì„"""
        if not person_database:
            return {}
        
        # ì‹œê°„ëŒ€ë³„ ì¸ì› ìˆ˜
        time_buckets = defaultdict(int)
        
        for person in person_database:
            timestamp = person.get('timestamp', 0)
            time_bucket = int(timestamp // 10) * 10  # 10ì´ˆ ë‹¨ìœ„
            time_buckets[time_bucket] += 1
        
        # í”¼í¬ ì‹œê°„ ì°¾ê¸°
        if time_buckets:
            peak_time = max(time_buckets.items(), key=lambda x: x[1])
            avg_count = np.mean(list(time_buckets.values()))
            
            return {
                'peak_time_seconds': peak_time[0],
                'peak_person_count': peak_time[1],
                'average_person_count': round(avg_count, 2),
                'total_time_span': max(time_buckets.keys()) - min(time_buckets.keys()),
                'activity_distribution': dict(time_buckets)
            }
        
        return {}
    
    def _analyze_scene_diversity(self, scene_analysis):
        """ì”¬ ë‹¤ì–‘ì„± ë¶„ì„"""
        if not scene_analysis:
            return {}
        
        scene_types = [scene.get('scene_type', 'unknown') for scene in scene_analysis]
        activity_levels = [scene.get('activity_level', 'unknown') for scene in scene_analysis]
        lighting_conditions = [scene.get('lighting', 'unknown') for scene in scene_analysis]
        
        return {
            'scene_type_distribution': dict(Counter(scene_types)),
            'activity_level_distribution': dict(Counter(activity_levels)),
            'lighting_distribution': dict(Counter(lighting_conditions)),
            'diversity_score': len(set(scene_types)) / max(len(scene_types), 1)
        }
    
    def _calculate_overall_quality(self, quality_metrics):
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        total_detections = quality_metrics.get('total_detections', 0)
        high_quality_detections = quality_metrics.get('high_quality_detections', 0)
        
        if total_detections == 0:
            return {'overall_score': 0.0, 'status': 'no_data'}
        
        quality_ratio = high_quality_detections / total_detections
        confidence_avg = quality_metrics.get('attribute_confidence_avg', 0)
        
        overall_score = (quality_ratio * 0.6) + (confidence_avg * 0.4)
        
        if overall_score > 0.8:
            status = 'excellent'
        elif overall_score > 0.6:
            status = 'good'
        elif overall_score > 0.4:
            status = 'fair'
        else:
            status = 'poor'
        
        return {
            'overall_score': round(overall_score, 3),
            'status': status,
            'quality_ratio': round(quality_ratio, 3),
            'confidence_average': round(confidence_avg, 3)
        }
    
    def _generate_key_insights(self, detailed_stats, temporal_analysis, scene_diversity):
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì¸êµ¬í†µê³„í•™ì  ì¸ì‚¬ì´íŠ¸
        if 'gender' in detailed_stats:
            gender_stats = detailed_stats['gender']
            if gender_stats:
                dominant_gender = max(gender_stats.items(), key=lambda x: x[1])[0]
                insights.append(f"ì£¼ìš” ì„±ë³„: {dominant_gender}")
        
        # ì‹œê°„ì  ì¸ì‚¬ì´íŠ¸
        if temporal_analysis and 'peak_time_seconds' in temporal_analysis:
            peak_time = temporal_analysis['peak_time_seconds']
            insights.append(f"ìµœëŒ€ í™œë™ ì‹œê°„: {peak_time}ì´ˆ ì§€ì ")
        
        # í™œë™ ì¸ì‚¬ì´íŠ¸
        if scene_diversity and 'activity_level_distribution' in scene_diversity:
            activity_dist = scene_diversity['activity_level_distribution']
            if activity_dist:
                dominant_activity = max(activity_dist.items(), key=lambda x: x[1])[0]
                insights.append(f"ì£¼ìš” í™œë™ ìˆ˜ì¤€: {dominant_activity}")
        
        return insights
    
    def _summarize_scene_analysis(self, scene_analysis):
        """ì”¬ ë¶„ì„ ìš”ì•½"""
        if not scene_analysis:
            return {}
        
        return {
            'total_scenes_analyzed': len(scene_analysis),
            'average_person_density': np.mean([s.get('person_density', 0) for s in scene_analysis]),
            'scene_complexity_distribution': dict(Counter([s.get('complexity', 'unknown') for s in scene_analysis])),
            'lighting_conditions': dict(Counter([s.get('lighting', 'unknown') for s in scene_analysis]))
        }
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
    def analyze_video_comprehensive(self, video, analysis_type='enhanced', progress_callback=None):
        """ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„± ìœ ì§€"""
        return self.analyze_video_comprehensive_advanced(video, analysis_type, progress_callback)
    
    def detect_and_analyze_persons(self, frame, frame_id):
        """ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„± ìœ ì§€"""
        timestamp = frame_id / 30.0  # 30fps ê°€ì •
        return self.detect_and_analyze_persons_advanced(frame, frame_id, timestamp)
    
    def _save_person_detections_to_db(self, video, frame, detected_persons, frame_id, timestamp):
        """PersonDetection ë°ì´í„°ì™€ YOLO ê°ì²´ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            from .models import PersonDetection, Frame
            
            # Frame ê°ì²´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±
            frame_obj, created = Frame.objects.update_or_create(
                video=video,
                image_id=frame_id,
                defaults={
                    'timestamp': timestamp,
                    'caption': f'Frame {frame_id}',
                    'detected_objects': detected_persons
                }
            )
            
            # PersonDetection ë°ì´í„° ì €ì¥
            for i, person in enumerate(detected_persons):
                if person.get('class') == 'person':
                    attributes = person.get('attributes', {})
                    
                    PersonDetection.objects.create(
                        video=video,
                        frame=frame_obj,
                        person_id=i + 1,
                        track_id=person.get('track_id', 0),
                        bbox_x1=person.get('bbox', [0, 0, 0, 0])[0],
                        bbox_y1=person.get('bbox', [0, 0, 0, 0])[1],
                        bbox_x2=person.get('bbox', [0, 0, 0, 0])[2],
                        bbox_y2=person.get('bbox', [0, 0, 0, 0])[3],
                        confidence=person.get('confidence', 0.0),
                        gender_estimation=attributes.get('gender', {}).get('value', 'unknown'),
                        gender_confidence=attributes.get('gender', {}).get('confidence', 0.0),
                        age_group=attributes.get('age', {}).get('value', 'unknown'),
                        age_confidence=attributes.get('age', {}).get('confidence', 0.0),
                        upper_body_color=attributes.get('clothing_color', {}).get('value', 'unknown'),
                        upper_color_confidence=attributes.get('clothing_color', {}).get('confidence', 0.0),
                        lower_body_color=attributes.get('clothing_color', {}).get('value', 'unknown'),
                        lower_color_confidence=attributes.get('clothing_color', {}).get('confidence', 0.0),
                        posture=attributes.get('posture', {}).get('value', 'unknown'),
                        posture_confidence=attributes.get('posture', {}).get('confidence', 0.0),
                        detailed_attributes=attributes
                    )
            
            print(f"âœ… {len(detected_persons)}ëª…ì˜ ì‚¬ëŒ íƒì§€ ì™„ë£Œ (DB ì €ì¥ ì™„ë£Œ)")
            
            # YOLO ê°ì²´ëŠ” ì—¬ì „íˆ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            yolo_objects = self._detect_yolo_objects(frame)
            self._save_yolo_objects_to_db(video, frame, yolo_objects, frame_id, timestamp)
                
        except Exception as e:
            print(f"âš ï¸ PersonDetection ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _save_yolo_objects_to_db(self, video, frame, detected_objects, frame_id, timestamp):
        """YOLO ê°ì²´ ê°ì§€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            from .models import YOLOObjectDetection, Frame
            
            # Frame ê°ì²´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±
            frame_obj, created = Frame.objects.update_or_create(
                video=video,
                image_id=frame_id,
                defaults={
                    'timestamp': timestamp,
                    'caption': f'Frame {frame_id}',
                    'detected_objects': detected_objects
                }
            )
            
            # ê°ì²´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_mapping = {
                'person': 'person',
                'bicycle': 'vehicle', 'car': 'vehicle', 'motorcycle': 'vehicle', 
                'airplane': 'vehicle', 'bus': 'vehicle', 'train': 'vehicle', 'truck': 'vehicle', 'boat': 'vehicle',
                'backpack': 'bag', 'umbrella': 'bag', 'handbag': 'bag', 'tie': 'bag', 'suitcase': 'bag',
                'bottle': 'food', 'wine glass': 'food', 'cup': 'food', 'fork': 'food', 'knife': 'food', 
                'spoon': 'food', 'bowl': 'food', 'banana': 'food', 'apple': 'food', 'sandwich': 'food', 
                'orange': 'food', 'broccoli': 'food', 'carrot': 'food', 'hot dog': 'food', 'pizza': 'food', 
                'donut': 'food', 'cake': 'food',
                'chair': 'furniture', 'couch': 'furniture', 'potted plant': 'furniture', 'bed': 'furniture', 
                'dining table': 'furniture', 'toilet': 'furniture', 'tv': 'furniture',
                'laptop': 'electronics', 'mouse': 'electronics', 'remote': 'electronics', 'keyboard': 'electronics', 
                'cell phone': 'electronics', 'microwave': 'electronics', 'oven': 'electronics', 'toaster': 'electronics', 
                'sink': 'electronics', 'refrigerator': 'electronics'
            }
            
            # ê¸°ì¡´ YOLO ê°ì²´ë“¤ ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
            YOLOObjectDetection.objects.filter(frame=frame_obj).delete()
            
            for i, obj in enumerate(detected_objects):
                # YOLOObjectDetection ê°ì²´ ìƒì„±
                yolo_detection = YOLOObjectDetection.objects.create(
                    video=video,
                    frame=frame_obj,
                    frame_number=frame_id,
                    timestamp=timestamp,
                    bbox_x1=obj['bbox'][0],
                    bbox_y1=obj['bbox'][1],
                    bbox_x2=obj['bbox'][2],
                    bbox_y2=obj['bbox'][3],
                    confidence=obj['confidence'],
                    class_name=obj['class']
                )
                
        except Exception as e:
            print(f"âš ï¸ YOLO ê°ì²´ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _detect_yolo_objects(self, frame):
        """YOLOë¡œ ëª¨ë“  ê°ì²´ ê°ì§€"""
        try:
            if self.model is None:
                return []
            
            # YOLO ì¶”ë¡ 
            results = self.model(frame, verbose=False)
            
            detected_objects = []
            
            for result in results:
                if result.boxes is not None:
                    for box in result.boxes:
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ (ì •ê·œí™”)
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = float(box.conf[0].cpu().numpy())
                        class_id = int(box.cls[0].cpu().numpy())
                        class_name = self.model.names[class_id]
                        
                        # ë””ë²„ê¹…: ëª¨ë“  ê°ì§€ëœ ê°ì²´ ë¡œê·¸ ì¶œë ¥
                        if confidence > 0.05:  # 5% ì´ìƒì´ë©´ ë¡œê·¸ ì¶œë ¥
                            print(f"ğŸ” YOLO ê°ì§€: {class_name} (ì‹ ë¢°ë„: {confidence:.1%})")
                        
                        # ì‹ ë¢°ë„ 1% ì´ìƒë§Œ ì €ì¥ (ê°€ë°© ë“± ì‘ì€ ê°ì²´ ê°ì§€ í–¥ìƒ)
                        if confidence > 0.01:
                            
                            detected_objects.append({
                                'class': class_name,
                                'class_id': class_id,
                                'confidence': confidence,
                                'bbox': [float(x1), float(y1), float(x2), float(y2)],
                                'description': f"{class_name} (ì‹ ë¢°ë„: {confidence:.1%})"
                            })
            
            return detected_objects
            
        except Exception as e:
            print(f"âš ï¸ YOLO ê°ì²´ ê°ì§€ ì˜¤ë¥˜: {e}")
            return []
    
    def _create_person_search_index(self, person_detection, attributes):
        """PersonSearchIndex ìƒì„±"""
        try:
            from .models import PersonSearchIndex
            
            # ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ìƒì„±
            searchable_text_parts = []
            
            # ì„±ë³„
            gender = attributes.get('gender', {}).get('value', '')
            if gender and gender != 'unknown':
                searchable_text_parts.append(gender)
            
            # ë‚˜ì´
            age = attributes.get('age', {}).get('value', '')
            if age and age != 'unknown':
                searchable_text_parts.append(age)
            
            # ì˜ìƒ ìƒ‰ìƒ
            clothing_color = attributes.get('clothing_color', {}).get('value', '')
            if clothing_color and clothing_color != 'unknown':
                searchable_text_parts.append(clothing_color)
            
            # ì†Œì§€í’ˆ
            accessories = attributes.get('accessories', {}).get('value', '')
            if accessories and accessories != 'unknown':
                searchable_text_parts.append(accessories)
            
            # ìì„¸
            posture = attributes.get('posture', {}).get('value', '')
            if posture and posture != 'unknown':
                searchable_text_parts.append(posture)
            
            searchable_text = ' '.join(searchable_text_parts)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            PersonSearchIndex.objects.create(
                person_detection=person_detection,
                searchable_text=searchable_text,
                gender_index=gender if gender != 'unknown' else 'unknown',
                age_group_index=age if age != 'unknown' else 'unknown',
                clothing_colors_index=[clothing_color] if clothing_color != 'unknown' else [],
                accessories_index=[accessories] if accessories != 'unknown' else [],
                posture_index=posture if posture != 'unknown' else 'unknown',
                visual_features_index={
                    'height_estimate': self._estimate_height_from_bbox(person_detection),
                    'body_type': 'unknown',  # í–¥í›„ êµ¬í˜„
                    'hair_style': 'unknown',  # í–¥í›„ êµ¬í˜„
                    'facial_features': []  # í–¥í›„ êµ¬í˜„
                }
            )
            
        except Exception as e:
            print(f"âš ï¸ PersonSearchIndex ìƒì„± ì˜¤ë¥˜: {e}")
    
    def _estimate_height_from_bbox(self, person_detection):
        """ë°”ìš´ë”© ë°•ìŠ¤ë¡œë¶€í„° í‚¤ ì¶”ì •"""
        try:
            height_ratio = (person_detection.bbox_y2 - person_detection.bbox_y1)
            
            if height_ratio > 0.7:  # í™”ë©´ì˜ 70% ì´ìƒ
                return 'tall'
            elif height_ratio < 0.3:  # í™”ë©´ì˜ 30% ë¯¸ë§Œ
                return 'short'
            else:
                return 'medium'
        except:
            return 'unknown'

    # ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ìœ ì§€
    def _find_video_path(self, video):
        """ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°"""
        from django.conf import settings
        
        possible_paths = [
            os.path.join(settings.MEDIA_ROOT, 'videos', video.filename),
            os.path.join(settings.MEDIA_ROOT, 'uploads', video.filename),
            getattr(video, 'file_path', None)
        ]
        
        for path in [p for p in possible_paths if p]:
            if os.path.exists(path):
                return path
        
        return None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_global_enhanced_video_analyzer = None

def get_video_analyzer():
    """ê³ ë„í™”ëœ ë¹„ë””ì˜¤ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_enhanced_video_analyzer
    if _global_enhanced_video_analyzer is None:
        _global_enhanced_video_analyzer = EnhancedVideoAnalyzer()
    return _global_enhanced_video_analyzer

def get_analyzer_status():
    """ë¶„ì„ê¸° ìƒíƒœ ë°˜í™˜"""
    analyzer = get_video_analyzer()
    return {
        'status': 'enhanced' if analyzer.model else 'limited',
        'features': {
            'yolo': analyzer.model is not None,
            'specialized_models': len(analyzer.specialized_models),
            'clip': CLIP_AVAILABLE,
            'advanced_par': True,
            'adaptive_sampling': True,
            'quality_assessment': True
        },
        'device': analyzer.device,
        'analysis_modes': ['basic', 'enhanced', 'comprehensive', 'custom']
    }

# í˜¸í™˜ì„±ì„ ìœ„í•œ í´ë˜ìŠ¤ëª… ë³„ì¹­
VideoAnalyzer = EnhancedVideoAnalyzer