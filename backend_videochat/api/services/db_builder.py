# chat/db_builder.py - ê³ ë„í™”ëœ ë¹„ë””ì˜¤ RAG ì‹œìŠ¤í…œ
from typing import Optional
import os
import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import torch
import numpy as np
from tqdm import tqdm
from dotenv import load_dotenv
from django.conf import settings
from django.core.cache import cache
from langchain_core.documents import Document
import os, logging
from konlpy.tag import Mecab, Okt

logger = logging.getLogger(__name__)

MECAB_DIC = os.getenv("MECAB_DIC", "/opt/homebrew/lib/mecab/dic/mecab-ko-dic")

def make_korean_analyzer(preferred: Optional[str] = None):
    if preferred == "okt":
        return Okt()
    try:
        return Mecab(dicpath=MECAB_DIC)
    except Exception:
        return Okt()

# LangChain ê´€ë ¨ import
try:
    from langchain_community.document_loaders import JSONLoader
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from langchain.retrievers import EnsembleRetriever
    from langchain_community.retrievers import BM25Retriever
    from langchain_openai import ChatOpenAI
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_AVAILABLE = True
except ImportError:
    print("âš ï¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ - RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    LANGCHAIN_AVAILABLE = False

# í•œêµ­ì–´ NLP ì²˜ë¦¬
try:
    from konlpy.tag import Mecab, Hannanum, Kkma
    KONLPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ KoNLPy ë¯¸ì„¤ì¹˜ - í•œêµ­ì–´ ì²˜ë¦¬ ê¸°ëŠ¥ ì œí•œ")
    KONLPY_AVAILABLE = False

load_dotenv()

# ê³ ê¸‰ ì„¤ì •
@dataclass
class AdvancedVideoRAGConfig:
    # FAISS ì„¤ì •
    use_gpu: bool = torch.cuda.is_available()
    nlist: int = 100  # sqrt(N) ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì¡°ì •
    nprobe: int = 10  # nlist/10
    embedding_model: str = "intfloat/multilingual-e5-large"
    embedding_dim: int = 1024
    
    # ì„ë² ë”© ë° ê²€ìƒ‰ ì„¤ì •
    chunk_size: int = 512
    chunk_overlap: int = 128
    top_k: int = 8  # 5ì—ì„œ 8ë¡œ ì¦ê°€
    similarity_threshold: float = 0.75  # 0.8ì—ì„œ 0.75ë¡œ ë‚®ì¶¤
    
    # ê³„ì¸µì  ê²€ìƒ‰ ì„¤ì •
    frame_level_weight: float = 0.4
    segment_level_weight: float = 0.35
    video_level_weight: float = 0.25
    
    # ìºì‹± ì„¤ì •
    cache_ttl_embedding: int = 7200  # 2ì‹œê°„ìœ¼ë¡œ ì¦ê°€
    cache_ttl_analysis: int = 3600   # 1ì‹œê°„ìœ¼ë¡œ ì¦ê°€
    cache_ttl_response: int = 14400  # 4ì‹œê°„ìœ¼ë¡œ ì¦ê°€
    
    # í•œêµ­ì–´ ì²˜ë¦¬ ì„¤ì •
    use_korean_morphology: bool = KONLPY_AVAILABLE
    korean_analyzer: str = "mecab"
    
    # í’ˆì§ˆ í‰ê°€ ì„¤ì •
    min_confidence_threshold: float = 0.3
    quality_boost_threshold: float = 0.7
    
    # ëª¨ë¸ ì„¤ì •
    llm_model: str = "gemma2-9b-it"
    max_tokens: int = 1024
    temperature: float = 0.2

class HierarchicalTemporalIndex:
    """ê³„ì¸µì  ì‹œê°„ì¶• ì¸ë±ì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.timeline = defaultdict(list)
        self.segments = []
        self.events = []
        self.semantic_clusters = {}  # ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°
        self.quality_index = {}  # í’ˆì§ˆë³„ ì¸ë±ìŠ¤
        
    def add_frame_data_advanced(self, timestamp: float, frame_id: int, 
                              caption: str, objects: List[str], scene_data: Dict,
                              quality_score: float = 0.5, attributes: Dict = None):
        """ê³ ë„í™”ëœ í”„ë ˆì„ ë°ì´í„° ì¶”ê°€"""
        event = {
            'timestamp': timestamp,
            'frame_id': frame_id,
            'caption': caption,
            'objects': objects,
            'scene_type': scene_data.get('scene_type', ''),
            'lighting': scene_data.get('lighting', 'normal'),
            'activity_level': scene_data.get('activity_level', 'low'),
            'person_count': scene_data.get('person_count', 0),
            'quality_score': quality_score,
            'attributes': attributes or {},
            'semantic_keywords': self._extract_semantic_keywords(caption, objects)
        }
        
        self.timeline[timestamp].append(event)
        self.events.append(event)
        
        # í’ˆì§ˆë³„ ì¸ë±ì‹±
        quality_tier = self._get_quality_tier(quality_score)
        if quality_tier not in self.quality_index:
            self.quality_index[quality_tier] = []
        self.quality_index[quality_tier].append(event)
    
    def _extract_semantic_keywords(self, caption: str, objects: List[str]) -> List[str]:
        """ì˜ë¯¸ì  í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ê°ì²´ì—ì„œ í‚¤ì›Œë“œ
        keywords.extend(objects)
        
        # ìº¡ì…˜ì—ì„œ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
        if caption:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš©)
            important_words = ['ì‚¬ëŒ', 'ë‚¨ì', 'ì—¬ì', 'ì•„ì´', 'ì°¨', 'ê±´ë¬¼', 'ê¸¸', 'ì˜·', 'ê°€ë°©']
            for word in important_words:
                if word in caption:
                    keywords.append(word)
        
        return list(set(keywords))
    
    def _get_quality_tier(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if quality_score >= 0.8:
            return 'high'
        elif quality_score >= 0.5:
            return 'medium'
        else:
            return 'low'
    
    def create_hierarchical_segments(self, segment_duration: float = 30.0):
        """ê³„ì¸µì  ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        if not self.events:
            return
        
        max_time = max(event['timestamp'] for event in self.events)
        current_time = 0
        
        while current_time < max_time:
            end_time = min(current_time + segment_duration, max_time)
            
            segment_events = [
                event for event in self.events 
                if current_time <= event['timestamp'] < end_time
            ]
            
            if segment_events:
                segment = self._create_detailed_segment(segment_events, current_time, end_time)
                self.segments.append(segment)
            
            current_time = end_time
        
        # ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§
        self._create_semantic_clusters()
    
    def _create_detailed_segment(self, events: List[Dict], start_time: float, end_time: float) -> Dict:
        """ìƒì„¸ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        # í’ˆì§ˆ ê¸°ë°˜ ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜
        high_quality_events = [e for e in events if e['quality_score'] >= 0.7]
        representative_events = high_quality_events if high_quality_events else events
        
        # ì£¼ìš” ê°ì²´ ì¶”ì¶œ (í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©)
        object_weights = defaultdict(float)
        for event in events:
            weight = event['quality_score']
            for obj in event['objects']:
                object_weights[obj] += weight
        
        dominant_objects = sorted(object_weights.keys(), key=object_weights.get, reverse=True)[:5]
        
        # í™œë™ ìˆ˜ì¤€ ë¶„ì„
        activity_levels = [e['activity_level'] for e in events if e['activity_level']]
        dominant_activity = max(set(activity_levels), key=activity_levels.count) if activity_levels else 'unknown'
        
        # ì¸êµ¬ í†µê³„
        person_counts = [e['person_count'] for e in events]
        avg_person_count = np.mean(person_counts) if person_counts else 0
        
        # ì”¬ ìš”ì•½ ìƒì„±
        scene_summary = self._generate_advanced_scene_summary(representative_events, dominant_objects, dominant_activity)
        
        return {
            'start_time': start_time,
            'end_time': end_time,
            'duration': end_time - start_time,
            'events': events,
            'high_quality_events': high_quality_events,
            'dominant_objects': dominant_objects,
            'object_weights': dict(object_weights),
            'scene_summary': scene_summary,
            'dominant_activity': dominant_activity,
            'average_person_count': avg_person_count,
            'quality_distribution': self._calculate_quality_distribution(events),
            'semantic_keywords': self._extract_segment_keywords(events)
        }
    
    def _generate_advanced_scene_summary(self, events: List[Dict], objects: List[str], activity: str) -> str:
        """ê³ ë„í™”ëœ ì”¬ ìš”ì•½ ìƒì„±"""
        if not events:
            return "ë¹ˆ êµ¬ê°„"
        
        # ëŒ€í‘œ ì´ë²¤íŠ¸ ì„ íƒ
        best_event = max(events, key=lambda e: e['quality_score'])
        
        # ê¸°ë³¸ ì •ë³´
        scene_type = best_event.get('scene_type', 'ì¼ë°˜')
        lighting = best_event.get('lighting', 'normal')
        
        # ìš”ì•½ ìƒì„±
        summary_parts = []
        
        if scene_type and scene_type != 'ì¼ë°˜':
            summary_parts.append(f"{scene_type} í™˜ê²½")
        
        if lighting != 'normal':
            summary_parts.append(f"{lighting} ì¡°ëª…")
        
        if objects:
            if len(objects) == 1:
                summary_parts.append(f"{objects[0]} ì¤‘ì‹¬")
            else:
                summary_parts.append(f"{objects[0]}, {objects[1]} ë“± ë‹¤ì–‘í•œ ê°ì²´")
        
        if activity != 'unknown':
            summary_parts.append(f"{activity} í™œë™")
        
        return "ì—ì„œ ".join(summary_parts) if summary_parts else "ì¼ë°˜ì ì¸ ì¥ë©´"
    
    def _calculate_quality_distribution(self, events: List[Dict]) -> Dict:
        """í’ˆì§ˆ ë¶„í¬ ê³„ì‚°"""
        if not events:
            return {}
        
        quality_scores = [e['quality_score'] for e in events]
        
        return {
            'average': np.mean(quality_scores),
            'max': np.max(quality_scores),
            'min': np.min(quality_scores),
            'std': np.std(quality_scores),
            'high_quality_ratio': sum(1 for q in quality_scores if q >= 0.7) / len(quality_scores)
        }
    
    def _extract_segment_keywords(self, events: List[Dict]) -> List[str]:
        """ì„¸ê·¸ë¨¼íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_keywords = []
        for event in events:
            all_keywords.extend(event.get('semantic_keywords', []))
        
        # ë¹ˆë„ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ
        keyword_counts = Counter(all_keywords)
        return [keyword for keyword, count in keyword_counts.most_common(10)]
    
    def _create_semantic_clusters(self):
        """ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ìƒì„±"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        keyword_to_events = defaultdict(list)
        
        for event in self.events:
            for keyword in event.get('semantic_keywords', []):
                keyword_to_events[keyword].append(event)
        
        # í´ëŸ¬ìŠ¤í„° ìƒì„± (ìµœì†Œ 2ê°œ ì´ìƒì˜ ì´ë²¤íŠ¸)
        for keyword, events in keyword_to_events.items():
            if len(events) >= 2:
                self.semantic_clusters[keyword] = {
                    'events': events,
                    'count': len(events),
                    'quality_score': np.mean([e['quality_score'] for e in events]),
                    'time_span': max(e['timestamp'] for e in events) - min(e['timestamp'] for e in events)
                }

class AdvancedKoreanTextProcessor:
    """ê³ ë„í™”ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    
    def __init__(self, analyzer: str = "mecab"):
        self.analyzer_type = analyzer
        self.analyzer = make_korean_analyzer(analyzer)
        
        # í™•ì¥ëœ íŒ¨í„´ ì •ì˜
        self.temporal_patterns = [
            r'(\d+)ì´ˆ', r'(\d+)ë¶„', r'(\d+)ì‹œê°„',
            r'ì²˜ìŒì—?', r'ë§ˆì§€ë§‰ì—?', r'ì¤‘ê°„ì—?', r'ì‹œì‘í• ?\s?ë•Œ?', r'ëë‚ ?\s?ë•Œ?',
            r'ë¨¼ì €', r'ë‚˜ì¤‘ì—?', r'ê·¸\s?ë‹¤ìŒì—?', r'ì´í›„ì—?', r'ì „ì—?',
            r'ì–¸ì œ', r'ëª‡\s?ë¶„', r'ëª‡\s?ì´ˆ', r'ì–¼ë§ˆë‚˜', r'ë™ì•ˆ',
            r'(\d+)ì‹œ\s?(\d+)ë¶„?', r'ì˜¤ì „', r'ì˜¤í›„', r'ìƒˆë²½', r'ì €ë…'
        ]
        
        self.person_patterns = [
            r'ì‚¬ëŒ', r'ë‚¨ì', r'ì—¬ì', r'ì•„ì´', r'ì–´ë¦°ì´', r'ì²­ì†Œë…„', r'ì„±ì¸', r'ë…¸ì¸',
            r'ì†Œë…„', r'ì†Œë…€', r'ë‚¨ì„±', r'ì—¬ì„±', r'ì¸ë¬¼', r'ë³´í–‰ì'
        ]
        
        self.object_patterns = [
            r'ì°¨', r'ìë™ì°¨', r'ë²„ìŠ¤', r'íŠ¸ëŸ­', r'ì˜¤í† ë°”ì´', r'ìì „ê±°',
            r'ê°€ë°©', r'í•¸ë“œë°±', r'ë°±íŒ©', r'ëª¨ì', r'ì•ˆê²½', r'ì˜·', r'ì‹ ë°œ'
        ]
        
        self.action_patterns = [
            r'ê±·[ëŠ”ë‹¤ê°€ê¸°]', r'ë›°[ëŠ”ë‹¤ê°€ê¸°]', r'ë‹¬ë¦¬[ëŠ”ë‹¤ê°€ê¸°]', r'ì„œ[ìˆëŠ”ë‹¤ê°€]',
            r'ì•‰[ì•„ìˆëŠ”ë‹¤ê°€]', r'ì›€ì§[ì´ì¸ë‹¤ê°€ì„]', r'ì§€ë‚˜[ê°€ëŠ”ë‹¤ê°„]'
        ]
    
    def extract_temporal_markers_advanced(self, text: str) -> Dict[str, List[str]]:
        """ê³ ë„í™”ëœ ì‹œê°„ í‘œí˜„ ì¶”ì¶œ"""
        import re
        
        markers = {
            'time_expressions': [],
            'sequence_markers': [],
            'duration_markers': [],
            'specific_times': []
        }
        
        for pattern in self.temporal_patterns:
            matches = re.findall(pattern, text)
            if matches:
                if any(word in pattern for word in ['ì´ˆ', 'ë¶„', 'ì‹œê°„']):
                    markers['duration_markers'].extend(matches)
                elif any(word in pattern for word in ['ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ë¨¼ì €', 'ë‚˜ì¤‘']):
                    markers['sequence_markers'].extend(matches)
                elif any(word in pattern for word in ['ì‹œ', 'ì˜¤ì „', 'ì˜¤í›„']):
                    markers['specific_times'].extend(matches)
                else:
                    markers['time_expressions'].extend(matches)
        
        return markers
    
    def analyze_question_intent_advanced(self, question: str) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ ì§ˆë¬¸ ì˜ë„ ë¶„ì„"""
        intent = {
            'primary_type': 'general',
            'secondary_types': [],
            'confidence': 0.5,
            'entities': {
                'temporal': [],
                'persons': [],
                'objects': [],
                'actions': [],
                'attributes': []
            },
            'complexity': 'simple',
            'required_analysis_level': 'frame'
        }
        
        q = question.lower()
        
        # ì‹œê°„ ê´€ë ¨ ë¶„ì„
        temporal_markers = self.extract_temporal_markers_advanced(question)
        if any(temporal_markers.values()):
            intent['primary_type'] = 'temporal'
            intent['confidence'] += 0.3
            intent['entities']['temporal'] = temporal_markers
            intent['required_analysis_level'] = 'segment'
        
        # ì‚¬ëŒ ê´€ë ¨ ë¶„ì„
        import re
        person_matches = []
        for pattern in self.person_patterns:
            matches = re.findall(pattern, q)
            person_matches.extend(matches)
        
        if person_matches:
            if intent['primary_type'] == 'general':
                intent['primary_type'] = 'person_analysis'
            else:
                intent['secondary_types'].append('person_analysis')
            intent['confidence'] += 0.2
            intent['entities']['persons'] = person_matches
        
        # ê°ì²´ ê´€ë ¨ ë¶„ì„
        object_matches = []
        for pattern in self.object_patterns:
            matches = re.findall(pattern, q)
            object_matches.extend(matches)
        
        if object_matches:
            if intent['primary_type'] == 'general':
                intent['primary_type'] = 'object_detection'
            else:
                intent['secondary_types'].append('object_detection')
            intent['confidence'] += 0.2
            intent['entities']['objects'] = object_matches
        
        # í–‰ë™ ê´€ë ¨ ë¶„ì„
        action_matches = []
        for pattern in self.action_patterns:
            matches = re.findall(pattern, q)
            action_matches.extend(matches)
        
        if action_matches:
            if intent['primary_type'] == 'general':
                intent['primary_type'] = 'action_recognition'
            else:
                intent['secondary_types'].append('action_recognition')
            intent['confidence'] += 0.2
            intent['entities']['actions'] = action_matches
            intent['required_analysis_level'] = 'segment'
        
        # ë³µì¡ë„ íŒë‹¨
        complexity_indicators = len(intent['secondary_types']) + len([v for v in intent['entities'].values() if v])
        
        if complexity_indicators >= 3:
            intent['complexity'] = 'complex'
            intent['required_analysis_level'] = 'video'
        elif complexity_indicators >= 2:
            intent['complexity'] = 'moderate'
        
        # ì†ì„± ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        attribute_keywords = ['ìƒ‰ê¹”', 'ì˜·', 'ëª¨ì', 'ê°€ë°©', 'ì•ˆê²½', 'í‚¤', 'ëª¸ë¬´ê²Œ', 'ë‚˜ì´']
        found_attributes = [attr for attr in attribute_keywords if attr in q]
        if found_attributes:
            intent['entities']['attributes'] = found_attributes
            if intent['primary_type'] == 'general':
                intent['primary_type'] = 'attribute_analysis'
            else:
                intent['secondary_types'].append('attribute_analysis')
        
        return intent

class MultiLevelCacheManager:
    """ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: AdvancedVideoRAGConfig):
        self.config = config
        self.memory_cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ
        self.quality_cache = {}  # í’ˆì§ˆë³„ ìºì‹œ
        
    def get_cache_key(self, video_id: str, query: str, cache_type: str, quality_level: str = 'all') -> str:
        """ê³„ì¸µì  ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{video_id}:{query}:{cache_type}:{quality_level}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_hierarchical_cache(self, video_id: str, query: str, analysis_level: str) -> Optional[Dict]:
        """ê³„ì¸µì  ìºì‹œ ì¡°íšŒ"""
        cache_levels = ['frame', 'segment', 'video']
        
        # ìš”ì²­ëœ ë ˆë²¨ë¶€í„° ìƒìœ„ ë ˆë²¨ê¹Œì§€ ê²€ìƒ‰
        for level in cache_levels[cache_levels.index(analysis_level):]:
            key = self.get_cache_key(video_id, query, f"search_{level}")
            
            # Django ìºì‹œ ì¡°íšŒ
            cached = cache.get(key)
            if cached:
                return {
                    'results': cached['results'],
                    'cached_level': level,
                    'timestamp': cached['timestamp']
                }
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì¡°íšŒ
            if key in self.memory_cache:
                return self.memory_cache[key]
        
        return None
    
    def set_hierarchical_cache(self, video_id: str, query: str, analysis_level: str, results: List[Dict]):
        """ê³„ì¸µì  ìºì‹œ ì €ì¥"""
        key = self.get_cache_key(video_id, query, f"search_{analysis_level}")
        
        cache_data = {
            'results': results,
            'analysis_level': analysis_level,
            'timestamp': time.time()
        }
        
        # Django ìºì‹œ ì €ì¥
        cache.set(key, cache_data, timeout=self.config.cache_ttl_analysis)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ (ì œí•œëœ í¬ê¸°)
        if len(self.memory_cache) < 100:
            self.memory_cache[key] = cache_data
    
    def get_quality_aware_cache(self, video_id: str, query: str, min_quality: float = 0.5) -> Optional[List[Dict]]:
        """í’ˆì§ˆ ì¸ì‹ ìºì‹œ ì¡°íšŒ"""
        quality_levels = ['high', 'medium', 'low']
        quality_thresholds = {'high': 0.7, 'medium': 0.5, 'low': 0.0}
        
        for quality_level in quality_levels:
            if quality_thresholds[quality_level] >= min_quality:
                key = self.get_cache_key(video_id, query, "quality_search", quality_level)
                cached = cache.get(key)
                if cached:
                    return cached
        
        return None

class SuperiorVideoRAGSystem:
    """ìµœê³ ê¸‰ ë¹„ë””ì˜¤ ë¶„ì„ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[AdvancedVideoRAGConfig] = None):
        self.config = config or AdvancedVideoRAGConfig()
        self.device = "cuda" if self.config.use_gpu and torch.cuda.is_available() else "cpu"
        
        # ê³ ë„í™”ëœ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.cache_manager = MultiLevelCacheManager(self.config)
        self.korean_processor = AdvancedKoreanTextProcessor(self.config.korean_analyzer)
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self._embeddings_initialized = False
        self._llm_initialized = False
        
        print(f"ğŸš€ Superior VideoRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
        
        if not LANGCHAIN_AVAILABLE:
            print("âš ï¸ LangChain ë¯¸ì„¤ì¹˜ - ê¸°ë³¸ RAG ê¸°ëŠ¥ë§Œ ì‚¬ìš©")
            return
        
        try:
            self._init_advanced_embeddings()
            self._init_llm()
        except Exception as e:
            print(f"âš ï¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë¶€ë¶„ ì‹¤íŒ¨: {e}")
        
        # ë¹„ë””ì˜¤ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì†Œ
        self.video_databases = {}
        self.temporal_indexes = {}
        self.quality_indexes = {}
        
        print("âœ… Superior VideoRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_advanced_embeddings(self):
        """ê³ ë„í™”ëœ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model_kwargs = {
                "device": self.device,
                "trust_remote_code": True
            }
            encode_kwargs = {
                'normalize_embeddings': True,
                'batch_size': 64,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
                'show_progress_bar': False
            }
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.config.embedding_model,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )
            self._embeddings_initialized = True
            print(f"âœ… ê³ ë„í™”ëœ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.config.embedding_model}")
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._embeddings_initialized = False
    
    def _init_llm(self):
        """LLM ì´ˆê¸°í™”"""
        try:
            self.llm = ChatOpenAI(
                model=self.config.llm_model,
                openai_api_key=os.environ["GROQ_API_KEY"],
                openai_api_base="https://api.groq.com/openai/v1",
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            self._llm_initialized = True
            print(f"âœ… ê³ ë„í™”ëœ LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.config.llm_model}")
        except Exception as e:
            print(f"âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._llm_initialized = False
    
    def process_video_analysis_json_advanced(self, json_file_path: str, video_id: str) -> bool:
        """ìµœê³ ê¸‰ ë¹„ë””ì˜¤ ë¶„ì„ JSON ì²˜ë¦¬"""
        try:
            if not os.path.exists(json_file_path):
                print(f"âš ï¸ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {json_file_path}")
                return False
            
            print(f"ğŸ“„ ê³ ë„í™”ëœ JSON ë¶„ì„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {json_file_path}")
            
            with open(json_file_path, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
            
            # ê³„ì¸µì  ì‹œê°„ì¶• ì¸ë±ìŠ¤ ìƒì„±
            temporal_index = HierarchicalTemporalIndex()
            
            # ë‹¤ì¸µ ë¬¸ì„œ ìƒì„±
            frame_documents = []
            segment_documents = []
            semantic_documents = []
            
            frame_results = analysis_data.get('frame_results', [])
            video_metadata = analysis_data.get('metadata', {})
            
            print(f"ğŸ“Š ì²˜ë¦¬í•  í”„ë ˆì„ ìˆ˜: {len(frame_results)}")
            
            # í”„ë ˆì„ë³„ ê³ ë„í™”ëœ ë¬¸ì„œ ìƒì„±
            for i, frame_result in enumerate(frame_results):
                frame_id = frame_result.get('image_id', i)
                timestamp = frame_result.get('timestamp', 0)
                
                # ë‹¤ì–‘í•œ ìº¡ì…˜ ì†ŒìŠ¤ í†µí•©
                caption = (frame_result.get('final_caption') or 
                          frame_result.get('enhanced_caption') or 
                          frame_result.get('caption') or '')
                
                objects = frame_result.get('objects', [])
                scene_analysis = frame_result.get('scene_analysis', {})
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self._calculate_frame_quality_score(frame_result)
                
                # ê³„ì¸µì  ì‹œê°„ì¶• ì¸ë±ìŠ¤ì— ì¶”ê°€
                temporal_index.add_frame_data_advanced(
                    timestamp, frame_id, caption, 
                    [obj.get('class', '') for obj in objects],
                    {
                        'scene_type': scene_analysis.get('scene_classification', {}).get('location', {}).get('label', ''),
                        'lighting': scene_analysis.get('lighting', 'normal'),
                        'activity_level': scene_analysis.get('activity_level', 'low'),
                        'person_count': len([obj for obj in objects if obj.get('class') == 'person'])
                    },
                    quality_score,
                    frame_result.get('attributes', {})
                )
                
                # ê³ ë„í™”ëœ í”„ë ˆì„ ë¬¸ì„œ ìƒì„±
                content_parts = self._build_advanced_frame_content(
                    frame_id, timestamp, caption, objects, scene_analysis, quality_score
                )
                
                if content_parts:
                    metadata = {
                        'video_id': video_id,
                        'frame_id': frame_id,
                        'timestamp': timestamp,
                        'objects': [obj.get('class', '') for obj in objects],
                        'scene_type': scene_analysis.get('scene_classification', {}).get('location', {}).get('label', ''),
                        'quality_score': quality_score,
                        'quality_tier': self._get_quality_tier(quality_score),
                        'level': 'frame'
                    }
                    
                    frame_documents.append(Document(
                        page_content='. '.join(content_parts), 
                        metadata=metadata
                    ))
            
            # ê³„ì¸µì  ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            temporal_index.create_hierarchical_segments()
            
            print(f"ğŸ“Š ìƒì„±ëœ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(temporal_index.segments)}")
            
            # ê³ ë„í™”ëœ ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¬¸ì„œ ìƒì„±
            for segment in temporal_index.segments:
                segment_content = self._build_advanced_segment_content(segment)
                
                metadata = {
                    'video_id': video_id,
                    'start_time': segment['start_time'],
                    'end_time': segment['end_time'],
                    'duration': segment['duration'],
                    'dominant_objects': segment['dominant_objects'],
                    'scene_summary': segment['scene_summary'],
                    'quality_distribution': segment['quality_distribution'],
                    'average_person_count': segment['average_person_count'],
                    'level': 'segment'
                }
                
                segment_documents.append(Document(
                    page_content=segment_content,
                    metadata=metadata
                ))
            
            # ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë¬¸ì„œ ìƒì„±
            for keyword, cluster in temporal_index.semantic_clusters.items():
                cluster_content = self._build_semantic_cluster_content(keyword, cluster)
                
                metadata = {
                    'video_id': video_id,
                    'semantic_keyword': keyword,
                    'event_count': cluster['count'],
                    'quality_score': cluster['quality_score'],
                    'time_span': cluster['time_span'],
                    'level': 'semantic'
                }
                
                semantic_documents.append(Document(
                    page_content=cluster_content,
                    metadata=metadata
                ))
            # ì „ì²´ ë¹„ë””ì˜¤ ë¬¸ì„œ ìƒì„±
            video_document = self._build_comprehensive_video_document(analysis_data, temporal_index)
            video_metadata_doc = {
                'video_id': video_id,
                'level': 'video',
                'total_frames': len(frame_results),
                'duration': video_metadata.get('duration', 0),
                'analysis_type': video_metadata.get('analysis_type', 'unknown'),
                'quality_summary': self._calculate_video_quality_summary(temporal_index)
            }
            
            all_documents = frame_documents + segment_documents + semantic_documents + [Document(
                page_content=video_document,
                metadata=video_metadata_doc
            )]
            
            print(f"ğŸ“Š ì´ ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(all_documents)}")
            
            # ìµœê³ ê¸‰ ê³„ì¸µì  ë²¡í„° DB ìƒì„±
            success = self._create_superior_hierarchical_vector_db(video_id, all_documents)
            
            if success:
                # ì‹œê°„ì¶• ì¸ë±ìŠ¤ ë° í’ˆì§ˆ ì¸ë±ìŠ¤ ì €ì¥
                self.temporal_indexes[video_id] = temporal_index
                self.quality_indexes[video_id] = self._create_quality_index(frame_results)
                
                print(f"âœ… ë¹„ë””ì˜¤ {video_id} ìµœê³ ê¸‰ RAG DB ìƒì„± ì™„ë£Œ: {len(all_documents)}ê°œ ë¬¸ì„œ")
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ ìµœê³ ê¸‰ RAG DB ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def _calculate_frame_quality_score(self, frame_result: Dict) -> float:
        """í”„ë ˆì„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_factors = []
        
        # ìº¡ì…˜ í’ˆì§ˆ
        caption = frame_result.get('final_caption') or frame_result.get('enhanced_caption') or frame_result.get('caption', '')
        if caption:
            caption_quality = min(1.0, len(caption.split()) / 10)  # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜
            quality_factors.append(caption_quality * 0.3)
        
        # ê°ì²´ ê°ì§€ í’ˆì§ˆ
        objects = frame_result.get('objects', [])
        if objects:
            avg_confidence = np.mean([obj.get('confidence', 0) for obj in objects])
            quality_factors.append(avg_confidence * 0.4)
        
        # ì†ì„± ë¶„ì„ í’ˆì§ˆ
        persons = frame_result.get('persons', [])
        if persons:
            attr_confidences = []
            for person in persons:
                attrs = person.get('attributes', {})
                for attr_name, attr_data in attrs.items():
                    if isinstance(attr_data, dict) and 'confidence' in attr_data:
                        attr_confidences.append(attr_data['confidence'])
            
            if attr_confidences:
                avg_attr_confidence = np.mean(attr_confidences)
                quality_factors.append(avg_attr_confidence * 0.3)
        
        return np.mean(quality_factors) if quality_factors else 0.5
    
    def _get_quality_tier(self, quality_score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if quality_score >= self.config.quality_boost_threshold:
            return 'high'
        elif quality_score >= self.config.min_confidence_threshold:
            return 'medium'
        else:
            return 'low'
    
    def _build_advanced_frame_content(self, frame_id: int, timestamp: float, 
                                    caption: str, objects: List, scene_analysis: Dict, 
                                    quality_score: float) -> List[str]:
        """ê³ ë„í™”ëœ í”„ë ˆì„ ë‚´ìš© êµ¬ì„±"""
        content_parts = []
        
        # í’ˆì§ˆ ì •ë³´ í¬í•¨
        quality_indicator = "ê³ í’ˆì§ˆ" if quality_score >= 0.7 else "ì¤‘í’ˆì§ˆ" if quality_score >= 0.4 else "ì €í’ˆì§ˆ"
        
        if caption:
            content_parts.append(f"í”„ë ˆì„ {frame_id} ({timestamp:.1f}ì´ˆ, {quality_indicator}): {caption}")
        
        # ê°ì²´ ì •ë³´ (ì‹ ë¢°ë„ í¬í•¨)
        if objects:
            high_conf_objects = [obj for obj in objects if obj.get('confidence', 0) > 0.5]
            if high_conf_objects:
                object_list = [f"{obj.get('class', '')}({obj.get('confidence', 0):.2f})" 
                             for obj in high_conf_objects]
                content_parts.append(f"ê³ ì‹ ë¢°ë„ ê°ì²´: {', '.join(object_list)}")
            
            # ëª¨ë“  ê°ì²´ë„ í¬í•¨
            all_objects = [obj.get('class', '') for obj in objects if obj.get('class')]
            if all_objects:
                content_parts.append(f"ì „ì²´ ê°ì§€ ê°ì²´: {', '.join(set(all_objects))}")
        
        # ìƒì„¸ ì¥ë©´ ë¶„ì„
        if scene_analysis:
            scene_details = []
            
            scene_class = scene_analysis.get('scene_classification', {})
            if scene_class:
                location = scene_class.get('location', {}).get('label', '')
                time_of_day = scene_class.get('time', {}).get('label', '')
                if location or time_of_day:
                    scene_details.append(f"ì¥ë©´: {location} {time_of_day}".strip())
            
            # ì¶”ê°€ ì”¬ ì •ë³´
            if scene_analysis.get('lighting'):
                scene_details.append(f"ì¡°ëª…: {scene_analysis['lighting']}")
            
            if scene_analysis.get('activity_level'):
                scene_details.append(f"í™œë™ë„: {scene_analysis['activity_level']}")
            
            if scene_details:
                content_parts.append(", ".join(scene_details))
            
            # OCR í…ìŠ¤íŠ¸
            ocr_text = scene_analysis.get('ocr_text', '')
            if ocr_text:
                content_parts.append(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {ocr_text}")
        
        return content_parts
    
    def _build_advanced_segment_content(self, segment: Dict) -> str:
        """ê³ ë„í™”ëœ ì„¸ê·¸ë¨¼íŠ¸ ë‚´ìš© êµ¬ì„±"""
        start_time = segment['start_time']
        end_time = segment['end_time']
        duration = segment['duration']
        scene_summary = segment['scene_summary']
        dominant_objects = segment['dominant_objects']
        avg_person_count = segment['average_person_count']
        quality_dist = segment['quality_distribution']
        
        content_parts = []
        
        # ê¸°ë³¸ ì‹œê°„ ì •ë³´
        content_parts.append(f"{start_time:.1f}ì´ˆ-{end_time:.1f}ì´ˆ êµ¬ê°„ ({duration:.1f}ì´ˆ ì§€ì†)")
        
        # ì”¬ ìš”ì•½
        content_parts.append(f"ì¥ë©´ ìš”ì•½: {scene_summary}")
        
        # ì£¼ìš” ê°ì²´ (ê°€ì¤‘ì¹˜ í¬í•¨)
        if dominant_objects:
            content_parts.append(f"ì£¼ìš” ê°ì²´: {', '.join(dominant_objects[:3])}")
        
        # ì¸ì› ì •ë³´
        if avg_person_count > 0:
            content_parts.append(f"í‰ê·  ì¸ì›: {avg_person_count:.1f}ëª…")
        
        # í’ˆì§ˆ ì •ë³´
        if quality_dist and 'average' in quality_dist:
            quality_desc = "ê³ í’ˆì§ˆ" if quality_dist['average'] >= 0.7 else "ì¤‘í’ˆì§ˆ" if quality_dist['average'] >= 0.4 else "ì €í’ˆì§ˆ"
            content_parts.append(f"ë¶„ì„ í’ˆì§ˆ: {quality_desc} (í‰ê·  {quality_dist['average']:.2f})")
        
        # í™œë™ ì •ë³´
        if 'dominant_activity' in segment:
            content_parts.append(f"ì£¼ìš” í™œë™: {segment['dominant_activity']}")
        
        # ê³ í’ˆì§ˆ ì´ë²¤íŠ¸ ìˆ˜
        high_quality_events = len(segment.get('high_quality_events', []))
        total_events = len(segment.get('events', []))
        if total_events > 0:
            content_parts.append(f"ê³ í’ˆì§ˆ ì´ë²¤íŠ¸: {high_quality_events}/{total_events}")
        
        return ". ".join(content_parts)
    
    def _build_semantic_cluster_content(self, keyword: str, cluster: Dict) -> str:
        """ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë‚´ìš© êµ¬ì„±"""
        event_count = cluster['count']
        quality_score = cluster['quality_score']
        time_span = cluster['time_span']
        
        content = f"'{keyword}' ê´€ë ¨ ì¥ë©´ë“¤: {event_count}ê°œ ì´ë²¤íŠ¸ì—ì„œ ë°œìƒ"
        
        if time_span > 0:
            content += f", {time_span:.1f}ì´ˆì— ê±¸ì³ ë‚˜íƒ€ë‚¨"
        
        quality_desc = "ê³ í’ˆì§ˆ" if quality_score >= 0.7 else "ì¤‘í’ˆì§ˆ" if quality_score >= 0.4 else "ì €í’ˆì§ˆ"
        content += f", ë¶„ì„ í’ˆì§ˆ: {quality_desc}"
        
        # ì´ë²¤íŠ¸ ì„¸ë¶€ ì •ë³´
        events = cluster.get('events', [])
        if events:
            timestamps = [e['timestamp'] for e in events]
            content += f", ì£¼ìš” ì¶œí˜„ ì‹œì : {min(timestamps):.1f}ì´ˆ-{max(timestamps):.1f}ì´ˆ"
        
        return content
    
    def _build_comprehensive_video_document(self, analysis_data: Dict, temporal_index: HierarchicalTemporalIndex) -> str:
        """ì¢…í•©ì ì¸ ë¹„ë””ì˜¤ ë¬¸ì„œ êµ¬ì„±"""
        metadata = analysis_data.get('metadata', {})
        
        content_parts = [
            f"ë¹„ë””ì˜¤ ì¢…í•© ë¶„ì„ ê²°ê³¼:",
            f"- ì´ ê¸¸ì´: {metadata.get('duration', 0)}ì´ˆ",
            f"- ë¶„ì„ëœ í”„ë ˆì„: {len(analysis_data.get('frame_results', []))}ê°œ",
            f"- ìƒì„±ëœ êµ¬ê°„: {len(temporal_index.segments)}ê°œ",
            f"- ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°: {len(temporal_index.semantic_clusters)}ê°œ"
        ]
        
        # ì „ì²´ ë¹„ë””ì˜¤ì˜ ì£¼ìš” ê°ì²´ (í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©)
        all_objects_weighted = defaultdict(float)
        for event in temporal_index.events:
            weight = event['quality_score']
            for obj in event.get('objects', []):
                all_objects_weighted[obj] += weight
        
        if all_objects_weighted:
            top_objects = sorted(all_objects_weighted.items(), key=lambda x: x[1], reverse=True)[:5]
            content_parts.append(f"- ì£¼ìš” ê°ì²´ (ê°€ì¤‘ì¹˜): {', '.join([f'{obj}({weight:.1f})' for obj, weight in top_objects])}")
        
        # í’ˆì§ˆë³„ ë¶„í¬
        quality_tiers = defaultdict(int)
        for event in temporal_index.events:
            tier = temporal_index._get_quality_tier(event['quality_score'])
            quality_tiers[tier] += 1
        
        if quality_tiers:
            quality_summary = ', '.join([f"{tier}í’ˆì§ˆ: {count}ê°œ" for tier, count in quality_tiers.items()])
            content_parts.append(f"- í’ˆì§ˆ ë¶„í¬: {quality_summary}")
        
        # ì‹œê°„ì  íŒ¨í„´
        if temporal_index.events:
            time_span = max(e['timestamp'] for e in temporal_index.events) - min(e['timestamp'] for e in temporal_index.events)
            content_parts.append(f"- ë¶„ì„ ì‹œê°„ ë²”ìœ„: {time_span:.1f}ì´ˆ")
        
        # ì˜ë¯¸ì  í‚¤ì›Œë“œ ìš”ì•½
        if temporal_index.semantic_clusters:
            top_keywords = sorted(temporal_index.semantic_clusters.items(), 
                                key=lambda x: x[1]['count'], reverse=True)[:5]
            keyword_summary = ', '.join([f"{keyword}({data['count']}íšŒ)" for keyword, data in top_keywords])
            content_parts.append(f"- ì£¼ìš” ì˜ë¯¸ í‚¤ì›Œë“œ: {keyword_summary}")
        
        return '\n'.join(content_parts)
    
    def _calculate_video_quality_summary(self, temporal_index: HierarchicalTemporalIndex) -> Dict:
        """ë¹„ë””ì˜¤ ì „ì²´ í’ˆì§ˆ ìš”ì•½ ê³„ì‚°"""
        if not temporal_index.events:
            return {}
        
        quality_scores = [e['quality_score'] for e in temporal_index.events]
        
        return {
            'average_quality': np.mean(quality_scores),
            'max_quality': np.max(quality_scores),
            'min_quality': np.min(quality_scores),
            'quality_std': np.std(quality_scores),
            'high_quality_ratio': sum(1 for q in quality_scores if q >= 0.7) / len(quality_scores),
            'total_events': len(temporal_index.events)
        }
    
    def _create_quality_index(self, frame_results: List[Dict]) -> Dict:
        """í’ˆì§ˆë³„ ì¸ë±ìŠ¤ ìƒì„±"""
        quality_index = {'high': [], 'medium': [], 'low': []}
        
        for frame_result in frame_results:
            quality_score = self._calculate_frame_quality_score(frame_result)
            tier = self._get_quality_tier(quality_score)
            quality_index[tier].append({
                'frame_id': frame_result.get('image_id', 0),
                'timestamp': frame_result.get('timestamp', 0),
                'quality_score': quality_score
            })
        
        return quality_index
    
    def _create_superior_hierarchical_vector_db(self, video_id: str, documents: List[Document]) -> bool:
        """ìµœê³ ê¸‰ ê³„ì¸µì  ë²¡í„° DB ìƒì„±"""
        if not self._embeddings_initialized or not documents:
            return False
        
        try:
            # ë¬¸ì„œ ë ˆë²¨ë³„ ë¶„ë¦¬
            frame_docs = [doc for doc in documents if doc.metadata.get('level') == 'frame']
            segment_docs = [doc for doc in documents if doc.metadata.get('level') == 'segment']
            semantic_docs = [doc for doc in documents if doc.metadata.get('level') == 'semantic']
            video_docs = [doc for doc in documents if doc.metadata.get('level') == 'video']
            
            print(f"ğŸ“Š ë¬¸ì„œ ë¶„ë¦¬: í”„ë ˆì„ {len(frame_docs)}, ì„¸ê·¸ë¨¼íŠ¸ {len(segment_docs)}, ì˜ë¯¸ {len(semantic_docs)}, ë¹„ë””ì˜¤ {len(video_docs)}")
            
            # ì „ì²´ í†µí•© FAISS ì¸ë±ìŠ¤ ìƒì„±
            db = FAISS.from_documents(documents, embedding=self.embeddings)
            
            # ë ˆë²¨ë³„ ì „ìš© ê²€ìƒ‰ê¸° ìƒì„±
            retrievers = {}
            
            if frame_docs:
                frame_db = FAISS.from_documents(frame_docs, embedding=self.embeddings)
                retrievers['frame'] = frame_db.as_retriever(
                    search_type="similarity",
                    search_kwargs={'k': self.config.top_k}
                )
            
            if segment_docs:
                segment_db = FAISS.from_documents(segment_docs, embedding=self.embeddings)
                retrievers['segment'] = segment_db.as_retriever(
                    search_type="similarity",
                    search_kwargs={'k': max(1, self.config.top_k // 2)}
                )
            
            if semantic_docs:
                semantic_db = FAISS.from_documents(semantic_docs, embedding=self.embeddings)
                retrievers['semantic'] = semantic_db.as_retriever(
                    search_type="similarity", 
                    search_kwargs={'k': max(1, self.config.top_k // 3)}
                )
            
            if video_docs:
                video_db = FAISS.from_documents(video_docs, embedding=self.embeddings)
                retrievers['video'] = video_db.as_retriever(
                    search_type="similarity",
                    search_kwargs={'k': 1}
                )
            
            # í’ˆì§ˆë³„ ê²€ìƒ‰ê¸° ìƒì„±
            quality_retrievers = {}
            for quality_tier in ['high', 'medium', 'low']:
                quality_docs = [doc for doc in documents 
                              if doc.metadata.get('quality_tier') == quality_tier]
                
                if quality_docs:
                    quality_db = FAISS.from_documents(quality_docs, embedding=self.embeddings)
                    quality_retrievers[quality_tier] = quality_db.as_retriever(
                        search_type="similarity",
                        search_kwargs={'k': self.config.top_k}
                    )
            
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸” ê²€ìƒ‰ê¸° êµ¬ì„±
            try:
                # BM25 ê²€ìƒ‰ê¸° ì¶”ê°€
                bm25_retriever = BM25Retriever.from_documents(documents)
                bm25_retriever.k = self.config.top_k
                
                # ì£¼ìš” ê²€ìƒ‰ê¸°ë“¤
                main_retrievers = []
                weights = []
                
                if 'frame' in retrievers:
                    main_retrievers.append(retrievers['frame'])
                    weights.append(self.config.frame_level_weight)
                
                if 'segment' in retrievers:
                    main_retrievers.append(retrievers['segment'])
                    weights.append(self.config.segment_level_weight)
                
                if 'video' in retrievers:
                    main_retrievers.append(retrievers['video'])
                    weights.append(self.config.video_level_weight)
                
                main_retrievers.append(bm25_retriever)
                weights.append(0.15)  # BM25 ê°€ì¤‘ì¹˜
                
                # ê°€ì¤‘ì¹˜ ì •ê·œí™”
                total_weight = sum(weights)
                normalized_weights = [w/total_weight for w in weights]
                
                ensemble_retriever = EnsembleRetriever(
                    retrievers=main_retrievers,
                    weights=normalized_weights
                )
                
            except Exception as e:
                print(f"âš ï¸ ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
                ensemble_retriever = db.as_retriever(
                    search_type="similarity",
                    search_kwargs={'k': self.config.top_k}
                )
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì €ì¥
            self.video_databases[video_id] = {
                'db': db,
                'retriever': ensemble_retriever,
                'level_retrievers': retrievers,
                'quality_retrievers': quality_retrievers,
                'documents': documents,
                'created_at': datetime.now(),
                'config': self.config,
                'document_stats': {
                    'total': len(documents),
                    'frame': len(frame_docs),
                    'segment': len(segment_docs),
                    'semantic': len(semantic_docs),
                    'video': len(video_docs)
                }
            }
            
            print(f"âœ… ìµœê³ ê¸‰ ê³„ì¸µì  ë²¡í„° DB ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ìµœê³ ê¸‰ ë²¡í„° DB ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def intelligent_search_video_content(self, video_id: str, query: str, 
                                       context: Optional[Dict] = None) -> List[Dict]:
        """ì§€ëŠ¥í˜• ë¹„ë””ì˜¤ ë‚´ìš© ê²€ìƒ‰"""
        if video_id not in self.video_databases:
            print(f"âš ï¸ ë¹„ë””ì˜¤ {video_id}ì˜ RAG DBê°€ ì—†ìŒ")
            return []
        
        try:
            # ì§ˆë¬¸ ì˜ë„ ë¶„ì„
            intent = self.korean_processor.analyze_question_intent_advanced(query)
            
            print(f"ğŸ§  ì§ˆë¬¸ ì˜ë„ ë¶„ì„:")
            print(f"   - ì£¼ íƒ€ì…: {intent['primary_type']}")
            print(f"   - ë¶€ íƒ€ì…: {intent['secondary_types']}")
            print(f"   - ì‹ ë¢°ë„: {intent['confidence']:.2f}")
            print(f"   - ë³µì¡ë„: {intent['complexity']}")
            print(f"   - í•„ìš” ë¶„ì„ ë ˆë²¨: {intent['required_analysis_level']}")
            
            # ê³„ì¸µì  ìºì‹œ í™•ì¸
            cached_result = self.cache_manager.get_hierarchical_cache(
                video_id, query, intent['required_analysis_level']
            )
            if cached_result:
                print(f"ğŸ¯ ê³„ì¸µì  ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜ (ë ˆë²¨: {cached_result['cached_level']})")
                return cached_result['results']
            
            # í’ˆì§ˆ ì¸ì‹ ìºì‹œ í™•ì¸
            min_quality = 0.7 if intent['confidence'] > 0.8 else 0.5
            quality_cached = self.cache_manager.get_quality_aware_cache(video_id, query, min_quality)
            if quality_cached:
                print(f"ğŸ¯ í’ˆì§ˆ ì¸ì‹ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
                return quality_cached
            
            # ì§€ëŠ¥í˜• ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
            results = self._execute_intelligent_search_strategy(video_id, query, intent)
            
            # ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if intent['primary_type'] == 'temporal' and video_id in self.temporal_indexes:
                results = self._add_advanced_temporal_context(video_id, query, results, intent)
            
            # í’ˆì§ˆ ê¸°ë°˜ í›„ì²˜ë¦¬
            results = self._apply_quality_boost(results, intent)
            
            # ê²°ê³¼ ìºì‹±
            self.cache_manager.set_hierarchical_cache(
                video_id, query, intent['required_analysis_level'], results
            )
            
            print(f"ğŸ” ì§€ëŠ¥í˜• ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            print(f"   - ì‚¬ìš©ëœ ì „ëµ: {intent['primary_type']}")
            print(f"   - í’ˆì§ˆ ë¶€ìŠ¤íŠ¸: {'ì ìš©' if intent['confidence'] > 0.7 else 'ë¯¸ì ìš©'}")
            
            return results
            
        except Exception as e:
            print(f"âŒ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    def _execute_intelligent_search_strategy(self, video_id: str, query: str, intent: Dict) -> List[Dict]:
        """ì§€ëŠ¥í˜• ê²€ìƒ‰ ì „ëµ ì‹¤í–‰"""
        db_info = self.video_databases[video_id]
        
        # ì˜ë„ë³„ ë§ì¶¤ ê²€ìƒ‰
        if intent['primary_type'] == 'temporal':
            return self._temporal_focused_search(db_info, query, intent)
        elif intent['primary_type'] == 'person_analysis':
            return self._person_focused_search(db_info, query, intent)
        elif intent['primary_type'] == 'object_detection':
            return self._object_focused_search(db_info, query, intent)
        elif intent['primary_type'] == 'attribute_analysis':
            return self._attribute_focused_search(db_info, query, intent)
        elif intent['primary_type'] == 'semantic':
            return self._semantic_focused_search(db_info, query, intent)
        else:
            return self._comprehensive_search(db_info, query, intent)
    
    def _temporal_focused_search(self, db_info: Dict, query: str, intent: Dict) -> List[Dict]:
        """ì‹œê°„ ì¤‘ì‹¬ ê²€ìƒ‰"""
        # ì„¸ê·¸ë¨¼íŠ¸ ë ˆë²¨ ìš°ì„  ê²€ìƒ‰
        if 'segment' in db_info['level_retrievers']:
            docs = db_info['level_retrievers']['segment'].get_relevant_documents(query)
        else:
            docs = db_info['retriever'].get_relevant_documents(query)
        
        return self._format_search_results_advanced(docs, search_type='temporal')
    
    def _person_focused_search(self, db_info: Dict, query: str, intent: Dict) -> List[Dict]:
        """ì‚¬ëŒ ì¤‘ì‹¬ ê²€ìƒ‰"""
        # í”„ë ˆì„ ë ˆë²¨ì—ì„œ ì‚¬ëŒ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
        if 'frame' in db_info['level_retrievers']:
            docs = db_info['level_retrievers']['frame'].get_relevant_documents(query)
        else:
            docs = db_info['retriever'].get_relevant_documents(query)
        
        # ì‚¬ëŒ ê´€ë ¨ ë¬¸ì„œë§Œ í•„í„°ë§
        person_docs = [doc for doc in docs 
                      if any(obj.lower() in ['person', 'man', 'woman', 'people'] 
                            for obj in doc.metadata.get('objects', []))]
        
        return self._format_search_results_advanced(person_docs or docs, search_type='person')
    
    def _object_focused_search(self, db_info: Dict, query: str, intent: Dict) -> List[Dict]:
        """ê°ì²´ ì¤‘ì‹¬ ê²€ìƒ‰"""
        # í”„ë ˆì„ ë ˆë²¨ ê²€ìƒ‰
        if 'frame' in db_info['level_retrievers']:
            docs = db_info['level_retrievers']['frame'].get_relevant_documents(query)
        else:
            docs = db_info['retriever'].get_relevant_documents(query)
        
        return self._format_search_results_advanced(docs, search_type='object')
    
    def _attribute_focused_search(self, db_info: Dict, query: str, intent: Dict) -> List[Dict]:
        """ì†ì„± ì¤‘ì‹¬ ê²€ìƒ‰"""
        # ê³ í’ˆì§ˆ ë¬¸ì„œ ìš°ì„  ê²€ìƒ‰
        if 'high' in db_info['quality_retrievers']:
            docs = db_info['quality_retrievers']['high'].get_relevant_documents(query)
            if not docs and 'medium' in db_info['quality_retrievers']:
                docs = db_info['quality_retrievers']['medium'].get_relevant_documents(query)
        else:
            docs = db_info['retriever'].get_relevant_documents(query)
        
        return self._format_search_results_advanced(docs, search_type='attribute')
    
    def _semantic_focused_search(self, db_info: Dict, query: str, intent: Dict) -> List[Dict]:
        """ì˜ë¯¸ ì¤‘ì‹¬ ê²€ìƒ‰"""
        # ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ìš°ì„  ê²€ìƒ‰
        if 'semantic' in db_info['level_retrievers']:
            semantic_docs = db_info['level_retrievers']['semantic'].get_relevant_documents(query)
            if semantic_docs:
                return self._format_search_results_advanced(semantic_docs, search_type='semantic')
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë ˆë²¨ ë³´ì¡° ê²€ìƒ‰
        if 'segment' in db_info['level_retrievers']:
            docs = db_info['level_retrievers']['segment'].get_relevant_documents(query)
        else:
            docs = db_info['retriever'].get_relevant_documents(query)
        
        return self._format_search_results_advanced(docs, search_type='semantic')
    
    def _comprehensive_search(self, db_info: Dict, query: str, intent: Dict) -> List[Dict]:
        """ì¢…í•© ê²€ìƒ‰"""
        # ëª¨ë“  ë ˆë²¨ì—ì„œ ê²€ìƒ‰í•˜ì—¬ í†µí•©
        all_results = []
        
        # ê° ë ˆë²¨ë³„ ê²€ìƒ‰
        for level, retriever in db_info['level_retrievers'].items():
            try:
                docs = retriever.get_relevant_documents(query)
                level_results = self._format_search_results_advanced(docs, search_type=level)
                all_results.extend(level_results)
            except Exception as e:
                print(f"âš ï¸ {level} ë ˆë²¨ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        unique_results = self._deduplicate_and_rank_results(all_results)
        
        return unique_results[:self.config.top_k]
    
    def _format_search_results_advanced(self, docs: List[Document], search_type: str = 'general') -> List[Dict]:
        """ê³ ë„í™”ëœ ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…"""
        results = []
        for doc in docs:
            result = {
                'content': doc.page_content,
                'metadata': doc.metadata,
                'level': doc.metadata.get('level', 'unknown'),
                'search_type': search_type,
                'frame_id': doc.metadata.get('frame_id'),
                'timestamp': doc.metadata.get('timestamp'),
                'objects': doc.metadata.get('objects', []),
                'quality_score': doc.metadata.get('quality_score', 0.5),
                'quality_tier': doc.metadata.get('quality_tier', 'medium')
            }
            
            # ë ˆë²¨ë³„ ì¶”ê°€ ì •ë³´
            if result['level'] == 'segment':
                result.update({
                    'start_time': doc.metadata.get('start_time'),
                    'end_time': doc.metadata.get('end_time'),
                    'duration': doc.metadata.get('duration'),
                    'dominant_objects': doc.metadata.get('dominant_objects', [])
                })
            elif result['level'] == 'semantic':
                result.update({
                    'semantic_keyword': doc.metadata.get('semantic_keyword'),
                    'event_count': doc.metadata.get('event_count', 0)
                })
            
            results.append(result)
        
        return results
    
    def _deduplicate_and_rank_results(self, results: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±° ë° ìˆœìœ„ ë§¤ê¸°ê¸°"""
        if not results:
            return []
        
        # ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        seen_content = set()
        unique_results = []
        
        for result in results:
            content_hash = hashlib.md5(result['content'].encode()).hexdigest()
            
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        # ì¢…í•© ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        def calculate_ranking_score(result):
            base_score = result.get('quality_score', 0.5)
            
            # ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜
            level_weights = {'frame': 0.8, 'segment': 1.0, 'semantic': 0.9, 'video': 0.6}
            level_weight = level_weights.get(result.get('level', 'frame'), 0.5)
            
            # í’ˆì§ˆ ë“±ê¸‰ ë³´ë„ˆìŠ¤
            quality_bonuses = {'high': 0.3, 'medium': 0.1, 'low': 0.0}
            quality_bonus = quality_bonuses.get(result.get('quality_tier', 'medium'), 0.0)
            
            return base_score * level_weight + quality_bonus
        
        unique_results.sort(key=calculate_ranking_score, reverse=True)
        return unique_results
    
    def _add_advanced_temporal_context(self, video_id: str, query: str, results: List[Dict], intent: Dict) -> List[Dict]:
        """ê³ ë„í™”ëœ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"""
        temporal_index = self.temporal_indexes.get(video_id)
        if not temporal_index:
            return results
        
        enhanced_results = []
        for result in results:
            timestamp = result.get('timestamp')
            if timestamp is not None:
                # ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
                temporal_context = self._analyze_temporal_context(temporal_index, timestamp, intent)
                result['advanced_temporal_context'] = temporal_context
            
            enhanced_results.append(result)
        
        return enhanced_results
    
    def _analyze_temporal_context(self, temporal_index: HierarchicalTemporalIndex, 
                                timestamp: float, intent: Dict) -> Dict:
        """ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        context = {
            'nearby_events': [],
            'sequence_analysis': {},
            'temporal_patterns': {}
        }
        
        # ì£¼ë³€ ì´ë²¤íŠ¸ ì°¾ê¸° (í™•ì¥ëœ ìœˆë„ìš°)
        window_size = 10.0  # 10ì´ˆ ìœˆë„ìš°
        nearby_events = []
        
        for event in temporal_index.events:
            time_diff = abs(event['timestamp'] - timestamp)
            if time_diff <= window_size:
                nearby_events.append({
                    'timestamp': event['timestamp'],
                    'caption': event['caption'],
                    'objects': event['objects'],
                    'quality_score': event['quality_score'],
                    'time_diff': time_diff
                })
        
        # ì‹œê°„ìˆœ ì •ë ¬
        nearby_events.sort(key=lambda x: x['timestamp'])
        context['nearby_events'] = nearby_events[:5]
        
        # ì‹œí€€ìŠ¤ ë¶„ì„
        if len(nearby_events) >= 2:
            context['sequence_analysis'] = {
                'sequence_length': len(nearby_events),
                'time_span': max(e['timestamp'] for e in nearby_events) - min(e['timestamp'] for e in nearby_events),
                'activity_progression': self._analyze_activity_progression(nearby_events)
            }
        
        return context
    
    def _analyze_activity_progression(self, events: List[Dict]) -> str:
        """í™œë™ ì§„í–‰ íŒ¨í„´ ë¶„ì„"""
        if len(events) < 2:
            return "ë‹¨ì¼ ì´ë²¤íŠ¸"
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë¶„ì„
        object_changes = []
        for i in range(1, len(events)):
            prev_objects = set(events[i-1]['objects'])
            curr_objects = set(events[i]['objects'])
            
            if prev_objects != curr_objects:
                object_changes.append("ë³€í™”")
            else:
                object_changes.append("ì§€ì†")
        
        if all(change == "ì§€ì†" for change in object_changes):
            return "ì•ˆì •ì  ì¥ë©´"
        elif object_changes.count("ë³€í™”") > len(object_changes) // 2:
            return "ì—­ë™ì  ë³€í™”"
        else:
            return "ì ì§„ì  ë³€í™”"
    
    def _apply_quality_boost(self, results: List[Dict], intent: Dict) -> List[Dict]:
        """í’ˆì§ˆ ê¸°ë°˜ ê²°ê³¼ í–¥ìƒ"""
        if intent['confidence'] < 0.7:
            return results
        
        boosted_results = []
        for result in results:
            quality_score = result.get('quality_score', 0.5)
            
            # ê³ í’ˆì§ˆ ê²°ê³¼ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
            if quality_score >= self.config.quality_boost_threshold:
                result['boosted'] = True
                result['boost_factor'] = 1.2
            
            boosted_results.append(result)
        
        # í’ˆì§ˆ ë¶€ìŠ¤íŠ¸ëœ ê²°ê³¼ ìš°ì„  ì •ë ¬
        boosted_results.sort(key=lambda x: (
            x.get('boosted', False),
            x.get('quality_score', 0.5)
        ), reverse=True)
        
        return boosted_results
    
    def generate_contextual_korean_answer(self, video_id: str, question: str, 
                                        context: Optional[Dict] = None) -> str:
        """ìƒí™© ì¸ì‹ í•œêµ­ì–´ ë‹µë³€ ìƒì„±"""
        if not self._llm_initialized:
            return "LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì§€ëŠ¥í˜• ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self.intelligent_search_video_content(video_id, question, context)
        
        if not search_results:
            return "ê´€ë ¨ëœ ë¹„ë””ì˜¤ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì§ˆë¬¸ ì˜ë„ ë¶„ì„
        intent = self.korean_processor.analyze_question_intent_advanced(question)
        
        # ìƒí™©ë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_contextual_korean_prompt(question, search_results, intent, context)
        
        try:
            response = self.llm.invoke(prompt)
            answer = response.content
            
            # ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë° ê°œì„ 
            enhanced_answer = self._enhance_answer_quality(answer, search_results, intent)
            
            # ì‘ë‹µ ìºì‹±
            cache_key = self.cache_manager.get_cache_key(video_id, question, "enhanced_response")
            cache.set(cache_key, enhanced_answer, timeout=self.config.cache_ttl_response)
            
            return enhanced_answer
            
        except Exception as e:
            print(f"âŒ ìƒí™© ì¸ì‹ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _build_contextual_korean_prompt(self, question: str, search_results: List[Dict], 
                                      intent: Dict, context: Optional[Dict] = None) -> str:
        """ìƒí™©ë³„ ë§ì¶¤ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì˜ë„ë³„ ì „ë¬¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€
        system_messages = {
            'temporal': """ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ì˜ ì‹œê°„ì  íë¦„ê³¼ ìˆœì„œë¥¼ ì •í™•íˆ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                         ì‹œê°„ ìˆœì„œ, ì´ë²¤íŠ¸ ë°œìƒ ì‹œì , ì§€ì† ì‹œê°„, ë³€í™” íŒ¨í„´ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.""",
            'person_analysis': """ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ì† ì¸ë¬¼ê³¼ ë³´í–‰ìë¥¼ ì„¸ë°€íˆ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                                ì‚¬ëŒì˜ ì™¸ëª¨, í–‰ë™, ì†ì„±, ìœ„ì¹˜ ë³€í™”ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.""",
            'object_detection': """ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ì† ê°ì²´ì™€ ì‚¬ë¬¼ì„ ì •í™•íˆ ì‹ë³„í•˜ê³  ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                                 ê°ì²´ì˜ ì¢…ë¥˜, ìœ„ì¹˜, ìƒíƒœ, ìƒí˜¸ì‘ìš©ì„ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.""",
            'attribute_analysis': """ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ì† ì‚¬ëŒë“¤ì˜ ì†ì„±ê³¼ íŠ¹ì§•ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                                   ì˜ë³µ, ì•¡ì„¸ì„œë¦¬, ì™¸ëª¨, ìì„¸ ë“±ì„ ìì„¸íˆ ë¶„ì„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.""",
            'action_recognition': """ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ì† í–‰ë™ê³¼ í™œë™ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                                   ë™ì‘ì˜ ì¢…ë¥˜, ì§„í–‰ ê³¼ì •, ìƒí˜¸ì‘ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""
        }
        
        system_msg = system_messages.get(intent['primary_type'], 
                                       "ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.")
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í’ˆì§ˆê³¼ ë ˆë²¨ë³„ë¡œ êµ¬ì„±
        context_sections = self._organize_context_by_quality_and_level(search_results)
        
        # ê³ í’ˆì§ˆ ì •ë³´ ê°•ì¡°
        high_quality_info = context_sections.get('high_quality', [])
        medium_quality_info = context_sections.get('medium_quality', [])
        
        context_text = ""
        
        if high_quality_info:
            context_text += "=== ê³ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ ===\n"
            for info in high_quality_info[:3]:  # ìƒìœ„ 3ê°œ
                context_text += f"- {info}\n"
            context_text += "\n"
        
        if medium_quality_info:
            context_text += "=== ë³´ì¡° ë¶„ì„ ê²°ê³¼ ===\n"
            for info in medium_quality_info[:2]:  # ìƒìœ„ 2ê°œ
                context_text += f"- {info}\n"
            context_text += "\n"
        
        # ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        temporal_info = self._extract_temporal_information(search_results)
        if temporal_info:
            context_text += "=== ì‹œê°„ì  ì •ë³´ ===\n"
            context_text += temporal_info + "\n\n"
        
        # ë¹„ë””ì˜¤ ë©”íƒ€ì •ë³´
        video_info = ""
        if context:
            video_info = f"""
ë¹„ë””ì˜¤ ì •ë³´:
- íŒŒì¼ëª…: {context.get('filename', 'unknown')}
- ê¸¸ì´: {context.get('duration', 0)}ì´ˆ
- ë¶„ì„ í’ˆì§ˆ: {context.get('analysis_quality', 'ì¤‘ê°„')}
- ì£¼ìš” íŠ¹ì§•: {', '.join(context.get('key_features', []))}
"""
        
        # ì˜ë„ë³„ íŠ¹ë³„ ì§€ì¹¨
        intent_specific_instructions = self._get_intent_specific_instructions(intent)
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""{system_msg}

{video_info}

ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼:
{context_text}

{intent_specific_instructions}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€ ìš”êµ¬ì‚¬í•­:
1. ì œê³µëœ ë¶„ì„ ê²°ê³¼ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”
2. ì‹œê°„ ì •ë³´(ì´ˆ, ë¶„)ì™€ êµ¬ì²´ì  ìœ„ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš”
3. ê³ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¶„ì„ ê²°ê³¼ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
6. ê´€ë ¨ ê°ì²´ë‚˜ ì¸ë¬¼ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ì¸ íŠ¹ì§•ê³¼ í•¨ê»˜ ì–¸ê¸‰í•˜ì„¸ìš”

ë‹µë³€:"""
        
        return prompt
    
    def _organize_context_by_quality_and_level(self, search_results: List[Dict]) -> Dict[str, List[str]]:
        """í’ˆì§ˆê³¼ ë ˆë²¨ë³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        organized = {
            'high_quality': [],
            'medium_quality': [],
            'low_quality': []
        }
        
        for result in search_results:
            quality_tier = result.get('quality_tier', 'medium')
            content = result['content']
            level = result.get('level', 'frame')
            
            # ë ˆë²¨ ì •ë³´ ì¶”ê°€
            if level == 'segment':
                start_time = result.get('start_time', 0)
                end_time = result.get('end_time', 0)
                content = f"[{start_time:.1f}s-{end_time:.1f}s êµ¬ê°„] {content}"
            elif level == 'frame':
                timestamp = result.get('timestamp', 0)
                frame_id = result.get('frame_id', 0)
                content = f"[í”„ë ˆì„ {frame_id}, {timestamp:.1f}s] {content}"
            elif level == 'semantic':
                keyword = result.get('semantic_keyword', '')
                content = f"[ì˜ë¯¸: {keyword}] {content}"
            
            # í’ˆì§ˆë³„ ë¶„ë¥˜
            if quality_tier == 'high':
                organized['high_quality'].append(content)
            elif quality_tier == 'medium':
                organized['medium_quality'].append(content)
            else:
                organized['low_quality'].append(content)
        
        return organized
    
    def _extract_temporal_information(self, search_results: List[Dict]) -> str:
        """ì‹œê°„ì  ì •ë³´ ì¶”ì¶œ"""
        temporal_info = []
        
        # ì‹œê°„ìˆœ ì •ë ¬
        timed_results = [r for r in search_results if r.get('timestamp') is not None]
        timed_results.sort(key=lambda x: x['timestamp'])
        
        if len(timed_results) >= 2:
            time_span = timed_results[-1]['timestamp'] - timed_results[0]['timestamp']
            temporal_info.append(f"ë¶„ì„ ì‹œê°„ ë²”ìœ„: {timed_results[0]['timestamp']:.1f}s ~ {timed_results[-1]['timestamp']:.1f}s ({time_span:.1f}ì´ˆ ë™ì•ˆ)")
        
        # ê³ ê¸‰ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸
        for result in search_results[:2]:  # ìƒìœ„ 2ê°œë§Œ
            temporal_context = result.get('advanced_temporal_context')
            if temporal_context:
                nearby_events = temporal_context.get('nearby_events', [])
                if nearby_events:
                    sequence_info = f"ì£¼ë³€ ì´ë²¤íŠ¸: {len(nearby_events)}ê°œ ê´€ë ¨ ì¥ë©´"
                    temporal_info.append(sequence_info)
                    break
        
        return '\n'.join(temporal_info) if temporal_info else ""
    
    def _get_intent_specific_instructions(self, intent: Dict) -> str:
        """ì˜ë„ë³„ íŠ¹ë³„ ì§€ì¹¨"""
        instructions = {
            'temporal': """
ì‹œê°„ ê´€ë ¨ ë‹µë³€ ì‹œ íŠ¹ë³„ ì£¼ì˜ì‚¬í•­:
- ì •í™•í•œ ì‹œê°„(ì´ˆ) ëª…ì‹œ
- ì´ë²¤íŠ¸ ìˆœì„œì™€ ì§€ì† ì‹œê°„ í¬í•¨
- ì‹œê°„ì  ë³€í™” íŒ¨í„´ ì„¤ëª…""",
            'person_analysis': """
ì¸ë¬¼ ë¶„ì„ ì‹œ íŠ¹ë³„ ì£¼ì˜ì‚¬í•­:
- êµ¬ì²´ì ì¸ ì™¸ëª¨ì™€ ì˜ë³µ íŠ¹ì§• ì„¤ëª…
- ìœ„ì¹˜ ë° í–‰ë™ ë³€í™” ì¶”ì 
- ë‹¤ë¥¸ ì¸ë¬¼ê³¼ì˜ êµ¬ë¶„ì  ëª…ì‹œ""",
            'object_detection': """
ê°ì²´ ë¶„ì„ ì‹œ íŠ¹ë³„ ì£¼ì˜ì‚¬í•­:
- ê°ì²´ì˜ ì •í™•í•œ ëª…ì¹­ê³¼ íŠ¹ì§•
- ìœ„ì¹˜ ë° ìƒíƒœ ë³€í™”
- ë‹¤ë¥¸ ê°ì²´ì™€ì˜ ê´€ê³„""",
            'attribute_analysis': """
ì†ì„± ë¶„ì„ ì‹œ íŠ¹ë³„ ì£¼ì˜ì‚¬í•­:
- ì‹ ë¢°ë„ê°€ ë†’ì€ ì†ì„± ì •ë³´ ìš°ì„  ì‚¬ìš©
- ë¶ˆí™•ì‹¤í•œ ì†ì„±ì€ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ
- ì—¬ëŸ¬ ê´€ì°° ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì¢…í•©ì  íŒë‹¨"""
        }
        
        return instructions.get(intent['primary_type'], "")
    
    def _enhance_answer_quality(self, answer: str, search_results: List[Dict], intent: Dict) -> str:
        """ë‹µë³€ í’ˆì§ˆ í–¥ìƒ"""
        # ê¸°ë³¸ ê²€ì¦
        if not answer or len(answer.strip()) < 10:
            return "ì œê³µëœ ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì‹œê°„ ì •ë³´ ê²€ì¦ ë° ë³´ê°•
        if intent['primary_type'] == 'temporal':
            answer = self._enhance_temporal_answer(answer, search_results)
        
        # í’ˆì§ˆ ì •ë³´ ì¶”ê°€
        high_quality_count = sum(1 for r in search_results if r.get('quality_tier') == 'high')
        if high_quality_count > 0:
            answer += f"\n\n(ì´ ë‹µë³€ì€ {high_quality_count}ê°œì˜ ê³ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)"
        
        return answer
    
    def _enhance_temporal_answer(self, answer: str, search_results: List[Dict]) -> str:
        """ì‹œê°„ ê´€ë ¨ ë‹µë³€ í–¥ìƒ"""
        # ì‹œê°„ ì •ë³´ê°€ ëˆ„ë½ëœ ê²½ìš° ë³´ê°•
        import re
        
        time_pattern = r'\d+\.?\d*ì´ˆ'
        if not re.search(time_pattern, answer):
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            timestamps = [r.get('timestamp') for r in search_results if r.get('timestamp') is not None]
            if timestamps:
                min_time, max_time = min(timestamps), max(timestamps)
                if min_time == max_time:
                    answer += f" (ê´€ë ¨ ì‹œì : {min_time:.1f}ì´ˆ)"
                else:
                    answer += f" (ê´€ë ¨ ì‹œê°„ëŒ€: {min_time:.1f}ì´ˆ~{max_time:.1f}ì´ˆ)"
        
        return answer
    
    # ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    def process_video_analysis_json(self, json_file_path: str, video_id: str) -> bool:
        """ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€"""
        return self.process_video_analysis_json_advanced(json_file_path, video_id)
    
    def search_video_content(self, video_id: str, query: str, top_k: int = 5):
        """ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€"""
        results = self.intelligent_search_video_content(video_id, query)
        return results[:top_k]
    
    def answer_question(self, video_id: str, question: str):
        """ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€"""
        return self.generate_contextual_korean_answer(video_id, question)
    
    def get_database_info(self, video_id: str = None):
        """ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ (í–¥ìƒë¨)"""
        if video_id:
            if video_id in self.video_databases:
                db_info = self.video_databases[video_id]
                temporal_index = self.temporal_indexes.get(video_id)
                quality_index = self.quality_indexes.get(video_id)
                
                return {
                    'video_id': video_id,
                    'document_count': len(db_info['documents']),
                    'document_stats': db_info.get('document_stats', {}),
                    'created_at': db_info['created_at'].isoformat(),
                    'config': {
                        'embedding_model': self.config.embedding_model,
                        'top_k': self.config.top_k,
                        'similarity_threshold': self.config.similarity_threshold
                    },
                    'temporal_index': {
                        'total_events': len(temporal_index.events) if temporal_index else 0,
                        'segments': len(temporal_index.segments) if temporal_index else 0,
                        'semantic_clusters': len(temporal_index.semantic_clusters) if temporal_index else 0
                    } if temporal_index else None,
                    'quality_distribution': {
                        tier: len(frames) for tier, frames in quality_index.items()
                    } if quality_index else None
                }
            else:
                return None
        else:
            return {
                'total_videos': len(self.video_databases),
                'videos': list(self.video_databases.keys()),
                'system_status': {
                    'embeddings_initialized': self._embeddings_initialized,
                    'llm_initialized': self._llm_initialized,
                    'device': self.device
                },
                'config_summary': {
                    'embedding_model': self.config.embedding_model,
                    'korean_analysis': self.config.use_korean_morphology,
                    'quality_aware': True,
                    'hierarchical_search': True
                }
            }

# ì „ì—­ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_superior_rag_system = None

def get_enhanced_video_rag_system(config: Optional[AdvancedVideoRAGConfig] = None):
    """ìµœê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _superior_rag_system
    if _superior_rag_system is None:
        _superior_rag_system = SuperiorVideoRAGSystem(config)
    return _superior_rag_system

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
def get_video_rag_system():
    """ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€"""
    return get_enhanced_video_rag_system()
